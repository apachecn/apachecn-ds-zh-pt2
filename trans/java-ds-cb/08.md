

# 第八章。从数据中深入学习

在本章中，我们将介绍以下配方:

*   使用面向 Java 的深度学习(DL4j)创建 Word2vec 神经网络
*   使用面向 Java 的深度学习(DL4j)创建深度信念神经网络
*   使用面向 Java 的深度学习(DL4j)创建深度自动编码器

# 简介

深度学习就是简单的多层神经网络。也被称为深度神经网络学习或无监督特征学习。作者认为，深度学习由于其解决现实世界数据问题的能力，将成为机器学习从业者和数据科学家的下一个帮凶。

**面向 Java 的深度学习** ( **DL4j** )是一个面向 JVM 的深度学习开源分布式 Java 库。它附带了其他库，如下所示:

*   Deeplearning4J:神经网络平台
*   ND4J:JVM 的 NumPy
*   DataVec:用于机器学习 ETL 操作的工具
*   JavaCPP:Java 和原生 C++之间的桥梁
*   仲裁器:机器学习算法的评估工具
*   RL4J:JVM 的深度强化学习

然而，考虑到本书的范围，我们将只关注 DL4j 的几个关键配方。具体来说，我们将讨论使用 Word2vec 算法的方法及其在现实世界 NLP 和信息检索问题中的应用，深度信念神经网络和深度自动编码器及其用法。强烈建议好奇的读者去 https://github.com/deeplearning4j/dl4j-examples 看更多的例子。注意，本章食谱中的代码是基于 GitHub 上的这些例子。

还要注意的是，在这一章中，很大一部分致力于展示 DL4j 库是如何设置的，因为这个过程非常复杂，为了成功地执行本书中的代码和自己的代码，读者需要集中注意力。

本章中的所有方法都有两个先决条件:Java Developer 或更高版本(作者的版本是 1.8)和 Apache Maven。本章中的食谱是使用 Eclipse Java IDE 实现的(作者有 Eclipse Mars)。虽然[https://deeplearning4j.org/quickstart](https://deeplearning4j.org/quickstart)包含了大量关于用 Java 设置 DL4j 的材料，但大多数都集中在另一个 IDE 上，名为 IntelliJ。

要执行本章中的配方，我们需要以下内容:

1.  要使用 DL4j，您需要安装 Apache Maven，这是一个软件项目管理和理解工具。在写这本书的时候，Apache Maven 的 3.3.9 版本是最新的，我们鼓励读者使用这个版本。
2.  Go to [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi) and download a binary zip archive into your system:![Introduction](graphics/image_08_001.jpg)
3.  Once you download, unzip the file archive, and you will find a folder structure as follows:![Introduction](graphics/image_08_002.jpg)
4.  Now you need to put the path of `bin` folder in this distribution in your class path. To do so, right-click on your My Computer icon and click on **properties**. Click on **Advanced system settings** and then on **Environmental variables...:**![Introduction](graphics/image_08_003.jpg)
5.  When the **Environment Variables** window appears, go to **System variables** and select the variable named `Path`. Click on the **Edit...** button:![Introduction](graphics/image_08_004.jpg)
6.  When the **Edit environment variable** window appears, click on the **New** button and add the path to the `bin` folder of the Maven distribution. Click on **OK** to complete this action:![Introduction](graphics/image_08_005.jpg)
7.  Now that you are back to **Environment Variables** window, set up the JAVA_HOME system variable. In the **System variables** section, click on the **New** button:![Introduction](graphics/image_08_006.jpg)
8.  将变量命名为`JAVA_HOME`，并将路径放入您的 **Java 开发工具包** ( **JDK** )文件夹(记住，不是`bin`文件夹)。

    ### 注意

    请注意,您需要在您的系统上安装至少七的 Java 语言(一种计算机语言，尤用于创建网站)版本才能运行本章中的配方
9.  Click on **OK** to complete the command. Close windows opened along the way:![Introduction](graphics/image_08_007.jpg)
10.  Now check if Maven has been installed properly using the `mvn -v` command:![Introduction](graphics/image_08_008.jpg)
11.  Also, check the version of Java installed on your system with the `java -version` command:![Introduction](graphics/image_08_009.jpg)
12.  Now open the Eclipse IDE. The author has the Mars version installed. Go to **File**, then click on **New,** then click on **Other**...:![Introduction](graphics/image_08_010.jpg)
13.  In the Wizard, expand the Maven option and select the `Maven` Project. Click on **Next**:![Introduction](graphics/image_08_011.jpg)
14.  Keep clicking on **Next** until you reach the following window. In this window, fill out the **Group Id** and **Artifact Id** as follows or with anything you like. Click on **Finish**:![Introduction](graphics/image_08_012.jpg)
15.  This will create a project as follows. If you expand your project by double-clicking on its name, you will see an xml file named `POM.xml`:![Introduction](graphics/image_08_013.jpg)
16.  双击`pom.xml`文件，当它打开时，删除其所有内容，并复制粘贴以下内容到其中:

    ```
            <project   
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
              xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
            http://maven.apache.org/xsd/maven-4.0.0.xsd"> 
           <modelVersion>4.0.0</modelVersion> 

            <groupId>org.deeplearning4j</groupId> 
            <artifactId>deeplearning4j-examples</artifactId> 
            <version>0.4-rc0-SNAPSHOT</version> 

            <name>DeepLearning4j Examples</name> 
           <description>Examples of training different data   
              sets</description> 
           <properties> 
            <nd4j.version>0.4-rc3.7</nd4j.version> 
            <dl4j.version>  0.4-rc3.7</dl4j.version> 
            <canova.version>0.0.0.13</canova.version> 
            <jackson.version>2.5.1</jackson.version> 
           </properties> 

          <distributionManagement> 
            <snapshotRepository> 
                <id>sonatype-nexus-snapshots</id> 
                <name>Sonatype Nexus snapshot repository</name> 

         <url>https://oss.sonatype.org/content/repositories/snapshots</url> 
            </snapshotRepository> 
            <repository> 
                <id>nexus-releases</id> 
                <name>Nexus Release Repository</name> 

            <url>http://oss.sonatype.org/service/local/
                staging/deploy/maven2/</url> 
            </repository> 
             </distributionManagement> 
            <dependencyManagement> 
             <dependencies> 
                <dependency> 
                    <groupId>org.nd4j</groupId> 
                    <artifactId>nd4j-x86</artifactId> 
                    <version>${nd4j.version}</version> 
                </dependency> 
              </dependencies> 
             </dependencyManagement> 
            <dependencies> 
            <dependency> 
                <groupId>org.deeplearning4j</groupId> 
                <artifactId>deeplearning4j-nlp</artifactId> 
                <version>${dl4j.version}</version> 
            </dependency> 

            <dependency> 
                <groupId>org.deeplearning4j</groupId> 
                <artifactId>deeplearning4j-core</artifactId> 
                <version>${dl4j.version}</version> 
            </dependency> 
            <dependency> 
                <groupId>org.deeplearning4j</groupId> 
                <artifactId>deeplearning4j-ui</artifactId> 
                <version>${dl4j.version}</version> 
            </dependency> 
            <dependency> 
                <groupId>org.nd4j</groupId> 
                <artifactId>nd4j-x86</artifactId> 
                <version>${nd4j.version}</version> 
            </dependency> 
            <dependency> 
                <artifactId>canova-nd4j-image</artifactId> 
                <groupId>org.nd4j</groupId> 
                <version>${canova.version}</version> 
            </dependency> 
            <dependency> 
                <artifactId>canova-nd4j-codec</artifactId> 
                <groupId>org.nd4j</groupId> 
                <version>${canova.version}</version> 
            </dependency> 
            <dependency> 
                <groupId>com.fasterxml.jackson.dataformat</groupId> 
                <artifactId>jackson-dataformat-yaml</artifactId> 
                <version>${jackson.version}</version> 
            </dependency> 

           </dependencies> 

            <build> 
              <plugins> 
                <plugin> 
                    <groupId>org.codehaus.mojo</groupId> 
                    <artifactId>exec-maven-plugin</artifactId> 
                    <version>1.4.0</version> 
                    <executions> 
                        <execution> 
                            <goals> 
                                <goal>exec</goal> 
                            </goals> 
                        </execution> 
                    </executions> 
                    <configuration> 
                        <executable>java</executable> 
                    </configuration> 
                </plugin> 
                <plugin> 
                    <groupId>org.apache.maven.plugins</groupId> 
                    <artifactId>maven-shade-plugin</artifactId> 
                    <version>1.6</version> 
                    <configuration> 

            <createDependencyReducedPom>true</createDependencyReducedPom> 
                  <filters> 
                  <filter> 
                  <artifact>*:*</artifact> 
                  <excludes> 
                     <exclude>org/datanucleus/**</exclude> 
                      <exclude>META-INF/*.SF</exclude> 
                      <exclude>META-INF/*.DSA</exclude> 
                      <exclude>META-INF/*.RSA</exclude> 
                  </excludes> 
                            </filter> 
                        </filters> 
                    </configuration> 
                    <executions> 
                        <execution> 
                            <phase>package</phase> 
                            <goals> 
                                <goal>shade</goal> 
                            </goals> 
                            <configuration> 
                                <transformers> 
                 <transformer implementation="org.apache.maven.plugins.
                   shade.resource.AppendingTransformer"> 
                 <resource>reference.conf</resource> 
                                    </transformer> 
                 <transformer implementation="org.apache.maven.plugins.
                         shade.resource.ServicesResourceTransformer"/> 
                 <transformer implementation="org.apache.maven.plugins.
                         shade.resource.ManifestResourceTransformer"> 
                                    </transformer> 
                                </transformers> 
                            </configuration> 
                        </execution> 
                    </executions> 
                </plugin> 

                <plugin> 
                    <groupId>org.apache.maven.plugins</groupId> 
                    <artifactId>maven-compiler-plugin</artifactId> 
                    <configuration> 
                        <source>1.7</source> 
                        <target>1.7</target> 
                    </configuration> 
                  </plugin> 
                </plugins> 
              </build> 
             </project> 

    ```

17.  This will download all the necessary dependencies (see the following screenshot for a partial picture), and you are ready to create some code:![Introduction](graphics/image_08_014.jpg)
18.  进入[https://github . com/deep learning 4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources](https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources)下载`raw_sentences.txt`文件到你的`C:/ drive`:

![Introduction](graphics/image_08_015.jpg)<title>Creating a Word2vec neural net using Deep Learning for Java (DL4j)</title>   <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 使用面向 Java 的深度学习(DL4j)创建 Word2vec 神经网络

Word2vec 可以看作是一个处理自然文本的两层神经网络。根据其典型用法，该算法的输入可以是文本语料库，其输出是该语料库中单词的一组特征向量。注意，严格地说，Word2vec 不是深度神经网络，因为它将文本翻译成深度神经网络可以读取和理解的数字形式。在这个菜谱中，我们将看到如何使用流行的名为 deep learning for Java 的深度学习 Java 库(从现在开始，DL4j)将 Word2vec 应用于原始文本。

## 怎么做...

1.  创建一个名为`Word2VecRawTextExample` :

    ```
            public class Word2VecRawTextExample { 

    ```

    的类
2.  为此类创建一个记录器。logger 工具已经包含在您的项目中，因为您已经使用 Maven 构建了您的项目:

    ```
            private static Logger log = 
              LoggerFactory.getLogger(Word2VecRawTextExample.class); 

    ```

3.  Start creating your main method:

    `<dependency><groupId>org.nd4j</groupId><artifactId>nd4j-native</artifactId><version>0.7.2</version></dependency>`

    ```
            public static void main(String[] args) throws Exception { 

    ```

4.  您要做的第一件事是获取您已经下载的示例原始句子文本文件的文件路径:

    ```
            String filePath = "c:/raw_sentences.txt"; 

    ```

5.  现在获取`.txt`文件中的原始句子，用迭代器遍历它们，并对它们进行预处理(例如，将所有内容转换成小写，去掉每行前后的空格):

    ```
            log.info("Load & Vectorize Sentences...."); 
            SentenceIterator iter = 
              UimaSentenceIterator.createWithPath(filePath); 

    ```

6.  Word2vec 使用单词或记号，而不是句子。因此，您的下一个任务将是对原始文本进行标记:

    ```
           TokenizerFactory t = new DefaultTokenizerFactory(); 
            t.setTokenPreProcessor(new CommonPreprocessor()); 

    ```

7.  词汇表缓存或 Vocab 缓存是 DL4j 中的一种机制，用于处理通用自然语言任务，如 TF-IDF。`InMemoryLookupCache`是参考实现:

    ```
            InMemoryLookupCache cache = new InMemoryLookupCache(); 
              WeightLookupTable table = new InMemoryLookupTable.Builder() 
                    .vectorLength(100) 
                    .useAdaGrad(false) 
                    .cache(cache) 
                   .lr(0.025f).build(); 

    ```

8.  Now that the data is ready, you are also ready to configure the `Word2vec` neural network:

    ```
            log.info("Building model...."); 
             Word2Vec vec = new Word2Vec.Builder() 
                  .minWordFrequency(5).iterations(1) 
                  .layerSize(100).lookupTable(table) 
                  .stopWords(new ArrayList<String>()) 
                  .vocabCache(cache).seed(42) 
                  .windowSize(5).iterate(iter).tokenizerFactory(t).build(); 

    ```

    minWordFrequency 是一个单词在语料库中必须出现的最小次数。在这个食谱中，如果出现少于五次，一个单词就没有学会。单词必须出现在多种上下文中，以了解关于它们的有用特征。如果你有一个非常大的语料库，提高最小值是合理的。LayerSize 表示单词向量中的特征数量或特征空间中的维数。接下来，通过开始神经网络训练来拟合模型:

    ```
            log.info("Fitting Word2Vec model...."); 
            vec.fit(); 

    ```

9.  将从神经网络创建的单词向量写入输出文件。在我们的例子中，输出被写到名为`c:/word2vec.txt` :

    ```
            <dependency><groupId>org.nd4j</groupId><artifactId>nd4j- 
              native</artifactId><version>0.7.2</version></dependency>
            log.info("Writing word vectors to text file...."); 
            WordVectorSerializer.writeWordVectors(vec, "c:/word2vec.txt"); 

    ```

    的文件中
10.  您还可以评估特征向量的质量。`vec.wordsNearest("word1", numWordsNearest)`为我们提供了被神经网络聚类为语义相似词的词。您可以使用 wordsNearest 的第二个参数设置所需的最近单词数。`vec.similarity("word1","word2")`会返回你输入的两个词的余弦相似度。它越接近 1，网络对这些单词的感知就越相似:

    ```
            log.info("Closest Words:"); 
            Collection<String> lst = vec.wordsNearest("man", 5);   
            System.out.println(lst); 
            double cosSim = vec.similarity("cruise", "voyage"); 
            System.out.println(cosSim); 

    ```

11.  前面几行的输出如下:

    ```
            [family, part, house, program, business] 
            1.0000001192092896 

    ```

12.  关闭 main 方法和类:

```
        } 
        } 

```

## 它是如何工作的...

1.  Right-click on your project name in Eclipse, select **New,** and then select Package. Give the following as your package name: `word2vec.chap8.science.data`. Click on **Finish**:![How it works...](graphics/image_08_016.jpg)
2.  现在你有了一个包，右击包名，选择**新建**，然后选择**类**。类的名字应该是`Word2VecRawTextExample`。点击**完成**:

![How it works...](graphics/image_08_017.jpg)

在编辑器中，复制并粘贴以下代码:

```
package word2vec.chap8.science.data; 

import org.deeplearning4j.models.embeddings.WeightLookupTable; 
import org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable; 
import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer; 
import org.deeplearning4j.models.word2vec.Word2Vec; 
import org.deeplearning4j.models.word2vec.wordstore.inmemory.InMemoryLookupCache; 
import org.deeplearning4j.text.sentenceiterator.SentenceIterator; 
import org.deeplearning4j.text.sentenceiterator.UimaSentenceIterator; 
import org.deeplearning4j.text.tokenization.tokenizer.preprocessor.CommonPreprocessor; 
import org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory; 
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory; 

import org.slf4j.Logger; 
import org.slf4j.LoggerFactory; 

import java.util.ArrayList; 
import java.util.Collection; 

public class Word2VecRawTextExample { 

    private static Logger log = LoggerFactory.getLogger(Word2VecRawTextExample.class); 

    public static void main(String[] args) throws Exception { 

        // Gets Path to Text file 
        String filePath = "c:/raw_sentences.txt"; 

        log.info("Load & Vectorize Sentences...."); 
        // Strip white space before and after for each line 
        SentenceIterator iter = 
           UimaSentenceIterator.createWithPath(filePath); 
        // Split on white spaces in the line to get words 
        TokenizerFactory t = new DefaultTokenizerFactory(); 
        t.setTokenPreProcessor(new CommonPreprocessor()); 

        InMemoryLookupCache cache = new InMemoryLookupCache(); 
        WeightLookupTable table = new InMemoryLookupTable.Builder() 
                .vectorLength(100) 
                .useAdaGrad(false) 
                .cache(cache) 
                .lr(0.025f).build(); 

        log.info("Building model...."); 
        Word2Vec vec = new Word2Vec.Builder() 
            .minWordFrequency(5).iterations(1) 
            .layerSize(100).lookupTable(table) 
            .stopWords(new ArrayList<String>()) 
            .vocabCache(cache).seed(42) 

            .windowSize(5).iterate(iter).tokenizerFactory(t).build(); 

        log.info("Fitting Word2Vec model...."); 
        vec.fit(); 

        log.info("Writing word vectors to text file...."); 
        // Write word 
        WordVectorSerializer.writeWordVectors(vec, "word2vec.txt"); 

        log.info("Closest Words:"); 
        Collection<String> lst = vec.wordsNearest("man", 5); 
        System.out.println(lst); 
        double cosSim = vec.similarity("cruise", "voyage"); 
        System.out.println(cosSim); 
    } 
} 

```

## 还有更多

*   `minWordFrequency`:是一个词在语料库中必须出现的最小次数。在这个食谱中，如果出现少于五次，一个单词就没有学会。单词必须出现在多种上下文中，以了解关于它们的有用特征。如果你有非常大的语料库，提高最小值是合理的。
*   `iterations`:这是您允许神经网络为一批数据更新其系数的次数。迭代次数太少会造成学习不充分，迭代次数太多会使网的训练时间变长。
*   `layerSize`:表示单词向量中的特征数或特征空间中的维数。
*   iterate 告诉网络它正在数据集的哪一批上进行训练。
*   `tokenizer`:输入当前批次的单词。



# 使用面向 Java 的深度学习(DL4j)创建深度信念神经网络

深度信念网络可以定义为一堆受限的玻尔兹曼机器，其中每个 RBM 层都与前一层和后一层进行通信。在这份食谱中，我们将看到如何创建这样一个网络。为了简单起见，在这个食谱中，我们把自己限制在神经网络的一个单独的隐藏层。因此，我们在这个食谱中开发的网络严格来说并不是一个深度信念神经网络，而是鼓励读者添加更多的隐藏层。

## 怎么做...

1.  创建一个名为`DBNIrisExample` :

    ```
            public class DBNIrisExample { 

    ```

    的类
2.  为该类创建一个日志记录器来记录消息:

    ```
            private static Logger log = 
              LoggerFactory.getLogger(DBNIrisExample.class); 

    ```

3.  开始写你的主要方法:

    ```
            public static void main(String[] args) throws Exception { 

    ```

4.  首先，定制 Nd4j 类的两个参数:要打印的最大切片数和每个切片的最大元素数。将它们设置为`-1` :

    ```
            Nd4j.MAX_SLICES_TO_PRINT = -1; 
            Nd4j.MAX_ELEMENTS_PER_SLICE = -1; 

    ```

5.  接下来，自定义其他参数:

    ```
          final int numRows = 4; 
           final int numColumns = 1; 
            int outputNum = 3; 
            int numSamples = 150; 
            int batchSize = 150; 
            int iterations = 5; 
            int splitTrainNum = (int) (batchSize * .8); 
            int seed = 123; 
            int listenerFreq = 1; 

    ```

    *   在 DL4j 中，输入的数据可以是二维数据，因此，需要指定数据的行数和列数。因为虹膜数据集是一维的，所以列数被设置为 1。
    *   代码中，`numSamples`为总数据量，batchSize 为每批数据量。
    *   splitTrainNum 是为训练和测试分配数据的变量。这里，所有数据集的 80%是训练数据，其余的被视为测试数据。listenerFreq 决定了我们在日志记录过程中看到损失函数值的频率。此处该值设置为 1，这意味着在每个时期后记录该值。

6.  使用以下代码自动加载 Iris 数据集，其中包含批次大小和样本数量信息:

    ```
            log.info("Load data...."); 
            DataSetIterator iter = new IrisDataSetIterator(batchSize,  
              numSamples); 

    ```

7.  格式化数据:

    ```
            DataSet next = iter.next(); 
              next.normalizeZeroMeanZeroUnitVariance();
    ```

8.  接下来，将数据分为训练和测试。对于分裂，使用随机种子并加强数值稳定性进行训练:

    ```
           log.info("Split data...."); 
            SplitTestAndTrain testAndTrain = 
               next.splitTestAndTrain(splitTrainNum, new Random(seed)); 
            DataSet train = testAndTrain.getTrain(); 
            DataSet test = testAndTrain.getTest(); 
            Nd4j.ENFORCE_NUMERICAL_STABILITY = true; 

    ```

9.  现在，写下下面的代码块来构建你的模型:

    ```
             MultiLayerConfiguration conf = new 
                NeuralNetConfiguration.Builder() 
                .seed(seed) 
                .iterations(iterations) 
                .learningRate(1e-6f) 
               .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) 
                .l1(1e-1).regularization(true).l2(2e-4) 
                .useDropConnect(true) 
                .list(2) 

    ```

10.  Let's examine this piece of code:

    *   使用种子方法，您可以锁定权重初始化以进行调整
    *   然后，设置预测或分类的训练迭代次数
    *   然后定义优化步长，并选择反向传播算法来计算梯度
    *   最后，在 list()方法中，提供 2 作为神经网络层数的参数(除了输入层之外)

11.  然后将下面的方法调用添加到上一步中的代码中。这段代码是为你的神经网络设置第一层:

    ```
             .layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, 
                 RBM.VisibleUnit.GAUSSIAN) 

                .nIn(numRows * numColumns) 

                .nOut(3) 
               .weightInit(WeightInit.XAVIER) 
                .k(1) 
               .activation("relu") 
               .lossFunction(LossFunctions.LossFunction.RMSE_XENT) 
               .updater(Updater.ADAGRAD) 
                .dropOut(0.5) 
               .build() 
              )
    ```

    *   第一行的 0 值是该层的索引
    *   k()是对比散度
    *   而不是二进制 RBM，在这种情况下我们不能使用，因为虹膜数据是浮点值，我们有 RBM。VisibleUnit.GAUSSIAN，使模型能够处理连续值
    *   Updater。ADAGRAD 用于优化学习速率

12.  然后将下面的方法调用添加到上一步中的代码中。这段代码是为你的神经网络设置第一层:

    ```
            .layer(1, new 
               OutputLayer.Builder(LossFunctions.LossFunction.MCXENT) 

                .nIn(3) 

                .nOut(outputNum) 
                .activation("softmax") 
                .build() 
            )  .build(); 

    ```

13.  最终确定模型构建:

    ```
            MultiLayerNetwork model = new MultiLayerNetwork(conf); 
            model.init(); 

    ```

14.  一旦模型被配置，完成它的训练:

    ```
            model.setListeners(Arrays.asList((IterationListener) new 
              ScoreIterationListener(listenerFreq))); 
            log.info("Train model...."); 
            model.fit(train); 

    ```

15.  您可以使用下面的代码评估权重:

    ```
            log.info("Evaluate weights...."); 
            for(org.deeplearning4j.nn.api.Layer layer : model.getLayers()) 
            { 
               INDArray w = 
                layer.getParam(DefaultParamInitializer.WEIGHT_KEY); 
              log.info("Weights: " + w); 
            } 

    ```

16.  最后，评价一下你的模型:

    ```
            log.info("Evaluate model...."); 
             Evaluation eval = new Evaluation(outputNum); 
             INDArray output = model.output(test.getFeatureMatrix()); 

             for (int i = 0; i < output.rows(); i++) { 
                 String actual = 
                    test.getLabels().getRow(i).toString().trim(); 
                 String predicted = output.getRow(i).toString().trim(); 
                 log.info("actual " + actual + " vs predicted " + 
                    predicted); 
             } 

             eval.eval(test.getLabels(), output); 
             log.info(eval.stats()); 

    ```

17.  这段代码的输出如下:

    ```
     ========================Scores================================== 
            Accuracy:  0.8333 
            Precision: 1 
            Recall:    0.8333 
            F1 Score:  0.9090909090909091 

    ```

18.  最后，关闭 main 方法和类:

```
        } 
        } 

```

## 它是如何工作的...

1.  在 Eclipse 中右键单击您的项目名称，选择 New，然后选择 Package。给出下面的包名:`deepbelief.chap8.science.data`。点击**完成**。
2.  现在您已经有了一个包，右键单击包名，选择 New，然后选择 Class。类的名字应该是`DBNIrisExample`。点击**完成**。

在编辑器中，复制并粘贴以下代码:

```
package deepbelief.chap8.science.data; 

import org.deeplearning4j.datasets.iterator.DataSetIterator; 
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator; 
import org.deeplearning4j.eval.Evaluation; 
import org.deeplearning4j.nn.api.OptimizationAlgorithm; 
import org.deeplearning4j.nn.conf.MultiLayerConfiguration; 
import org.deeplearning4j.nn.conf.NeuralNetConfiguration; 
import org.deeplearning4j.nn.conf.Updater; 
import org.deeplearning4j.nn.conf.layers.OutputLayer; 
import org.deeplearning4j.nn.conf.layers.RBM; 
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork; 
import org.deeplearning4j.nn.params.DefaultParamInitializer; 
import org.deeplearning4j.nn.weights.WeightInit; 
import org.deeplearning4j.optimize.api.IterationListener; 
import org.deeplearning4j.optimize.listeners.ScoreIterationListener; 
import org.nd4j.linalg.api.ndarray.INDArray; 
import org.nd4j.linalg.dataset.DataSet; 
import org.nd4j.linalg.dataset.SplitTestAndTrain; 
import org.nd4j.linalg.factory.Nd4j; 
import org.nd4j.linalg.lossfunctions.LossFunctions; 
import org.slf4j.Logger; 
import org.slf4j.LoggerFactory; 
import java.util.Arrays; 
import java.util.Random; 

public class DBNIrisExample { 

    private static Logger log = 
      LoggerFactory.getLogger(DBNIrisExample.class); 

    public static void main(String[] args) throws Exception { 
        Nd4j.MAX_SLICES_TO_PRINT = -1; 
        Nd4j.MAX_ELEMENTS_PER_SLICE = -1; 

        final int numRows = 4; 
        final int numColumns = 1; 
        int outputNum = 3; 
        int numSamples = 150; 
        int batchSize = 150; 
        int iterations = 5; 
        int splitTrainNum = (int) (batchSize * .8); 
        int seed = 123; 
        int listenerFreq = 1; 

        log.info("Load data...."); 
        DataSetIterator iter = new IrisDataSetIterator(batchSize, 
          numSamples); 
        DataSet next = iter.next(); 
        next.normalizeZeroMeanZeroUnitVariance(); 

        log.info("Split data...."); 
        SplitTestAndTrain testAndTrain = 
           next.splitTestAndTrain(splitTrainNum, new Random(seed)); 
        DataSet train = testAndTrain.getTrain(); 
        DataSet test = testAndTrain.getTest(); 
        Nd4j.ENFORCE_NUMERICAL_STABILITY = true; 

        log.info("Build model...."); 
        MultiLayerConfiguration conf = new 
          NeuralNetConfiguration.Builder() 
           .seed(seed) 
           .iterations(iterations) 
           .learningRate(1e-6f) 
           .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) 
           .l1(1e-1).regularization(true).l2(2e-4) 
           .useDropConnect(true) 
           .list(2) 
           .layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, 
             RBM.VisibleUnit.GAUSSIAN) 
            .nIn(numRows * numColumns) 
            .nOut(3) 
            .weightInit(WeightInit.XAVIER) 
            .k(1) 
            .activation("relu") 
            .lossFunction(LossFunctions.LossFunction.RMSE_XENT) 
            .updater(Updater.ADAGRAD) 
            .dropOut(0.5) 
            .build() 
          ) 
          .layer(1, new 
             OutputLayer.Builder(LossFunctions.LossFunction.MCXENT) 
            .nIn(3) 
            .nOut(outputNum) 
            .activation("softmax") 
            .build() 
        ) 
        .build(); 
        MultiLayerNetwork model = new MultiLayerNetwork(conf); 
        model.init(); 

        model.setListeners(Arrays.asList((IterationListener) new 
           ScoreIterationListener(listenerFreq))); 
        log.info("Train model...."); 
        model.fit(train); 

        log.info("Evaluate weights...."); 
        for(org.deeplearning4j.nn.api.Layer layer : model.getLayers()) 
        { 
            INDArray w = 
            layer.getParam(DefaultParamInitializer.WEIGHT_KEY); 
            log.info("Weights: " + w); 
        } 

        log.info("Evaluate model...."); 
        Evaluation eval = new Evaluation(outputNum); 
        INDArray output = model.output(test.getFeatureMatrix()); 

        for (int i = 0; i < output.rows(); i++) { 
            String actual = 
              test.getLabels().getRow(i).toString().trim(); 
            String predicted = output.getRow(i).toString().trim(); 
            log.info("actual " + actual + " vs predicted " + 
              predicted); 
        } 

        eval.eval(test.getLabels(), output); 
        log.info(eval.stats()); 

    } 
} 

```



# 使用面向 Java 的深度学习(DL4j)创建深度自动编码器

深度自动编码器是由两个对称的深度信任网络组成的深度神经网络。网络通常有两个独立的四或五个浅层(受限玻尔兹曼机器)，代表网络的编码和解码部分。在这个菜谱中，您将开发一个深度自动编码器，由一个输入层、四个解码层、四个编码层和一个输出层组成。为此，我们将使用一个非常受欢迎的数据集，名为 MNIST。

### 注意

要了解更多关于 MNIST 的信息，请访问 http://yann.lecun.com/exdb/mnist/。如果你想了解更多关于深度自动编码器的信息，请访问 https://deeplearning4j.org/deepautoencoder。来完成命令。关闭沿途打开的窗户。命令。命令。并点击**其他**...直到你到达下面的窗口。在该窗口中，按照如下方式填写**组 Id** 和**工件 Id** ，或者填写您喜欢的任何内容。点击**完成**。

## 怎么做...

1.  首先创建一个名为`DeepAutoEncoderExample` :

    ```
            public class DeepAutoEncoderExample { 

    ```

    的类
2.  在整个代码中，您将记录消息。因此，为您的类创建一个日志记录器:

    ```
            private static Logger log = 
               LoggerFactory.getLogger(DeepAutoEncoderExample.class); 

    ```

3.  开始编写你的`main`方法:

    ```
            public static void main(String[] args) throws Exception {
    ```

4.  在您的主方法的最开始，定义一些需要更改或配置的参数:

    ```
            final int numRows = 28; 
            final int numColumns = 28; 
            int seed = 123; 
            int numSamples = MnistDataFetcher.NUM_EXAMPLES; 
            int batchSize = 1000; 
            int iterations = 1; 
            int listenerFreq = iterations/5;
    ```

    *   行和列被设置为 28，因为 MNIST 数据库中的图像大小是 28×28 像素
    *   随机选择 123 个种子
    *   numSamples 是示例数据集中的样本总数
    *   batchSize 被设置为 1000，以便每次
    *   `listenerFreq`决定时使用 1000 个数据样本

5.  然后，将批次大小和样本数量信息加载到 MNIST 数据点:

    ```
          log.info("Load data...."); 
          DataSetIterator iter = new 
             MnistDataSetIterator(batchSize,numSamples,true); 

    ```

6.  接下来，您将配置神经网络。首先，使用种子、迭代并通过将线梯度下降设置为优化算法来构建多层神经网络。您还设置了总共 10 层:一个输入层、四个编码层、一个解码层和一个输出层:

    ```
            log.info("Build model...."); 
            MultiLayerConfiguration conf = new 
             NeuralNetConfiguration.Builder() 
            .seed(seed) 
            .iterations(iterations) 
            .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT) 
            .list(10) 

    ```

7.  然后将下面的代码添加到上一步的代码中。这是你创建所有 10 个反向传播层的地方:

    ```
            .layer(0, new RBM.Builder().nIn(numRows * 
               numColumns).nOut(1000).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(1, new RBM.Builder().nIn(1000).nOut(500).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(2, new RBM.Builder().nIn(500).nOut(250).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(3, new RBM.Builder().nIn(250).nOut(100).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(4, new RBM.Builder().nIn(100).nOut(30).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build())
                  //encoding 
                    stops 
           .layer(5, new RBM.Builder().nIn(30).nOut(100).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build())              
                 //decoding 
                    starts 
           .layer(6, new RBM.Builder().nIn(100).nOut(250).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(7, new RBM.Builder().nIn(250).nOut(500).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(8, new RBM.Builder().nIn(500).nOut(1000).lossFunction
                 (LossFunctions.LossFunction.RMSE_XENT).build()) 
           .layer(9, new OutputLayer.Builder(LossFunctions.
               LossFunction.RMSE_XENT).nIn(1000).nOut(numRows*numColumns).
                   build()) 
           .pretrain(true).backprop(true) .build(); 

    ```

8.  现在您已经配置了模型，初始化它:

    ```
            MultiLayerNetwork model = new MultiLayerNetwork(conf); 
            model.init(); 

    ```

9.  最终确定培训:

    ```
            model.setListeners(Arrays.asList((IterationListener) new    
            ScoreIterationListener(listenerFreq))); 
             log.info("Train model...."); 
              while(iter.hasNext()) { 
             DataSet next = iter.next(); 
              model.fit(new   
               DataSet(next.getFeatureMatrix(),next.getFeatureMatrix())); 
            } 

    ```

10.  最后，关闭`main`方法和类:

```
       } 
       } 

```

## 它是如何工作的...

1.  在 Eclipse 中右键单击您的项目名称，选择 New，然后选择 Package。给出下面的包名:`deepbelief.chap8.science.data`。点击**完成**。
2.  现在您已经有了一个包，右击包名，选择 **New** ，然后选择 Class。班级的名字应该是`DeepAutoEncoderExample.`点击**完成**。

在编辑器中，复制并粘贴以下代码:

```
package deepbelief.chap8.science.data; 
import org.deeplearning4j.datasets.fetchers.MnistDataFetcher; 
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator; 
import org.deeplearning4j.nn.api.OptimizationAlgorithm; 
import org.deeplearning4j.nn.conf.MultiLayerConfiguration; 
import org.deeplearning4j.nn.conf.NeuralNetConfiguration; 
import org.deeplearning4j.nn.conf.layers.OutputLayer; 
import org.deeplearning4j.nn.conf.layers.RBM; 
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork; 
import org.deeplearning4j.optimize.api.IterationListener; 
import org.deeplearning4j.optimize.listeners.ScoreIterationListener; 
import org.nd4j.linalg.dataset.DataSet; 
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator; 
import org.nd4j.linalg.lossfunctions.LossFunctions; 
import org.slf4j.Logger; 
import org.slf4j.LoggerFactory; 
import java.util.Arrays; 

public class DeepAutoEncoderExample { 
     private static Logger log = 
       LoggerFactory.getLogger(DeepAutoEncoderExample.class); 

    public static void main(String[] args) throws Exception { 
        final int numRows = 28; 
        final int numColumns = 28; 
        int seed = 123; 
        int numSamples = MnistDataFetcher.NUM_EXAMPLES; 
        int batchSize = 1000; 
        int iterations = 1; 
        int listenerFreq = iterations/5; 

        log.info("Load data...."); 
        DataSetIterator iter = new 
          MnistDataSetIterator(batchSize,numSamples,true); 

        log.info("Build model...."); 
        MultiLayerConfiguration conf = new 
          NeuralNetConfiguration.Builder() 
                .seed(seed) 
                .iterations(iterations) 

       .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT) 
                .list(10)

       .layer(0, new RBM.Builder().nIn(numRows * 
           numColumns).nOut(1000).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(1, new RBM.Builder().nIn(1000).nOut(500).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(2, new RBM.Builder().nIn(500).nOut(250).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(3, new RBM.Builder().nIn(250).nOut(100).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(4, new RBM.Builder().nIn(100).nOut(30).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
             //encoding 
                stops 
       .layer(5, new RBM.Builder().nIn(30).nOut(100).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build())
             //decoding 
                starts 
       .layer(6, new RBM.Builder().nIn(100).nOut(250).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(7, new RBM.Builder().nIn(250).nOut(500).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(8, new RBM.Builder().nIn(500).nOut(1000).lossFunction
             (LossFunctions.LossFunction.RMSE_XENT).build()) 
       .layer(9, new OutputLayer.Builder(LossFunctions.
           LossFunction.RMSE_XENT).nIn(1000).nOut(numRows*numColumns).
               build()) 
       .pretrain(true).backprop(true) .build(); 

        MultiLayerNetwork model = new MultiLayerNetwork(conf); 
        model.init(); 

        model.setListeners(Arrays.asList((IterationListener) new 
           ScoreIterationListener(listenerFreq))); 

        log.info("Train model...."); 
        while(iter.hasNext()) { 
            DataSet next = iter.next(); 
            model.fit(new 
          DataSet(next.getFeatureMatrix(),next.getFeatureMatrix())); 
        } 
    } 
} 
```