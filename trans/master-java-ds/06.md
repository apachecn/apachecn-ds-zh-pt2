

# 六、使用文本——自然语言处理和信息检索

在前两章中，我们讨论了机器学习的基础知识:我们谈到了监督和非监督问题。

在这一章中，我们将看看如何使用这些方法来处理文本信息，我们将用我们正在运行的例子来说明我们的大部分想法:构建一个搜索引擎。这里，我们将最终使用来自 HTML 的文本信息，并将其包含到机器学习模型中。

首先，我们将从自然语言处理的基础开始，自己实现一些基本思想，然后研究 NLP 库中可用的高效实现。

本章涵盖以下主题:

*   信息检索基础
*   使用 Apache Lucene 进行索引和搜索
*   自然语言处理基础
*   文本的无监督模型——降维、聚类和单词嵌入
*   文本的监督模型——文本分类和排序学习

本章结束时，你将学会如何为机器学习做简单的文本预处理，如何使用 Apache Lucene 进行索引，如何将单词转换成向量，以及如何对文本进行聚类和分类。



# 自然语言处理和信息检索

**自然语言处理** ( **NLP** )是计算机科学和计算语言学的一部分，处理文本数据。对于计算机来说，文本是非结构化的，自然语言处理有助于找到结构并从中提取有用的信息。

**信息检索** ( **IR** )是一门研究在大型非结构化数据集中搜索的学科。典型地，这些数据集是文本，并且 IR 系统帮助用户找到他们想要的。像 Google 或 Bing 这样的搜索引擎就是这种 IR 系统的例子:它们接受一个查询，并提供一个根据与该查询的相关性排序的文档集合。

通常，信息检索系统使用自然语言处理来理解文档的内容——因此，当用户需要时，可以检索这些文档。在这一章中，我们将复习用于信息检索的文本处理的基础知识。



# 向量空间模型-单词袋和 TF-IDF

对计算机来说，文本只是一串没有特定结构的字符。因此，我们称文本**为非结构化数据**。然而，对人类来说，文本当然有一个结构，我们用它来理解内容。IR 和 NLP 模型试图做的事情是相似的:它们找到文本中的结构，用它来提取那里的信息，并理解文本是关于什么的。

实现它的最简单的可能方式被称为**单词包**:我们获取一个文本，将其分割成单个单词(我们称之为**记号**，然后将该文本表示为一个无序的记号集合以及与每个记号相关联的一些权重。

让我们考虑一个例子。如果我们取一个文档，它由一个句子组成(*我们使用 Java 进行数据科学，因为我们喜欢 Java)* ，它可以表示如下:

```
(because, 1), (data, 1), (for, 1), (java, 2), (science, 1), (use, 1), (we, 2)

```

在这里，句子中的每个单词都根据该单词出现的次数进行加权。

现在，当我们能够以这种方式表示文档时，我们可以用它来比较一个文档和另一个文档。

例如，如果我们取另一个句子如 *Java 是好的企业开发*，我们可以表示如下:

```
(development, 1), (enterprise, 1), (for, 1), (good, 1), (java, 1)

```

我们可以看到这两个文档之间有一些交集，这可能意味着这两个文档是相似的，交集越高，文档越相似。

现在，如果我们认为单词是某个向量空间中的维度，权重是这些维度的值，那么我们可以将文档表示为向量:

![](img/bow-vectors.png)

如果我们采用这种矢量表示，我们可以用两个矢量之间的内积作为相似性的度量。的确，如果两个文档有很多共同的词，它们之间的内积就会很高，如果它们没有共享文档，内积就是零。

这个想法被称为**向量空间模型**，这是在许多信息检索系统中使用的:所有文档以及用户查询都被表示为向量。一旦查询和文档在同一个空间，我们可以把查询和文档之间的相似性看作它们之间的相关性。因此，我们根据文档与用户查询的相似性对文档进行排序。

从原始文本到矢量包含几个步骤。通常，它们如下:

*   首先，我们对文本进行标记化，也就是说，将它转换成单个标记的集合。
*   然后，我们去掉 is、will、to 等虚词。它们通常仅用于链接目的，没有任何重要意义。这些词被称为停用词。
*   有时我们也会将令牌转换成某种范式。例如，我们可能希望将 cat 和 cats 映射到 cat，因为这两个不同单词背后的概念是相同的。这是通过词干化或词汇化实现的。
*   最后，我们计算每个标记的权重，并将它们放入向量空间。

以前，我们使用出现的次数来加权术语；这被称为术语频率加权。然而，有些词比其他词更重要，术语频率并不总是能捕捉到这一点。

比如*锤子*可以比*工具*更重要，因为它更具体。逆文档频率是一种不同的加权方案，它惩罚一般的单词而支持特定的单词。在内部，它基于包含该术语的文档的数量，其思想是更具体的术语出现在比一般术语更少的文档中。

最后是词频和逆文档频的组合，缩写为 TF-IDF。顾名思义，令牌`t`的权重由两部分组成:TF 和 IDF:

```
weight(t) = tf(t) * idf(t)

```

下面是对前面等式中提到的术语的解释:

*   `tf(t)`:这是令牌`t`在文本中出现次数的函数
*   `idf(t)`:这是包含令牌的文档数量的函数

定义这些函数有多种方法，但最常见的是使用以下定义:

*   `tf(t)`:这是`t`在文档中出现的次数
*   `idf(t) = log(N / df(t))`:此处`df(t)`为文件数，包含`t`，`N`为文件总数

以前，我们建议可以使用内积来度量文档之间的相似性。这种方法有一个问题:它是无界的，这意味着它可以接受任何正值，这使得它更难解释。此外，较长的文档往往与其他所有文档具有更高的相似性，因为它们包含更多的单词。

这个问题的解决方案是对向量内部的权重进行归一化，使其范数变为 1。然后，计算内积总会得到一个介于 0 和 1 之间的有界值，较长的文档影响较小。归一化向量之间的内积通常称为*余弦相似度*，因为它对应于这两个向量在向量空间中形成的角度的余弦。



# 向量空间模型实现

现在我们已经有了足够的背景信息，可以开始编写代码了。

首先，假设我们有一个文本文件，其中每一行都是一个文档，我们希望索引这个文件的内容并能够查询它。比如我们可以从[的 https://OCW . MIT . edu/ans 7870/6/6.006/s08/lecture notes/files/t8 . Shakespeare . txt](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt)中取一些文本保存到`simple-text.txt`。

那么我们可以这样理解:

```
Path path = Paths.get("data/simple-text.txt");
List<List<String>> documents = Files.lines(path, StandardCharsets.UTF_8)
     .map(line -> TextUtils.tokenize(line))
     .map(line -> TextUtils.removeStopwords(line))
     .collect(Collectors.toList());

```

我们使用标准库中的`Files`类，然后使用两个函数:

*   第一个是`TextUtils.tokenize`，它接受一个字符串并生成一个令牌列表
*   第二个是`TextUtils.removeStopwords`，删除了 a、The 等功能词

实现标记化的一个简单而天真的方法是根据正则表达式拆分字符串:

```
public static List<String> tokenize(String line) {
     Pattern pattern = Pattern.compile("W+");
     String[] split = pattern.split(line.toLowerCase());
     return Arrays.stream(split)
             .map(String::trim)
             .filter(s -> s.length() > 2)
             .collect(Collectors.toList());
 }

```

表达式`W+`的意思是在所有非拉丁字符上分割字符串。当然，它将无法处理包含非拉丁字符的语言，但这是实现标记化的一种快速方法。此外，它对英语也很有效，并且可以适用于其他欧洲语言。

这里的另一件事是丢弃小于两个字符的短标记——这些标记通常是停用词，所以丢弃它们是安全的。第二个函数获取一个标记列表，并从中删除所有停用词。下面是它的实现:

```
Set<String> EN_STOPWORDS = ImmutableSet.of("a", "an", "and", "are", "as", "at", "be", ... 
public static List<String> removeStopwords(List<String> line) {
     return line.stream()
             .filter(token -> !EN_STOPWORDS.contains(token))
             .collect(Collectors.toList());
 }

```

这非常简单:我们保存一组英语停用词，然后对于每个标记，我们只需检查它是否在这个集合中。你可以从 http://www.ranks.nl/stopwords 那里得到一份很好的英语停用词清单。

向这个管道中添加令牌规范化也很容易。现在，我们将跳过它，但是我们将在本章的后面回到它。

现在我们已经标记了文本，所以下一步是在向量空间中表示标记。让我们为它创建一个特殊的类。我们称之为`CountVectorizer`。

名称`CountVectorizer`的灵感来自 scikit-learn 中一个具有类似功能的类，sci kit-learn 是一个用 Python 进行机器学习的优秀包。如果你熟悉这个库，你可能会注意到我们有时会借用那里的名字(比如方法的名字`fit()`和`transform()`)。

因为我们不能直接创建一个向量空间，它的维度由单词索引，我们将首先把所有文本中的所有不同的标记映射到某个列号。

此外，在这一步计算文档频率是有意义的，并使用它来丢弃只出现在少数文档中的标记。通常，这样的术语是拼写错误、不存在的单词或者过于罕见而对结果没有任何影响。

在代码中，它看起来像这样:

```
Multiset<String> df = HashMultiset.create();
documents.forEach(list -> df.addAll(Sets.newHashSet(list)));
Multiset<String> docFrequency = Multisets.filter(df, p -> df.count(p) >= minDf);

List<String> vocabulary = Ordering.natural().sortedCopy(docFrequency.elementSet());
Map<String, Integer> tokenToIndex = new HashMap<>(vocabulary.size());

for (int i = 0; i < vocabulary.size(); i++) {
    tokenToIndex.put(vocabulary.get(i), i);
}

```

我们使用来自 Guava 的一个`Multiset`来计算文档频率，然后我们应用过滤，其中`minDf`是一个参数，它指定了最小的文档频率。在丢弃不常用的令牌后，我们将一个列号与每个剩余的相关联，并将其放入一个`Map`。

现在，我们可以使用文档频率来计算 IDF:

```
int numDocuments = documents.size();
double numDocumentsLog = Math.log(numDocuments + 1);
double[] idfs = new double[vocabulary.size()];

for (Entry<String> e : docFrequency.entrySet()) {
    String token = e.getElement();
    double idfValue = numDocumentsLog - Math.log(e.getCount() + 1);
    idfs[tokenToIndex.get(token)] = idfValue;
}

```

在执行之后，`idfs`数组将包含我们词汇表中所有标记的 IDF 部分权重。

现在我们准备将标记化的文档放入向量空间:

```
int ncol = vocabulary.size();
SparseDataset tfidf = new SparseDataset(ncol);

for (int rowNo = 0; rowNo < documents.size(); rowNo++) {
    List<String> doc = documents.get(rowNo);
    Multiset<String> row = HashMultiset.create(doc);
    for (Entry<String> e : row.entrySet()) {
        String token = e.getElement();
        double tf = e.getCount();
        int colNo = tokenToIndex.get(token);
        double idf = idfs[colNo];
        tfidf.set(rowNo, colNo, tf * idf);
    }
}

tfidf.unitize();

```

由于结果向量非常稀疏，我们使用 Smile 中的`SparseDataset`来存储它们。然后，对于文档中的每个令牌，我们计算它的 TF 并乘以 IDF，以获得 TF-IDF 权重。

代码的最后一行将长度规范化应用于文档向量。这样，计算向量之间的内积将得到余弦相似性得分，这是一个介于 0 和 1 之间的有界值。

现在，让我们将代码放入一个类中，这样我们可以在以后重用它:

```
public class CountVectorizer {
    void fit(List<List<String>> documents);
    SparseDataset tranform(List<List<String>> documents);
}

```

我们定义的函数执行以下操作:

*   `fit`创建从令牌到列号的映射，并计算 IDF
*   将文档集合转换成稀疏矩阵
*   构造函数应该使用`minDf`，它指定了一个令牌的最小文档频率。

现在我们可以用它来矢量化我们的数据集:

```
List<List<String>> documents = Files.lines(path, StandardCharsets.UTF_8)
         .map(line -> TextUtils.tokenize(line))
         .map(line -> TextUtils.removeStopwords(line))
         .collect(Collectors.toList());

int minDf = 5;
CountVectorizer cv = new CountVectorizer(minDf);
cv.fit(documents);
SparseDataset docVectors = cv.transform(documents);

```

现在假设我们作为用户想要查询这个文档集合。为了能够做到这一点，我们需要实现以下内容:

1.  首先，在相同的向量空间中表示一个查询:也就是说，对文档应用完全相同的过程(标记化、停用词删除等等)。
2.  然后，计算查询和每个文档之间的相似度。
3.  最后，使用相似性得分对文档进行排序，从最高到最低。

假设我们的查询是`the probabilistic interpretation of tf-idf`。然后，以类似的方式将其映射到向量空间:

```
List<String> query = TextUtils.tokenize("the probabilistic interpretation of tf-idf");
query = TextUtils.removeStopwords(query);
SparseDataset queryMatrix = vectorizer.transfrom(Collections.singletonList(query));
SparseArray queryVector = queryMatrix.get(0).x;

```

我们之前创建的方法接受文档的集合，而不是单个文档，所以首先我们将它包装到一个列表中，然后获得包含结果的矩阵的第一行。

我们现在拥有的是`docVector`，它是一个包含我们的文档集合的稀疏矩阵，还有`queryVector`，一个包含查询的稀疏向量。这样，获得相似性就很容易了:我们只需要将矩阵与向量相乘，结果将包含相似性得分。

和上一章一样，我们将利用**Matrix Java Toolkit**(**MTJ**)来解决这个问题。因为我们在做矩阵向量乘法，矩阵在左边，所以存储值的最好方式是基于行的表示。我们已经编写了一个实用方法，用于将 Smile 的`SparseDataset`转换为 MTJ 的`CompRowMatrix`。

又来了:

```
public static CompRowMatrix asRowMatrix(SparseDataset dataset) {
    int ncols = dataset.ncols();
    int nrows = dataset.size();

    FlexCompRowMatrix X = new FlexCompRowMatrix(nrows, ncols);
    SparseArray[] array = dataset.toArray(new SparseArray[0]);

    for (int rowIdx = 0; rowIdx < array.length; rowIdx++) {
        Iterator<Entry> row = array[rowIdx].iterator();
        while (row.hasNext()) {
            Entry entry = row.next();
            X.set(rowIdx, entry.i, entry.x);
        }
    }

    return new CompRowMatrix(X);
}

```

现在我们还需要从 MTJ 将一个`SparseArray`对象转换成一个`SparseVector`对象。

让我们也为此创建一个方法:

```
public static SparseVector asSparseVector(int dim, SparseArray vector) {
    int size = vector.size();
    int[] indexes = new int[size];
    double[] values = new double[size];

    Iterator<Entry> iterator = vector.iterator();
    int idx = 0;

    while (iterator.hasNext()) {
        Entry entry = iterator.next();
        indexes[idx] = entry.i;
        values[idx] = entry.x;
        idx++;
    }

    return new SparseVector(dim, indexes, values, false);
}

```

注意，我们还必须将结果向量的维数传递给这个方法。这是由于`SparseArray`的限制，它不存储关于它的信息。

现在我们可以使用这些方法来计算相似性:

```
CompRowMatrix X = asRowMatrix(docVectors);
SparseVector v = asSparseVector(docVectors.ncols(), queryVector);
DenseVector result = new DenseVector(X.numRows());
X.mult(v, result);
double[] scores = result.getData();

```

scores 数组现在包含每个文档的查询的余弦相似性得分。该数组的索引对应于原始文档集合的索引。也就是说，为了查看查询和第 10 个文档之间的相似性，我们查看数组的第 10 个元素。因此，我们需要根据分数对数组进行排序，同时保留原始索引。

让我们首先为它创建一个类:

```
public class ScoredIndex implements Comparable<ScoredIndex> {
    private final int index;
    private final double score;

    // constructor and getters omitted

    @Override
    public int compareTo(ScoredIndex that) {
        return -Double.compare(this.score, that.score);
    }
 }

```

这个类实现了`Comparable`接口，所以现在我们可以把这个类的所有对象放到一个集合中，然后进行排序。最后，集合中的第一个元素得分最高。让我们这样做:

```
double minScore = 0.2;
List<ScoredIndex> scored = new ArrayList<>(scores.length);

for (int idx = 0; idx < scores.length; idx++) {
    double score = scores[idx];
    if (score >= minScore) {
        scored.add(new ScoredIndex(idx, score));
    }
}

Collections.sort(scored);

```

我们还添加了一个 0.2 的相似性阈值来对更少的元素进行排序:我们假设低于这个分数的元素是不相关的，所以我们忽略它们。

最后，我们可以迭代结果并查看最相关的文档:

```
for (ScoredIndex doc : scored) {
    System.out.printf("> %.4f ", doc.getScore());
    List<String> document = documents.get(doc.getIndex());
    System.out.println(String.join(" ", document));
}

```

这样，我们自己实现了一个简单的 IR 系统，完全从零开始。但是，实现相当幼稚。在现实中，有相当多的候选文档，因此用它们中的每一个来计算查询的余弦相似性是不可行的。有一种特殊的数据结构叫做倒排索引，可以用来解决这个问题，现在我们来看看它的一个实现:Apache Lucene。



# 索引和 Apache Lucene

之前，我们研究了如何实现一个简单的搜索引擎，但是它不能很好地适应文档的数量。

首先，它需要将查询与我们集合中的每一个文档进行比较，随着文档的增长，这变得非常耗时。然而，大多数文档与查询不相关，只有一小部分与查询相关。我们可以有把握地假设，如果一个文档与一个查询相关，那么它应该包含至少一个来自该查询的单词。这是倒排索引数据结构背后的思想:对于每个单词，它跟踪包含它的文档。当给定一个查询时，它可以快速找到至少包含一个术语的文档。

还有一个内存问题:在某些时候，文档将不再适合内存，我们需要能够将它们存储在磁盘上，并在需要时进行检索。

Apache Lucene 解决了这些问题:它实现了一个持久的倒排索引，在速度和存储方面都非常高效，并且经过了高度优化和时间验证。在[第二章](a005c42b-d837-402c-9bba-971b440268b5.xhtml)、*数据处理工具箱*中我们收集了一些原始的 HTML 数据，所以让我们用 Lucene 为它建立一个索引。

首先，我们需要将库包含到 pom 中:

```
<dependency>
  <groupId>org.apache.lucene</groupId>
  <artifactId>lucene-core</artifactId>
  <version>6.2.1</version>
</dependency>
<dependency>
  <groupId>org.apache.lucene</groupId>
  <artifactId>lucene-analyzers-common</artifactId>
  <version>6.2.1</version>
</dependency>
<dependency>
  <groupId>org.apache.lucene</groupId>
  <artifactId>lucene-queryparser</artifactId>
  <version>6.2.1</version>
</dependency>

```

Lucene 是非常模块化的，可以只包含我们需要的东西。在我们的例子中，这是:

*   包:我们在使用 Lucene 时总是需要它
*   `analyzers-common`模块:它包含了文本处理的公共类
*   `queryparser`:这是用于解析查询的模块

Lucene 提供了几种类型的索引，包括内存索引和文件系统索引。我们将使用文件系统:

```
File index = new File(INDEX_PATH);
FSDirectory directory = FSDirectory.open(index.toPath());

```

接下来，我们需要定义一个分析器:这是一个完成所有文本处理步骤的类，包括标记化、停用词移除和规范化。

`StandardAnalyzer`是一个基本的`Analyzer`，它删除了一些英语停用词，但不执行任何词干化或词汇化。它对英文文本非常有效，所以让我们用它来建立索引:

```
StandardAnalyzer analyzer = new StandardAnalyzer();
IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(analyzer))

```

现在我们准备索引文档了！

让我们来看看之前浏览过的 URL，并对它们的内容进行索引:

```
UrlRepository urls = new UrlRepository();
Path path = Paths.get("data/search-results.txt");
List<String> lines = 
        FileUtils.readLines(path.toFile(), StandardCharsets.UTF_8);

for (String line : lines) {
    String[] split = line.split("t");
    String url = split[3];
    Optional<String> html = urls.get(url);
    if (!html.isPresent()) {
        continue;
    }

    org.jsoup.nodes.Document jsoupDoc = Jsoup.parse(html.get());
    Element body = jsoupDoc.body();
    if (body == null) {
        continue;
    }

    Document doc = new Document();
    doc.add(new Field("url", url, URL_FIELD));
    doc.add(new Field("title", jsoupDoc.title(), URL_FIELD));
    doc.add(new Field("content", body.text(), BODY_FIELD));
    writer.addDocument(doc);
}

writer.commit();
writer.close();
directory.close();

```

让我们仔细看看这里的一些东西。首先，`UrlRepository`是一个类，存储我们在[第二章](a005c42b-d837-402c-9bba-971b440268b5.xhtml)、*数据处理工具箱*中创建的一些 URL 的抓取的 HTML 内容。给定一个 URL，它返回一个`Optional`对象，如果存储库有它的数据，它就包含响应；否则它返回一个空的`Optional`。

然后我们用 JSoup 解析原始 HTML 并提取标题和正文。现在我们有了文本数据，我们把它放入 Lucene `Document`。

Lucene 中的一个`Document`由字段组成，每个`Field`对象存储一些关于文档的信息。一个`Field`有一些属性，比如:

*   无论我们是否将值存储在索引中。如果我们这样做，那么以后我们可以提取内容。
*   无论我们是否将价值指数化。如果它被索引，那么它就变得可搜索，我们可以查询它。
*   不管是不是分析出来的。如果是，我们将分析器应用于内容，这样我们就可以查询单个令牌。否则只有精确匹配是可能的。

这些和其他属性保存在`FieldType`对象中。

例如，下面是我们如何指定`URL_FIELD`的属性:

```
FieldType field = new FieldType();
field.setTokenized(false);
field.setStored(true);
field.freeze();

```

这里我们说我们不想对它进行标记化，而是想将值存储在索引中。`freeze()`方法确保一旦我们指定了属性，它们就不能再被更改。

下面是我们如何指定`BODY_FIELD`:

```
FieldType field = new FieldType();
field.setStored(false);
field.setTokenized(true);
field.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
field.freeze();

```

在这种情况下，我们只分析它，但不存储字段的确切内容。通过这种方式，仍然可以对其进行查询，但是由于没有存储内容，该字段在索引中占用的空间较少。

它非常快速地处理我们的数据集，并在执行后在文件系统中创建一个索引，我们可以查询它。让我们开始吧。

```
String userQuery = "cheap used cars";

File index = new File(INDEX_PATH);
FSDirectory directory = FSDirectory.open(index.toPath());
DirectoryReader reader = DirectoryReader.open(directory);
IndexSearcher searcher = new IndexSearcher(reader);

StandardAnalyzer analyzer = new StandardAnalyzer();
AnalyzingQueryParser parser = new AnalyzingQueryParser("content", analyzer);
Query query = parser.parse(userQuery);

TopDocs result = searcher.search(query, 10);
ScoreDoc[] scoreDocs = result.scoreDocs;

for (ScoreDoc scored : scoreDocs) {
    int docId = scored.doc;
    float luceneScore = scored.score;
    Document doc = searcher.doc(docId);
    System.out.println(luceneScore + " " + doc.get("url") + " " + doc.get("title"));
}

```

在这段代码中，我们首先打开索引，然后指定用于处理查询的分析器。使用这个分析器，我们解析查询，并使用解析后的查询从索引中提取前 10 个匹配的文档。我们存储了 URL 和标题，所以现在我们可以在查询时检索这些信息并呈现给用户。



# 自然语言处理工具

自然语言处理是计算机科学和计算语言学中处理文本的一个领域。正如我们之前看到的，信息检索使用简单的 NLP 技术来索引和检索文本信息。

但是 NLP 可以做得更多。有相当多的主要 NLP 任务，如文本摘要或机器翻译，但我们不会涵盖它们，只讨论基本的任务:

*   句子分割:给定文本，我们把它分割成句子
*   **标记化:**给定一个句子，将其拆分成单独的标记
*   **引理化:**给定一个令牌，我们想求出它的引理。例如，对于单词*猫*和*猫*，词条是*猫。*
*   **词性标注(POS Tagging)** :给定一系列标记，目标是确定每个标记的词性。例如，它意味着将标记动词与标记 like 相关联，或者将标记名词与标记 laptop 相关联。
*   **命名实体识别(NER)** :在一系列记号中，找出与命名实体相对应的记号，例如城市、国家、其他地理名称、人名等等。例如，它应该将 Paul McCartney 标记为人名，将德国标记为国名。

让我们看看实现这些基本方法的一个库:Stanford CoreNLP。



# 斯坦福·科伦普

Java 中有相当多成熟的 NLP 库。比如斯坦福 CoreNLP，OpenNLP，还有 GATE。我们之前介绍过的许多库都有一些 NLP 模块，例如，Smile 或 JSAT。

在本章中，我们将使用斯坦福 CoreNLP。没有特别的原因，如果需要，应该可以在任何其他库中复制这些示例。

让我们从在`pom.xml`中指定以下依赖关系开始:

```
<dependency>
  <groupId>edu.stanford.nlp</groupId>
  <artifactId>stanford-corenlp</artifactId>
  <version>3.6.0</version>
</dependency>
<dependency>
  <groupId>edu.stanford.nlp</groupId>
  <artifactId>stanford-corenlp</artifactId>
  <version>3.6.0</version>
  <classifier>models</classifier>
</dependency>

```

有两个依赖项:第一个是 NLP 包本身，第二个包含第一个模块使用的模型。这些模型适用于英语，但也存在适用于其他欧洲语言(如德语或西班牙语)的模型。

这里的主要抽象是一个 StanfordCoreNLP 类，它充当处理管道。它指定了应用于原始文本的一系列步骤。

考虑下面的例子:

```
Properties props = new Properties();
props.put("annotators", "tokenize, ssplit, pos, lemma");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

```

在这里，我们创建一个管道，它获取文本，对文本进行标记，将文本拆分成句子，对每个标记应用 POS 模型，然后找到它的词条。

我们可以这样使用它:

```
String text = "some text";

Annotation document = new Annotation(text);
pipeline.annotate(document);
List<Word> results = new ArrayList<>();

List<CoreLabel> tokens = document.get(TokensAnnotation.class);
for (CoreLabel tokensInfo : tokens) {
    String token = tokensInfo.get(TextAnnotation.class);
    String lemma = tokensInfo.get(LemmaAnnotation.class);
    String pos = tokensInfo.get(PartOfSpeechAnnotation.class);
    results.add(new Word(token, lemma, pos));
}

```

在这段代码中，`Word`是我们的类，它保存了关于标记的信息:表面形式(出现在文本中的形式)、引理(规范化的形式)和词性。

很容易修改管道来添加额外的步骤。例如，如果我们希望添加 NER，那么我们首先要做的是将`NER`添加到管道中:

```
Properties props = new Properties();
props.put("annotators", "tokenize, ssplit, pos, lemma, ner");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

```

然后，对于每个令牌，提取相关的`NER`标签:

```
String ner = tokensInfo.get(NamedEntityTagAnnotation.class);

```

尽管如此，前面的代码仍然需要一些手工清理；如果我们运行它，我们可能会注意到它还输出标点符号和停用词。通过在循环中添加一些额外的检查，很容易解决这个问题:

```
for (CoreLabel tokensInfo : tokens) {
    String token = tokensInfo.get(TextAnnotation.class);
    String lemma = tokensInfo.get(LemmaAnnotation.class);
    String pos = tokensInfo.get(PartOfSpeechAnnotation.class);
    String ner = tokensInfo.get(NamedEntityTagAnnotation.class);

    if (isPunctuation(token) || isStopword(token)) {
        continue;
    } 

    results.add(new Word(token, lemma, pos, ner));
}

```

`isStopword`方法的实现很简单:我们只需检查令牌是否在停用字词集中。检查标点符号也不难:

```
public static boolean isPunctuation(String token) {
    char first = token.charAt(0);
    return !Character.isAlphabetic(first) && !Character.isDigit(first);
}

```

我们只是验证`String`的第一个字符不是字母也不是数字。如果是这样，那么一定是标点符号。

NER 还有另一个问题，我们可能需要解决:它没有将同类的连续单词连接到一个令牌中。考虑这个例子:*我叫贾斯汀比伯，我住在纽约*。它将产生以下 NER 标签分配:

*   贾斯汀->人
*   比伯->人
*   新建->位置
*   约克->位置
*   其他令牌被映射到`O`

我们可以用下面的代码片段连接标有相同`NER`标记的连续令牌:

```
String prevNer = "O";

List<List<Word>> groups = new ArrayList<>();
List<Word> group = new ArrayList<>();

for (Word w : words) {
    String ner = w.getNer();
    if (prevNer.equals(ner) && !"O".equals(ner)) {
        group.add(w);
        continue;
    }

    groups.add(group);
    group = new ArrayList<>();
    group.add(w);

    prevNer = ner;
}

groups.add(group);

```

所以我们简单地检查序列，看看当前标签是否与前一个标签相同。如果是这样，那么我们停止一个组，开始下一个组。如果我们看到`O`，那么我们总是假设它是下一组。之后，我们只需要过滤空的组，如果需要的话，将文本字段合并成一个。

虽然这对人来说似乎没什么大不了的，但对于像纽约这样的地名来说可能很重要:这些标记合在一起与单独的标记 New 和 York 具有完全不同的含义，因此将它们作为单个标记对待可能对 IR 系统很有用。

接下来，我们将看到如何在 Apache Lucene 中利用 NLP 工具，如 Stanford CoreNLP。



# 定制 Apache Lucene

Apache Lucene 是一个古老且非常强大的搜索库。它写于 1999 年，从那以后，许多用户不仅采用了它，还为这个库创建了许多不同的扩展。

尽管如此，有时 Lucene 内置的 NLP 功能还不够，还需要一个专门的 NLP 库。

例如，如果我们想在标记中包含 POS 标签，或者查找命名实体，那么我们需要像 Stanford CoreNLP 这样的东西。在 Lucene 工作流中包含这样的外部专用 NLP 库并不困难，这里我们将看到如何实现。

让我们使用 StanfordNLP 库和我们在上一节中实现的标记器。我们可以用一个方法`tokenize`将其命名为`StanfordNlpTokenizer`，在这里我们将放入之前编写的用于标记化的代码。

我们可以使用这个类来标记抓取的 HTML 数据的内容。和以前一样，我们使用 JSoup 从 HTML 中提取文本，但是现在，我们不是将标题和正文直接放入文档，而是首先使用 CoreNLP 管道自己对其进行预处理。我们可以通过创建以下实用程序方法来实现，然后使用它来标记标题和正文:

```
public static String tokenize(StanfordNlpTokenizer tokenizer, String text) {
    List<Word> tokens = tokenizer.tokenize(text);
    return tokens.stream()
                .map(Word::getLemma)
                .map(String::toLowerCase)
                .collect(Collectors.joining(" "));
}

```

请注意，这里我们使用了引理，而不是令牌本身，最后我们再次将所有内容放回到一个字符串中。

通过这个修改，我们可以使用 Lucene 的`WhitespaceAnalyzer`。与`StandardAnalyzer`相反，它非常简单，它所做的就是用一个空白字符分割文本。在我们的例子中，字符串已经由 CoreNLP 准备和处理，所以 Lucene 以期望的形式索引内容。

完整的修改版本将如下所示:

```
Analyzer analyzer = new WhitespaceAnalyzer();
IndexWriter writer = 
        new IndexWriter(directory, new IndexWriterConfig(analyzer));
StanfordNlpTokenizer tokenizer = new StanfordNlpTokenizer();

for (String line : lines) {
    String[] split = line.split("t");
    String url = split[3];
    Optional<String> html = urls.get(url);
    if (!html.isPresent()) {
        continue;
    }

    org.jsoup.nodes.Document jsoupDoc = Jsoup.parse(html.get());
    Element body = jsoupDoc.body();
    if (body == null) {
        continue;
    }

    String titleTokens = tokenize(tokenizer, jsoupDoc.title());
    String bodyTokens = tokenize(tokenizer, body.text());

    Document doc = new Document();
    doc.add(new Field("url", url, URL_FIELD));
    doc.add(new Field("title", titleTokens, URL_FIELD));
    doc.add(new Field("content", bodyTokens, BODY_FIELD)); 
    writer.addDocument(doc);
}

```

可以对某些字段使用 Lucene 的`StandardAnalyzer`，对其他字段使用经过定制预处理的`WhitespaceAnalyzer`。为此，我们需要使用`PerFieldAnalyzerWrapper`，在这里我们可以为每个字段指定一个特定的`Analyzer`。

这为我们如何预处理和分析文本提供了很大的灵活性，但是它不允许我们改变排序公式:Lucene 用来排序文档的公式。在本章的后面，我们也将看到如何做到这一点，但首先我们将看看如何在文本分析中使用机器学习。



# 文本的机器学习

机器学习在文本处理中起着重要的作用。它允许更好地理解隐藏在文本中的信息，并提取隐藏在那里的有用知识。我们已经从前面的章节中熟悉了机器学习模型，事实上，我们甚至已经将其中的一些用于文本，例如，来自斯坦福 CoreNLP 的 POS tagger 和 NER 都是基于机器学习的模型。

在[第 4 章](24f23333-1326-47d1-9cb2-9ab9c53f82e8.xhtml)、*监督学习-分类和回归*和[第 5 章](e4294e91-73ee-46ca-8fb5-eb6183c0e361.xhtml)、*非监督学习-聚类和降维*中，我们涵盖了监督和非监督机器学习问题。就文本而言，两者都在帮助组织文本或提取有用信息方面发挥着重要作用。在本节中，我们将看到如何将它们应用于文本数据。



# 文本的无监督学习

正如我们所知，无监督机器学习处理没有提供标签信息的情况。对于文本，这意味着只让它处理大量的文本数据，而没有关于内容的额外信息。尽管如此，它可能经常是有用的，现在我们将看到如何对文本使用降维和聚类。



# 潜在语义分析

**潜在语义分析** ( **LSA** )，也称为**潜在语义索引** ( **LSI** )，是无监督降维技术对文本数据的应用。

LSA 试图解决的问题是:

*   同义词:这意味着多个单词具有相同的意思
*   多义性:这意味着一个词有多个意思

基于术语的浅层技术(如单词袋)无法解决这些问题，因为它们只查看术语的精确原始形式。例如，像 help 和 assist 这样的词将被分配到向量空间的不同维度，即使它们在语义上非常接近。

为了解决这些问题，LSA 将文档从通常的词汇向量空间转移到其他一些语义空间，在这些空间中，意义相近的词对应于同一维度，多义词的值跨维度拆分。

这是通过查看术语-术语共现矩阵来实现的。假设是这样的:如果两个词经常在同一个语境中使用，那么它们是同义的，反之，如果一个词是多义的，那么它将在不同的语境中使用。降维技术可以检测这种共现模式，并将它们压缩到更小维度的向量空间中。

一种这样的降维技术是**奇异值分解** ( **SVD** )。如果 *X* 是一个文档术语矩阵，比如我们从`CountVectorizer`得到的矩阵，那么 *X* 的 SVD 是:

***XV = US***

上述等式中的各项解释如下:

*   *V* 是在术语-术语共现矩阵 *X ^T X* 上计算的术语的基础
*   *U* 是在文档-文档共现矩阵 XX ^T 上计算的文档的基础

因此，通过对 *X* 应用截断的 SVD，我们降低了术语-术语共现矩阵 X ^T X 的维数，然后可以使用这个新的简化基 *V* 来表示我们的文档。

我们的文档矩阵存储在`SparseDataset`中。如果您还记得，我们已经在这样的对象上使用了 SVD:首先，我们将 SparseDataset 转换成基于列的`SparseMatrix`，然后对它应用 SVD:

```
SparseMatrix matrix = data.toSparseMatrix();
SingularValueDecomposition svd = SingularValueDecomposition.decompose(matrix, n);
double[][] termBasis = svd.getV();

```

然后下一步是把我们的矩阵投射到这个新的项基上。在前一章中，我们已经使用以下方法做到了这一点:

```
public static double[][] project(SparseDataset dataset, double[][] Vd) {
    CompRowMatrix X = asRowMatrix(dataset);
    DenseMatrix V = new DenseMatrix(Vd);
    DenseMatrix XV = new DenseMatrix(X.numRows(), V.numColumns());
    X.mult(V, XV);
    return to2d(XV);
}

```

这里，`asRowMatrix`将`SparseDataset`转换成来自 MTJ 的`CompRowMatrix`，`to2d`将来自 MTJ 的密集矩阵转换成双精度的二维数组。

一旦我们将原始数据投影到 LSA 空间，它就不再是归一化的了。我们可以通过实现以下方法来解决这个问题:

```
public static double[][] l2RowNormalize(double[][] data) {
    for (int i = 0; i < data.length; i++) {
        double[] row = data[i];
        ArrayRealVector vector = new ArrayRealVector(row, false);
        double norm = vector.getNorm();
        if (norm != 0) {
            vector.mapDivideToSelf(norm);
            data[i] = vector.getDataRef();
        }
    }

    return data;
}

```

这里，我们对输入矩阵的每一行应用长度规范化，为此我们使用 Apache Commons Math 中的`ArrayRealVector`。

为了方便起见，我们可以为 LSA 创建一个特殊的类。姑且称之为`TruncatedSVD`，它会有如下签名:

```
public class TruncatedSVD {
    void fit(SparseDataset data);
    double[][] transform(SparseDataset data);
}

```

它有以下方法:

*   `fit`学习新学期基础
*   `transform`通过将数据投射到已学习的基础上来减少数据的维度
*   构造函数应该有两个参数:`n`，期望的维数和结果是否应该被规范化

我们可以将 LSA 应用到我们的 IR 系统中:现在，代替单词袋空间中的余弦相似度，我们进入 LSA 空间并在那里计算余弦。为此，我们首先需要在索引期间将文档映射到这个空间，然后在查询期间，我们对用户查询执行相同的转换。然后，计算余弦只是一个矩阵乘法。

所以，让我们先来看看我们之前使用的代码:

```
List<List<String>> documents = Files.lines(path, StandardCharsets.UTF_8)
        .map(line -> TextUtils.tokenize(line))
        .map(line -> TextUtils.removeStopwords(line))
        .collect(Collectors.toList());

int minDf = 5;
CountVectorizer cv = new CountVectorizer(minDf);
cv.fit(documents);
SparseDataset docVectors = cv.transform(documents);

```

现在，我们使用刚刚创建的`TruncatedSVD`类将`docVectors`映射到 LSA 空间:

```
int n = 150;
boolean normalize = true;
TruncatedSVD svd = new TruncatedSVD(n, normalize);
svd.fit(docVectors);
double[][] docsLsa = svd.transform(docVectors);

```

我们重复同样的查询:

```
List<String> query = TextUtils.tokenize("cheap used cars");
query = TextUtils.removeStopwords(query);
SparseDataset queryVectors = vectorizer.transfrom(Collections.singletonList(query));
double[] queryLsa = svd.transform(queryVectors)[0];

```

像前面一样，我们将查询包装到一个列表中，然后提取结果的第一行。然而，这里我们有一个密集的向量，而不是稀疏的。现在，剩下的是计算相似性，这只是一个矩阵向量乘法:

```
DenseMatrix X = new DenseMatrix(docsLsa);
DenseVector v = new DenseVector(vector);
DenseVector result = new DenseVector(X.numRows());
X.mult(v, result);
double[] scores = result.getData();

```

执行之后，scores 数组将包含相似性，我们可以使用 ScoredIndex 类根据这个分数对文档进行排序。这是非常有用的，所以让我们把它变成一个实用方法:

```
public static List<ScoredIndex> wrapAsScoredIndex(double[] scores, double minScore) {
    List<ScoredIndex> scored = new ArrayList<>(scores.length);

    for (int idx = 0; idx < scores.length; idx++) {
        double score = scores[idx];
        if (score >= minScore) {
            scored.add(new ScoredIndex(idx, score));
        }
    }

    Collections.sort(scored);
    return scored;
}

```

最后，我们从列表中取出第一个元素，并像以前一样将它们呈现给用户。



# 文本聚类

在[第 5 章](e4294e91-73ee-46ca-8fb5-eb6183c0e361.xhtml)、*无监督学习——聚类和降维*中，我们介绍了降维和聚类。我们已经讨论了如何对文本进行降维，但还没有谈到聚类。

文本聚类对于理解什么是文档集合也是一种有用的技术。当我们想要聚类文本时，目标类似于非文本情况:我们想要找到具有许多共同点的文档组:例如，这种组中的文档应该是关于同一主题的。在某些情况下，这对于 IR 系统是有用的。例如，如果一个主题是不明确的，我们可能希望对搜索引擎结果进行分组。

means 是一个简单而强大的聚类算法，它非常适合文本。让我们使用抓取的文本，并尝试使用 *K* -means 在其中找到一些话题。首先，我们加载文档并将它们矢量化。我们将使用来自 Smile 的 *K* -Means 实现，如果你还记得的话，它不支持稀疏矩阵，所以我们还需要降低维数。为此，我们将使用 LSA。

```
List<List<String>> documents = ... // read the crawl data

int minDf = 5;
CountVectorizer cv = new CountVectorizer(minDf);
cv.fit(documents);

SparseDataset docVectors = cv.transform(documents);
int n = 150;
boolean normalize = true;
TruncatedSVD svd = new TruncatedSVD(n, normalize);
svd.fit(docVectors);

double[][] docsLsa = svd.transform(docVectors);

```

数据是准备好的，所以我们可以应用*K*-意思是:

```
int maxIter = 100;
int runs = 3;
int k = 100;
KMeans km = new KMeans(docsLsa, k, maxIter, runs);

```

这里，`k`，你应该还记得上一章的内容，是我们想要寻找的集群的数量。这里对`K`的选择是相当随意的，所以可以随意试验并选择`K`的任何其他值。

一旦它完成了，我们可以看看结果的质心。然而，这些质心在 LSA 空间中，而不在原始项空间中。为了让他们回来，我们需要反转 LSA 变换。

为了从原始空间到 LSA 空间，我们使用了由基项构成的矩阵。因此，为了进行逆变换，我们需要该矩阵的逆。因为基是正交的，所以它的逆与转置相同，我们将用它来求 LSA 变换的逆。代码看起来是这样的:

```
double[][] centroids = km.centroids();
double[][] termBasis = svd.getTermBasis();
double[][] centroidsOriginal = project(centroids, t(termBasis));

```

以下是`t`方法计算转置的方式:

```
public static double[][] t(double[][] M) {
    Array2DRowRealMatrix matrix = new Array2DRowRealMatrix(M, false);
    return matrix.transpose().getData();
}

```

而投影法只是计算矩阵-矩阵乘法。

现在，当质心在原始空间时，我们找到每个质心最重要的项。

为此，我们取一个质心，看看最大的维度是多少:

```
List<String> terms = vectorizer.vocabulary();
for (int centroidId = 0; centroidId < k; centroidId++) {
    double[] centroid = centroidsOriginal[centroidId];
    List<ScoredIndex> scored = wrapAsScoredIndex(centroid, 0.0);
    for (int i = 0; i < 20; i++) {
        ScoredIndex scoredTerm = scored.get(i);
        int position = scoredTerm.getIndex();
        String term = terms.get(position);
        System.out.print(term + ", ");
    }
    System.out.println();
}

```

这里，terms 是包含来自`CountVectorizer,`的维度名称的列表，而`wrapAsScoredIndex`是我们之前编写的函数；它接受一个双精度数组，创建一个`ScoredIndex`对象列表，并对其进行排序。

当您运行它时，您可能会看到类似于这些集群的内容:

| **集群 1** | **集群 2** | **集群 3** |
| 血压低血压低症状心脏病的治疗 | 惠普打印机打印机打印 laserjet 支持 officejet 打印墨水软件 | 汽车汽车丰田福特本田二手宝马雪佛兰汽车日产 |

我们只取了前三组，它们显然是有意义的。也有一些聚类不太有意义，这表明该算法可以进一步调整:我们可以调整*中的*K*K*-LSA 的均值和维数。



# 单词嵌入

到目前为止，我们已经介绍了如何对文本数据应用降维和聚类。还有另一种类型的无监督学习，它特定于文本:单词嵌入。你可能听说过 **Word2Vec，**就是这样一种算法。

单词嵌入试图解决的问题是如何将单词嵌入到低维向量空间中，使得语义上接近的单词在这个空间中是接近的，而不同的单词是远离的。

例如，猫和狗应该离得很近，但是笔记本电脑和天空应该离得很远。

这里，我们将实现一个基于共现矩阵的单词嵌入算法。它建立在 LSA 的思想之上:在那里，我们可以用术语所包含的文档来表示它们。所以，如果两个单词包含在同一个文档中，它们应该是相关的。然而，文档对于一个单词来说是一个相当宽泛的上下文，所以我们可以把它缩小到一个句子，或者缩小到感兴趣的单词前后的几个单词。

例如，考虑下面的句子:

我们使用 Java 进行数据科学，因为我们喜欢 Java。Java 有利于企业开发。

然后，我们将文本标记化，分成句子，删除停用词，得到以下内容:

*   “我们”、“使用”、“java”、“数据”、“科学”、“我们”、“喜欢”、“java”
*   “java”、“好”、“企业”、“开发”

现在，假设对于这里的每个单词，我们想看看前面的两个单词和后面的两个单词是什么。这会给我们每个单词的上下文。对于本例，它将是:

*   我们->使用 java
*   用-> we；java，数据
*   java -> we，使用；数据，科学
*   数据->使用，java 科学，我们
*   java ->我们，喜欢
*   java ->好，企业
*   好的-> Java；企业，发展
*   企业-> java，不错；发展
*   发展->好，企业

然后，我们可以建立一个共现矩阵，在这个矩阵中，每当一个单词出现在另一个单词的上下文中时，我们就将它置 1。所以，对于“我们”，我们会在“使用”和“java”上加+1，以此类推。

最后，每个单元格将显示一个单词 w [1] (来自矩阵的行)在另一个单词 w [2] (来自矩阵的列)的上下文中出现了多少次。接下来，如果我们用奇异值分解降低这个矩阵的维数，我们已经比普通的 LSA 方法有了很大的改进。

但是我们可以更进一步，用**点态互信息** ( **PMI** )代替计数。

PMI 是衡量两个随机变量之间相关性的指标。它最初来自信息论，但在计算语言学中经常用于测量两个词之间的关联程度。它的定义如下:

![](img/png-3.png)

它检查两个单词 w 和 v 是否偶然同时出现。如果它们是偶然发生的，那么联合概率 *p(w，v)* 应该等于边际概率 *p(w) p(v)* 的乘积，所以 PMI 为 0。但是，如果两个单词之间确实存在关联，PMI 会得到高于 0 的值，因此值越高，关联越强。

我们通常通过浏览文本并计数来估计这些概率:

*   对于边际概率，我们只计算令牌出现的次数
*   对于连接概率，我们看共生矩阵

我们使用以下公式:

*   `p(w) = c(w) / N`，其中`c(w)`为`w`在体内出现的次数，`N`为令牌总数
*   `p(w, v) = c(w, v) / N`，其中`c(w, v)`是来自共生矩阵的值，`N`也是令牌的数量

然而，在实践中，`c(w, v)`、`c(w)`和`c(v)`的小值会扭曲概率，因此通常通过添加一些小数字`λ`来平滑它们:

*   `p(w) = [c(w) + λ] / [N + Kλ]`，其中`K`是语料库中唯一标记的数量
*   `p(w, v) = [c(w, v) + λ] / [N + Kλ]`

如果我们替换前面等式中的 PMI 公式，我们会得到以下公式:

*PMI(w, v) = log [c(w, v) + λ] + log [N + Kλ] - log [c(w) + λ] - log [c(v) + λ]*

所以我们能做的只是用 PMI 替换共现矩阵中的计数，然后计算这个矩阵的 SVD。在这种情况下，得到的嵌入将具有更好的质量。

现在，让我们实现它。首先，您可能已经注意到，我们需要有句子，而以前我们只有一串标记，没有句子边界的检测。我们知道，斯坦福 CoreNLP 可以做到，所以让我们创建一个管道:

```
Properties props = new Properties();
props.put("annotators", "tokenize, ssplit, pos, lemma");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

```

我们将使用句子分割器来检测句子，然后我们将采用单词的引理而不是表面形式。

但是让我们首先创建一些有用的类。之前我们用`List<List<String>>`说我们传递一个文档集合，每个文档都是一个令牌序列。现在我们把每个文档拆分成句子，再把每个句子拆分成令牌，就变成了`List<List<List<String>>>`，有点难以理解。我们可以用一些有意义的类来代替，比如`Document`和`Sentence:`

```
public class Document {
    private List<Sentence> sentences;
    // getter, setter and constructor is omitted
}

public class Sentence {
    private List<String> tokens;
    // getter, setter and constructor is omitted
}

```

尽可能创建这样的小班。尽管在开始时看起来有些冗长，但它对以后阅读代码和理解意图非常有帮助。

现在，让我们用它们来标记一个文档。我们可以用下面的方法创建一个`Tokenizer`类:

```
public Document tokenize(String text) {
    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    List<Sentence> sentencesResult = new ArrayList<>();
    List<CoreMap> sentences = document.get(SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {
        List<CoreLabel> tokens = sentence.get(TokensAnnotation.class);
        List<String> tokensResult = new ArrayList<>();

        for (CoreLabel tokensInfo : tokens) {
            String token = tokensInfo.get(TextAnnotation.class);
            String lemma = tokensInfo.get(LemmaAnnotation.class);
            if (isPunctuation(token) 
                    || isStopword(token) 
                    || lemma.length() <= 2) {
                continue;
            }

            tokensResult.add(lemma.toLowerCase());
        }

        if (!tokensResult.isEmpty()) {
            sentencesResult.add(new Sentence(tokensResult));
        }
    }

    return new Document(sentencesResult);
}

```

因此，我们在这里将句子拆分器应用于文本，然后，对于每个句子，收集标记。我们已经看到了`isPunctuation`和`isStopword`方法——这里它们的实现和前面的一样。

然后我们可以再次使用抓取的 HTML 数据集，并对用 JSoup 提取的内容应用标记器。为了简洁起见，我们将省略这一部分。现在，我们准备从这些数据中构建共现矩阵。

与`CountVectorizer`中一样，第一步是应用文档频率过滤器来丢弃不常用的标记，然后构建一个将标记与某个整数相关联的映射:得到的稀疏矩阵的列数。我们已经知道怎么做了，所以我们可以跳过这一部分。

然后，为了估计`p(w)`和`p(v)`，我们需要知道每个令牌出现的次数:

```
Multiset<String> counts = HashMultiset.create();
for (Document doc : documents) {
    for (Sentence sentence : doc.getSentences()) {
        counts.addAll(sentence.getTokens());
    }
}

```

现在，我们可以开始计算共生矩阵。为此，我们可以使用 Guava 的`Table`类:

```
Table<String, String, Integer> coOccurrence = HashBasedTable.create();
for (Document doc : documents) {
    for (Sentence sentence : doc.getSentences()) {
        processWindow(sentence, window, coOccurrence);
    }
}

```

这里，我们用以下内容定义`processWindow`函数:

```
List<String> tokens = sentence.getTokens();

for (int idx = 0; idx < tokens.size(); idx++) {
    String token = tokens.get(idx);

    Map<String, Integer> tokenRow = coOccurrence.row(token);

    for (int otherIdx = idx - window; 
            otherIdx <= idx + window; 
            otherIdx++) {

        if (otherIdx < 0 
                || otherIdx >= tokens.size() 
                || otherIdx == idx) {
            continue;
        }

        String other = tokens.get(otherIdx);
        int currentCnt = tokenRow.getOrDefault(other, 0);
        tokenRow.put(other, currentCnt + 1);
    }
}

```

这里我们在文档的每个句子上滑动一个指定大小的窗口。然后，对于该窗口中心的单词，我们查看前后的单词，并且对于它们中的每一个，将同现计数增加 1。

下一步是根据这些数据创建一个 PMI 值矩阵。像以前一样，我们将使用 Smile 的`SparseDataset`类来保存这些值:

```
int vocabularySize = vocabulary.size();

double logTotalNumTokens = Math.log(counts.size() + vocabularySize * smoothing);
SparseDataset result = new SparseDataset(vocabularySize);

for (int rowIdx = 0; rowIdx < vocabularySize; rowIdx++) {
    String token = vocabulary.get(rowIdx);
    double logMainTokenCount = Math.log(counts.count(token) + smoothing);
    Map<String, Integer> tokenCooc = coOccurrence.row(token);

    for (Entry<String, Integer> otherTokenEntry : tokenCooc.entrySet()) {
        String otherToken = otherTokenEntry.getKey();
        double logOtherTokenCount = Math.log(counts.count(otherToken) + smoothing);
        double logCoOccCount = Math.log(otherTokenEntry.getValue() + smoothing);

        double pmi = logCoOccCount + logTotalNumTokens 
                   - logMainTokenCount - logOtherTokenCount;

        if (pmi > 0) {
            int colIdx = tokenToIndex.get(otherToken);
            result.set(rowIdx, colIdx, pmi);
        }
    }

}

```

在这段代码中，我们只是将 PMI 公式应用于我们拥有的同现计数。最后，我们对这个矩阵执行 SVD，为此我们只需使用我们之前创建的`TruncatedSVD`类。

现在，我们可以看看我们训练的嵌入是否有意义。为此，我们可以选择一些术语，并为每个术语找到最相似的术语。这可以通过以下方式实现:

*   首先，对于给定的令牌，我们查找它的向量表示
*   然后，我们计算这个记号与其余向量的相似度。我们知道，这可以通过矩阵向量乘法来实现
*   最后，我们按分数对乘法的结果进行排序，并显示分数最高的记号。

到目前为止，我们已经完成了几次完全相同的过程，所以我们可以跳过代码。当然，它可以在本章的代码包中找到。

但是让我们看看结果。我们选取了几个词:**猫**、**德**和**笔记本电脑**，以下是最相似的几个词，根据我们刚刚训练的嵌入:

| **猫** | **德国** | **笔记本电脑** |
| 0.835 宠物 | 0.829 国家 | 0.882 笔记本 |
| 0.812 狗 | 0.815 移民 | 0.869 英寸超极本 |
| 0.796 小猫 | 0.813 联合 | 0.866 桌面 |
| 0.793 搞笑 | 0.808 个国家 | 0.865 专业版 |
| 0.788 小狗 | 0.802 巴西 | 0.845 触摸屏 |
| 0.762 动物 | 0.789 加拿大 | 0.842 联想 |
| 0.742 庇护所 | 0.777 德语 | 0.841 游戏 |
| 0.727 朋友 | 0.776 澳大利亚 | 0.836 片 |
| 0.727 救援 | 0.760 欧洲 | 0.834 华硕 |
| 0.726 图片 | 0.759 外国 | 0.829 macbook |

即使不理想，结果还是有意义的。通过在更多的文本数据上训练这些嵌入，或者微调诸如 SVD 的维数、最小文档频率和平滑量之类的参数，可以进一步改进它。

当训练单词嵌入时，获取更多的数据总是一个好主意。维基百科是一个很好的文本资料来源；它有多种语言版本，他们定期在 https://dumps.wikimedia.org/发布垃圾信息。如果维基百科还不够，你可以使用*普通抓取*([http://commoncrawl.org/](http://commoncrawl.org/))，他们抓取互联网上的所有内容，并免费提供给任何人。我们还会在[第 9 章](http://chapter%209)、*缩放数据科学*中谈到常见的抓取。

最后，互联网上有很多经过预先训练的单词嵌入。

比如，你可以看看这里的收藏:[https://github.com/3Top/word2vec-api](https://github.com/3Top/word2vec-api)。从那里加载嵌入非常容易。

为此，让我们首先创建一个类来存储向量:

```
public class WordEmbeddings {
    private final double[][] embeddings;
    private final List<String> vocabulary;
    private final Map<String, Integer> tokenToIndex;
    // constructor and getters are omitted

    List<ScoredToken> mostSimilar(String top, int topK, double minSimilarity);
    Optional<double[]> representation(String token);
}

```

该类具有以下字段和方法:

*   `embeddings`:这是存储向量的数组
*   这是所有令牌的列表
*   `tokenToIndex`:这是从令牌到存储向量的索引的映射
*   `mostSimilar`:这将返回与所提供的令牌最相似的前 K 个令牌
*   `representation`:返回一个术语的向量表示，如果没有向量，则可选。不存在

当然，我们可以把基于 PMI 的嵌入放在那里。但是让我们看看如何从前面的链接中加载现有的 GloVe 和 Word2Vec 向量。

对于 Word2Vec 和 GloVe 来说，存储向量的文本文件格式非常相似，所以我们只能介绍其中一种。GloVe 稍微简单一点，我们按如下方式使用它:

*   首先，从[http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)下载预训练的嵌入
*   拆开包装；在不同维度的相同语料库上训练了几个文件
*   让我们用`glove.6B.300d.txt`

存储格式很简单；每一行都有一个标记，后面跟着一系列数字。这些数字显然是令牌的嵌入向量。让我们来读一读:

```
List<Pair<String, double[]>> pairs =
        Files.lines(file.toPath(), StandardCharsets.UTF_8)
             .parallel()
             .map(String::trim)
             .filter(StringUtils::isNotEmpty)
             .map(line -> parseGloveTextLine(line))
             .collect(Collectors.toList());

List<String> vocabulary = new ArrayList<>(pairs.size());
double[][] embeddings = new double[pairs.size()][];

for (int i = 0; i < pairs.size(); i++) {
    Pair<String, double[]> pair = pairs.get(i);
    vocabulary.add(pair.getLeft());
    embeddings[i] = pair.getRight();
}

embeddings = l2RowNormalize(embeddings);
WordEmbeddings result = new WordEmbeddings(embeddings, vocabulary);

```

在这里，我们解析文本文件的每一行，然后创建词汇表并标准化向量的长度。`parseGloveTextLine`有以下内容:

```
List<String> split = Arrays.asList(line.split(" "));
String token = split.get(0);
double[] vector = split.subList(1, split.size()).stream()
        .mapToDouble(Double::parseDouble).toArray();
Pair<String, double[]> result = ImmutablePair.of(token, vector);

```

这里，`ImmutablePair`是 Apache Commons Lang 中的一个对象。

让我们用同样的词，看看他们的邻居使用这些手套嵌入。这是结果:

| **猫** | **德国** | **笔记本电脑** |
| - 0.682 狗
- 0.682 猫
- 0.587 宠物
- 0.541 狗
- 0.490 猫
- 0.488 猴
- 0.473 马
- 0.463 宠物
- 0.461 兔
- 0.459 豹 | - 0.749 德国
- 0.663 奥地利
- 0.646 柏林
- 0.597 欧洲
- 0.586 慕尼黑
- 0.579 波兰
- 0.577 瑞士
- 0.575 德国
- 0.559 丹麦
- 0.557 法国 | - 0.796 台笔记本电脑
- 0.673 台电脑
- 0.599 台手机
- 0.596 台电脑
- 0.580 台便携式
- 0.562 台台式
- 0.547 台手机
- 0.546 台笔记本
- 0.544 台
- 0.529 台手机 |

结果确实有意义，而且在某些情况下，它比我们训练自己的嵌入更好。

正如我们提到的，word2vec 向量的文本格式与 GloVe 向量非常相似，因此只需稍加修改就可以阅读它们。然而，有一种存储 word2vec 嵌入的二进制格式。它有点复杂，但是如果你想知道如何阅读它，看看本章的代码包。

在这一章的后面，我们将看到如何应用单词嵌入来解决监督学习问题。



# 文本的监督学习

有监督的机器学习方法对于文本数据也相当有用。像在通常的设置中一样，这里有标签信息，我们可以用它来理解文本中的信息。

将监督学习应用于文本的一个非常常见的例子是垃圾邮件检测:每当你点击电子邮件客户端的垃圾邮件按钮时，这些数据就会被收集起来，然后放入分类器中。然后，训练该分类器来区分垃圾邮件和非垃圾邮件。

在本节中，我们将通过两个例子来研究如何对文本使用监督方法:首先，我们将构建一个情感分析模型，然后我们将使用一个排名分类器对搜索结果进行重新排名。



# 文本分类

文本分类是一个问题，其中给定一组文本和标签，它训练一个模型，该模型可以为新的看不见的文本预测这些标签。这里的设置是监督学习的常用设置，只是现在我们有了文本数据。

有许多可能的分类问题，如下所示:

*   **垃圾邮件检测**:预测电子邮件是否是垃圾邮件
*   **情感分析**:预测文本的情感是正面还是负面
*   **语言检测**:给定一个文本，检测它的语言

几乎在所有情况下，文本分类的一般工作流程都是相似的:

*   我们对文本进行标记和矢量化
*   然后，我们拟合一个线性分类器，将每个记号视为一个特征

众所周知，如果我们对文本进行矢量化，得到的向量非常稀疏。这就是为什么使用线性模型是一个好主意:它们非常快，可以轻松处理文本数据的稀疏性和高维数。

所以让我们来解决其中一个问题。

比如我们可以拿一个情感分析问题，建立一个模型，这个模型预测文本的极性，也就是文本是正面的还是负面的。

我们可以从这里取数据:`http://ai.stanford.edu/~amaas/data/sentiment/`。这个数据集包含从 IMDB 中提取的 50.000 个带标签的电影评论，作者提供了预定义的训练测试分割。为了从那里存储评论，我们可以为它创建一个类:

```
public class SentimentRecord {
    private final String id;
    private final boolean train;
    private final boolean label;
    private final String content;
    // constructor and getters omitted
}

```

我们不会详细讨论从归档文件中读取数据的代码，但是像往常一样，欢迎您查看代码包。

至于模型，我们将使用 LIBLINEAR——正如你已经从[第四章](24f23333-1326-47d1-9cb2-9ab9c53f82e8.xhtml)、*中知道的——监督学习——分类和回归*。这是一个快速实现线性分类器的库，如逻辑回归和线性 SVM。

现在，让我们来读数据:

```
List<SentimentRecord> data = readFromTagGz("data/aclImdb_v1.tar.gz");

List<SentimentRecord> train = data.stream()
        .filter(SentimentRecord::isTrain)
        .collect(Collectors.toList());

List<List<String>> trainTokens = train.stream()
        .map(r -> r.getContent())
        .map(line -> TextUtils.tokenize(line))
        .map(line -> TextUtils.removeStopwords(line))
        .collect(Collectors.toList());

```

在这里，我们从归档中读取数据，然后对训练数据进行标记。接下来，我们对文本进行矢量化处理:

```
int minDf = 10;
CountVectorizer cv = new CountVectorizer(minDf);
cv.fit(trainTokens);
SparseDataset trainData = cv.transform(trainTokens);

```

到目前为止，没什么新发现。但是现在我们需要将`SparseDataset`转换成 LIBLINEAR 格式。让我们为此创建几个实用方法:

```
public static Feature[][] wrapX(SparseDataset dataset) {
    int nrow = dataset.size();
    Feature[][] X = new Feature[nrow][];

    int i = 0;
    for (Datum<SparseArray> inrow : dataset) {
        X[i] = wrapRow(inrow);
        i++;
    }

    return X;
}

public static Feature[] wrapRow(Datum<SparseArray> inrow) {
    SparseArray features = inrow.x;

    int nonzero = features.size();
    Feature[] outrow = new Feature[nonzero];
    Iterator<Entry> it = features.iterator();

    for (int j = 0; j < nonzero; j++) {
        Entry next = it.next();
        outrow[j] = new FeatureNode(next.i + 1, next.x);
    }

    return outrow;
}

```

第一个方法`wrapX`，采用一个`SparseDataset`并创建一个二维数组的`Feature`对象。这是 LIBLINEAR 存储数据的格式。第二个方法是`wrapRow`，它获取一个特定的`SparseDataset`行，并将其包装成一个由`Feature`对象组成的一维数组。

现在，我们需要提取标签信息并创建一个描述数据的`Problem`类的实例:

```
double[] y = train.stream().mapToDouble(s -> s.getLabel() ? 1.0 : 0.0).toArray();
Problem problem = new Problem();
problem.x = wrapX(dataset);
problem.y = y;
problem.n = dataset.ncols() + 1;
problem.l = dataset.size();

```

然后，我们定义参数并训练模型:

```
Parameter param = new Parameter(SolverType.L1R_LR, 1, 0.001);
Model model = Linear.train(problem, param);

```

这里，我们用 L1 正则化和成本参数`C=1`指定一个逻辑回归模型。

线性分类器，如逻辑回归或带 L1 正则化的 SVM，非常适合处理高稀疏性问题，如文本分类。L1 惩罚确保模型收敛得非常快，此外，它还迫使解变得稀疏:也就是说，它执行特征选择，只保留最有信息的单词。

为了预测概率，我们可以创建另一个实用方法，它采用一个模型和一个测试数据集，并返回一个一维概率数组:

```
public static double[] predictProba(Model model, SparseDataset dataset) {
    int n = dataset.size();
    double[] results = new double[n];
    double[] probs = new double[2];
    int i = 0;

    for (Datum<SparseArray> inrow : dataset) {
        Feature[] row = wrapRow(inrow);
        Linear.predictProbability(model, row, probs);
        results[i] = probs[1];
        i++;
    }

    return results;
}

```

现在我们可以测试模型了。因此，我们获取测试数据，对其进行标记化和矢量化，然后调用 predictProba 方法来检查输出。最后，我们可以使用一些评估指标(如 AUC)来评估性能。在这种特殊情况下，AUC 为 0.89，对于该数据集来说，这是相当好的性能。



# 学习信息检索排序

学习排序是一系列处理排序数据的算法。这个家庭是监督机器学习的一部分；为了对数据进行排序，我们需要知道哪些项目更重要，需要首先显示。

学习排名通常用于构建搜索引擎的环境中；基于一些相关性评估，我们建立了一个模型，试图将相关项目的排名高于不相关的项目。在无监督排序的情况下，例如 TF-IDF 权重上的余弦，我们通常只有一个特征，通过该特征我们对文档进行排序。然而，可能会有更多的特性，我们可能希望将它们包含在模型中，并让它以最佳的方式组合它们。

学习给模型排序有几种类型。其中一些被称为“逐点”——它们被单独应用于每个文档，并与其他训练数据隔离开来考虑。尽管这是一个严格的假设，但这些算法很容易实现，并且在实践中运行良好。通常，这相当于使用分类或回归模型，然后根据分数对项目进行排序。

让我们回到构建搜索引擎的运行示例，并在其中加入更多的特性。以前是无人监管的；我们只是根据一个特征，余弦，对项目进行了排序。但是我们可以增加更多的特征，使之成为一个监督学习的问题。

然而，为此，我们需要知道标签。我们已经有了肯定的标签:对于一个查询，我们知道大约 30 个相关的文档。但是我们不知道负面标签:我们使用的搜索引擎只返回相关的页面。所以我们需要得到反面的例子，然后就有可能训练一个二进制分类器来区分相关和不相关的页面。

有一种技术我们可以用来获得负数据，它被称为负抽样。这种想法是基于这样一种假设，即语料库中的大多数文档是不相关的，因此如果我们从那里随机抽取一些文档并说它们是不相关的，那么我们在大多数情况下都是正确的。如果一个被采样的文档变得相关，那么不会发生任何不好的事情；这只是一个嘈杂的观察，不应该影响整体结果。

因此，我们采取以下措施:

*   首先，我们读取排名数据，并根据查询对文档进行分组
*   然后，我们将查询分成两个不重叠的组:一个用于训练，一个用于验证
*   接下来，在每个组中，我们进行一个查询并随机抽取 9 个负面例子。来自否定查询的这些 URL 被分配了否定标签
*   最后，我们基于这些标记的文档/查询对训练一个模型

在阴性取样步骤中，重要的是为了训练，我们不从验证组中取阴性样本，反之亦然。如果我们只在训练/验证组中采样，那么我们可以确定我们的模型可以很好地推广到看不见的查询和文档。

负采样很容易实现，所以让我们开始吧:

```
private static List<String> negativeSampling(String positive, List<String> collection,
                int size, Random rnd) {
    Set<String> others = new HashSet<>(collection);
    others.remove(positive);
    List<String> othersList = new ArrayList<>(others);
    Collections.shuffle(othersList, rnd);
    return othersList.subList(0, size);
}

```

想法如下:首先，我们得到整个查询集合，并删除我们当前正在考虑的一个。然后，我们洗牌并从中挑选前 *N* 个。

既然我们有正面和负面的例子，我们需要提取特征，我们将把这些特征放入模型中。让我们创建一个`QueryDocumentPair`类，它将包含关于用户查询的信息以及关于文档的数据:

```
public class QueryDocumentPair {
    private final String query;
    private final String url;
    private final String title;
    private final String bodyText;
    private final String allHeaders;
    private final String h1;
    private final String h2;
    private final String h3;
    // getters and constructor omitted
}

```

这个类的对象可以通过用 JSoup 解析 HTML 内容并提取标题、正文、所有标题文本(h1-h6)以及 h1、h2、h3 标题来创建。

我们将使用这些字段来计算特征。

例如，我们可以计算以下各项:

*   查询和所有其他文本字段之间的单词包 TF-IDF 相似度
*   查询和所有其他文本字段之间的 LSA 相似性
*   嵌入查询和标题以及 h1、h2 和 h3 标题之间的相似性。

我们已经知道如何计算前两种类型的特征:

*   我们使用`CountVectorizer`分别对每个字段进行矢量化，并使用转换方法对查询进行矢量化
*   对于 LSA，我们以同样的方式使用`TruncatedSVD`类；我们在文本字段上训练它，然后将其应用于查询
*   然后，我们在单词袋和 LSA 空间中计算文本字段和查询之间的余弦相似度

然而，我们没有在这里讨论最后一个，使用单词嵌入。想法如下:

*   对于查询，获取每个令牌的向量，并将它们放入一个矩阵中
*   对于标题(或其他文本字段)，执行相同的操作
*   通过矩阵乘法计算每个查询向量与每个标题向量的相似度
*   查看相似性的分布，并获取该分布的一些特征，如最小值、平均值、最大值和标准偏差。我们可以使用这些值作为特征
*   此外，我们可以获取平均查询向量和平均标题向量，并计算它们之间的相似性

让我们实现这一点。首先，创建一个方法来获取令牌集合的向量:

```
public static double[][] wordsToVec(WordEmbeddings we, List<String> tokens) {
    List<double[]> vectors = new ArrayList<>(tokens.size());
    for (String token : tokens) {
        Optional<double[]> vector = we.representation(token);
        if (vector.isPresent()) {
            vectors.add(vector.get());
        }
    }

    int nrows = vectors.size();
    double[][] result = new double[nrows][];
    for (int i = 0; i < nrows; i++) {
        result[i] = vectors.get(i);
    }

    return result;
}

```

这里，我们使用之前创建的`WordsEmbeddings`类，然后对于每个令牌，我们查找它的表示，如果它存在，我们就把它放入一个矩阵中。

然后，获得所有相似性只是两个嵌入矩阵的乘法运算:

```
private static double[] similarities(double[][] m1, double[][] m2) {
    DenseMatrix M1 = new DenseMatrix(m1);
    DenseMatrix M2 = new DenseMatrix(m2);
    DenseMatrix M1M2 = new DenseMatrix(M1.numRows(), M2.numRows());
    M1.transBmult(M2, M1M2);
    return M1M2.getData();
}

```

众所周知，MTJ 将矩阵的值按列存储在一维数据数组中，之前，我们将其转换为二维数组。在这种情况下，我们并不真的需要这样做，所以我们按原样取这些值。

现在，给定一个查询列表和一个来自其他字段(例如，title)的标记列表，我们计算分布特征:

```
int size = query.size();

List<Double> mins = new ArrayList<>(size);
List<Double> means = new ArrayList<>(size);
List<Double> maxs = new ArrayList<>(size);
List<Double> stds = new ArrayList<>(size);

for (int i = 0; i < size; i++) {
    double[][] queryEmbed = wordsToVec(glove, query.get(i));
    double[][] textEmbed = wordsToVec(glove, text.get(i));
    double[] similarities = similarities(queryEmbed, textEmbed);

    DescriptiveStatistics stats = new DescriptiveStatistics(similarities);
    mins.add(stats.getMin());
    means.add(stats.getMean());
    maxs.add(stats.getMax());
    stds.add(stats.getStandardDeviation());
}

```

当然，在这里我们可以添加更多的特性，比如 25 或 75 个百分点，但是现在这四个特性已经足够了。请注意，有时 queryEmbed 或 textEmbed 可以为空，我们需要通过向每个列表添加多个`NaN`实例来处理这种情况。

我们还提到了另一个有用的特征，平均向量之间的相似性。我们以类似的方式计算:

```
List<Double> avgCos = new ArrayList<>(size);
for (int i = 0; i < size; i++) {
    double[] avgQuery = averageVector(wordsToVec(glove, query.get(i)));
    double[] avgText = averageVector(wordsToVec(glove, text.get(i)));
    avgCos.add(dot(avgQuery, avgText));
}

```

这里，点是两个向量的内积，`averageVector`是这样实现的:

```
private static double[] averageVector(double[][] rows) {
    ArrayRealVector acc = new ArrayRealVector(rows[0], true);
    for (int i = 1; i < rows.length; i++) {
        ArrayRealVector vec = new ArrayRealVector(rows[0], false);
        acc.combineToSelf(1.0, 1.0, vec);
    }

    double norm = acc.getNorm();
    acc.mapDivideToSelf(norm);
    return acc.getDataRef();
}

```

一旦我们计算了所有这些特征，我们就可以把它们放入一个 doubles 数组中，并用它来训练一个分类器。有许多可能的型号可供我们选择。

例如，我们可以使用 Smile 中的随机森林分类器:通常，基于树的方法非常善于发现特征之间的复杂交互，这些方法对于学习任务排序非常有效。

还有一件事我们还没有讨论:如何评价排名结果。对于排名模型有特殊的评估指标，如**平均精度** ( **图**)或**归一化贴现累计增益** ( **NDCG** )，但对于我们目前的情况 AUC 绰绰有余。回想一下，对 AUC 的一种可能解释是，它对应于随机选择的阳性样本的排名高于随机选择的阴性样本的概率。

因此，AUC 非常适合这项任务，在我们的实验中，随机森林模型实现了 98%的 AUC。在这一节中，我们省略了一些代码，但是和往常一样，完整的代码可以在代码包中找到，您可以更详细地浏览特征提取管道。



# 用 Lucene 重新排序

在这一章中，我们已经提到 Lucene 是可以定制的，我们已经了解了如何在 Lucene 之外进行预处理，然后将结果无缝集成到 Lucene 工作流中。

当涉及到对搜索结果重新排序时，情况或多或少是相似的。常见的方法是按原样获取 Lucene 排名，并检索前 100 个(或更多)结果。然后，我们获取这些已经检索到的文档，并将排序模型应用于此以进行重新排序。

如果我们有这样一个重新排序的模型，我们需要确保我们存储了所有用于训练的数据。在我们的例子中，它是一个`QueryDocumentPair`类，我们从中提取相关性特征。所以让我们创建一个索引:

```
FSDirectory directory = FSDirectory.open(index);
WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(analyzer));

List<HtmlDocument> docs = // read documents for indexing

for (HtmlDocument htmlDoc : docs.) {
    String url, title, bodyText, ... // extract the field values
    Document doc = new Document();
    doc.add(new Field("url", url, URL_FIELD));
    doc.add(new Field("title", title, TEXT_FIELD));
    doc.add(new Field("bodyText", bodyText, TEXT_FIELD));
    doc.add(new Field("allHeaders", allHeaders, TEXT_FIELD));
    doc.add(new Field("h1", h1, TEXT_FIELD));
    doc.add(new Field("h2", h2, TEXT_FIELD));
    doc.add(new Field("h3", h3, TEXT_FIELD));

    writer.addDocument(doc);
}

writer.commit();
writer.close();
directory.close();

```

在这段代码中，`HtmlDocument`是一个存储文档细节的类——它们的标题、正文、标题等等。我们遍历所有的文档，并将这些信息放入 Lucene 的索引中。

在本例中，所有字段都被存储，因为稍后在查询时，我们将需要检索这些值并使用它们来计算特性。

这样，索引就建立起来了，现在，让我们来查询它:

```
RandomForest rf = load("project/random-forest-model.bin");

FSDirectory directory = FSDirectory.open(index);
DirectoryReader reader = DirectoryReader.open(directory);
IndexSearcher searcher = new IndexSearcher(reader);

WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
AnalyzingQueryParser parser = new AnalyzingQueryParser("bodyText", analyzer);

String userQuery = "cheap used cars";
Query query = parser.parse(userQuery);

TopDocs result = searcher.search(query, 100);

List<QueryDocumentPair> data = wrapResults(userQuery, searcher, result);
double[][] matrix = extractFeatures(data);
double[] probs = predict(rf, matrix);

List<ScoredIndex> scored = wrapAsScoredIndex(probs);
for (ScoredIndex idx : scored) {
    QueryDocumentPair doc = data.get(idx.getIndex());
    System.out.printf("%.4f: %s, %s%n", idx.getScore(), doc.getTitle(), doc.getUrl());
}

```

在这段代码中，我们首先读取之前训练和保存的模型，然后读取索引。接下来，用户给出一个查询，我们解析它，并从索引中检索前 100 个结果。我们需要的所有值都存储在索引中，所以我们获取它们并将它们放入`QueryDocumentPair+`——这是在`wrapResults`方法中发生的事情。然后，我们提取特征，应用随机森林模型，并在将结果呈现给用户之前，使用分数对结果进行重新排序。

在特征提取步骤，遵循我们用于训练的完全相同的程序是非常重要的。否则，模型结果可能是无意义的或误导的。实现这一点的最佳方法是创建一个特殊的方法来提取特征，并在训练模型和查询时使用它。如果你需要返回 100 个以上的结果，你可以对 Lucene 返回的前 100 个条目进行重新排序，但是对 100 个以上的条目保持原来的顺序。实际上，用户很少会超过第一页，所以到达第 100 个条目的可能性很小，所以我们通常不需要麻烦地在那里重新排序文档。

让我们仔细看看`wrapResults`方法的内容:

```
List<QueryDocumentPair> data = new ArrayList<>();

for (ScoreDoc scored : result.scoreDocs) {
    int docId = scored.doc;
    Document doc = searcher.doc(docId);

    String url = doc.get("url");
    String title = doc.get("title");
    String bodyText = doc.get("bodyText");
    String allHeaders = doc.get("allHeaders");
    String h1 = doc.get("h1");
    String h2 = doc.get("h2");
    String h3 = doc.get("h3");

    QueryDocumentPair pair = new QueryDocumentPair(userQuery, 
            url, title, bodyText, allHeaders, h1, h2, h3);
    data.add(pair);
}

```

因为所有的字段都被存储了，所以我们可以从索引中获取它们并构建`QueryDocumentPair`对象。然后，我们只需应用完全相同的程序进行特征提取，并将它们放入我们的模型中。

这样，我们就创建了一个基于 Lucene 的搜索引擎，然后使用机器学习模型对查询结果进行重新排序。还有很大的进一步改进空间:可以添加更多功能或获得更多训练数据，也可以尝试使用不同的模型。在下一章，我们将讨论 XGBoost，它也可以用于学习任务排序。



# 摘要

在这一章中，我们涵盖了信息检索和自然语言处理领域的许多基础知识，包括信息检索的基础知识以及如何将机器学习应用于文本。在这样做的时候，我们首先实现了一个简单的搜索引擎，然后在 Apache Lucene 的基础上使用了一种学习排序的方法来实现一个工业级的 IR 模型。

在下一章，我们将看看梯度提升机器，以及 XGBoost，这种算法的一种实现。这个库为许多数据科学问题提供了最先进的性能，包括分类、回归和排序。