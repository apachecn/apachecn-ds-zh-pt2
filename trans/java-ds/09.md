# 第九章。文本分析

文本分析是一个广泛的话题，通常被称为**自然语言处理** ( **NLP** )。它用于许多不同的任务，包括文本搜索、语言翻译、情感分析、语音识别和分类等等。由于自然语言的特殊性和模糊性，分析的过程可能很困难。然而，在这个领域已经做了大量的工作，并且有几个 Java APIs 支持这项工作。

我们将从介绍 NLP 中使用的基本概念和任务开始。其中包括以下内容:

*   **记号化**:将文本拆分成单个记号或单词的过程。
*   **停用词**:这些是常用词，可能不需要处理。它们包括诸如 the、a 和 to 这样的词。
*   名称实体识别 ( **NER** ):这是识别文本元素的过程，比如人名、地点或事物。
*   **词类** ( **词性**):这标识了一个句子的语法部分，如名词、动词、形容词等等。
*   **关系**:这里，我们关心的是识别文本的各个部分是如何相互关联的，比如句子的主语和宾语。

单词、句子和段落的概念是众所周知的。然而，提取和分析这些组件并不总是那么简单。术语**语料库**通常指文本的集合。

与大多数数据科学问题一样，预处理文本非常重要。通常，这包括处理以下任务:

*   处理 Unicode
*   将文本转换为大写或小写
*   删除停用词

我们在第三章、*数据清理*中研究了几种用于标记化和移除停用词的技术。在这一章中，我们将关注词性、NER、从句子中提取关系、文本分类和情感分析。

有几个可用的 NLP APIs，包括:

*   OpenNLP(【https://opennlp.apache.org/】T2):一个开源的 Apache 项目
*   **斯坦福****NLP**(【http://nlp.stanford.edu/software/】T4):另一个开源库
*   **([https://uima.apache.org/](https://uima.apache.org/)):阿帕奇项目配套管道**
*   **LingPipe(【http://alias-i.com/lingpipe/】T2):大量使用管道的库**
*   ****DL4J**(【http://deeplearning4j.org/】T2):Java 深度学习库支持深度学习神经网络的各种类，包括对 NLP 的支持**

**本章我们将使用 OpenNLP 和 DL4J 来演示文本分析。我们选择这些是因为它们都很有名，并且有很好的出版资源来获得额外的支持。**

**我们将使用谷歌 **Word2Vec** 和 **Doc2Vec** 神经网络来执行文本分类。这包括基于其他单词的特征向量以及使用标记信息来分类文档。最后，我们将讨论情感分析。这种类型的分析试图给文本赋予意义，并使用 Word2Vec 网络。**

**我们从 NER 开始讨论。**

# **实现命名实体识别**

**这有时被称为寻找人和事物。给定一个文本片段，我们可能想要识别所有在场人员的姓名。然而，这并不总是容易的，因为像 Rob 这样的名字也可能被用作动词。**

**在这一节中，我们将演示如何使用 OpenNLP 的`TokenNameFinderModel`类来查找文本中的名称和位置。虽然我们可能还想找到其他实体，但这个例子将展示这项技术的基础。我们从名字开始。**

**大多数名字出现在一行中。我们不希望使用多行，因为像州这样的实体可能会在无意中被错误地识别。考虑下面的句子:**

**吉姆向北走了。达科塔往南走了。**

**如果我们忽略这个时期，那么北达科他州可能会被识别为一个位置，而实际上它并不存在。**

## **使用 OpenNLP 执行 NER**

**我们从处理异常的 try-catch 块开始我们的例子。OpenNLP 使用在不同数据集上训练过的模型。在这个例子中，`en-token.bin`和`en-ner-person.bin`文件分别包含英语文本的标记化和英语名称元素的模型。这些文件可以从 http://opennlp.sourceforge.net/models-1.5/的[下载。然而，这里使用的 IO 流是标准的 Java:](http://opennlp.sourceforge.net/models-1.5/)**

```
try (InputStream tokenStream =  
            new FileInputStream(new File("en-token.bin")); 
        InputStream personModelStream = new FileInputStream( 
            new File("en-ner-person.bin"));) { 
    ... 
} catch (Exception ex) { 
    // Handle exceptions 
} 
```

**使用令牌流初始化`TokenizerModel`类的一个实例。然后这个实例被用来创建实际的`TokenizerME`标记器。我们将用这个例子来修饰我们的句子:**

```
TokenizerModel tm = new TokenizerModel(tokenStream); 
TokenizerME tokenizer = new TokenizerME(tm); 
```

**`TokenNameFinderModel`类用于保存命名实体的模型。它是使用人模型流初始化的。由于我们正在寻找名称，因此使用这个模型创建了一个`NameFinderME`类的实例:**

```
TokenNameFinderModel tnfm = new 
  TokenNameFinderModel(personModelStream); 
NameFinderME nf = new NameFinderME(tnfm); 
```

**为了演示这个过程，我们将使用下面的句子。然后，我们使用 tokenizer 和`tokenizer`方法将它转换成一系列标记:**

```
String sentence = "Mrs. Wilson went to Mary's house for dinner."; 
String[] tokens = tokenizer.tokenize(sentence); 
```

**`Span`类保存关于实体位置的信息。`find`方法将返回位置信息，如下所示:**

```
Span[] spans = nf.find(tokens); 
```

**这个数组保存在句子中找到的 person 实体的信息。然后，我们显示这些信息，如下所示:**

```
for (int i = 0; i < spans.length; i++) { 
    out.println(spans[i] + " - " + tokens[spans[i].getStart()]); 
} 
```

**这个序列的输出如下。请注意，它标识了威尔逊夫人的姓，而不是“夫人”:**

```
**[1..2) person - Wilson**
**[4..5) person - Mary** 
```

**一旦这些实体被提取出来，我们就可以使用它们进行专门的分析。**

## **识别位置实体**

**我们还可以找到其他类型的实体，如日期和位置。在下面的例子中，我们找到了句子中的位置。它与前面的 person 示例非常相似，除了模型使用了一个`en-ner-location.bin`文件:**

```
try (InputStream tokenStream =  
            new FileInputStream("en-token.bin"); 
        InputStream locationModelStream = new FileInputStream( 
            new File("en-ner-location.bin"));) { 

    TokenizerModel tm = new TokenizerModel(tokenStream); 
    TokenizerME tokenizer = new TokenizerME(tm); 

    TokenNameFinderModel tnfm =  
        new TokenNameFinderModel(locationModelStream); 
    NameFinderME nf = new NameFinderME(tnfm); 

    sentence = "Enid is located north of Oklahoma City."; 
    String tokens[] = tokenizer.tokenize(sentence); 

    Span spans[] = nf.find(tokens); 

    for (int i = 0; i < spans.length; i++) { 
        out.println(spans[i] + " - " +  
        tokens[spans[i].getStart()]); 
    } 
} catch (Exception ex) { 
    // Handle exceptions 
} 
```

**使用前面定义的句子，模型只能找到第二个城市，如下所示。这可能是由于名字`Enid`引起的混淆，它既是一个城市的名字也是一个人的名字:**

```
**[5..7) location - Oklahoma** 
```

**假设我们使用下面的句子:**

```
sentence = "Pond Creek is located north of Oklahoma City."; 
```

**然后我们得到这个输出:**

```
 ****[1..2) location - Creek
[6..8) location - Oklahoma**** 
```

**不幸的是，它错过了`Pond Creek`镇。NER 是许多应用程序的有用工具，但像许多技术一样，它并不总是万无一失的。所介绍的 NER 方法以及许多其他 NLP 示例的准确性将根据诸如模型的准确性、所使用的语言以及实体的类型等因素而变化。**

**我们也可能对文本如何分类感兴趣。我们将在下一节研究一种方法。**

 **# 文本分类

对文本进行分类是机器学习和数据科学的重要组成部分。我们必须能够为各种应用对文本进行分类，包括文档检索和网络搜索。在我们确定数据对特定应用程序或搜索结果的有用性之前，给数据分配特定的标签通常是很重要的。

在这一章中，我们将展示一种技术，这种技术涉及到使用 DL4J 类的段落向量和标签数据。这个例子允许我们读入文档，并根据文档内部的文本，给文档分配一个标签(或分类)。我们还将展示一个根据相似性对文本进行分类的例子。这意味着我们将匹配具有相似结构的短语和单词。这个例子也将使用 DL4J。

## Word2Vec 和 Doc2Vec

我们将在本章的几个例子中使用 Word2Vec 和 Doc2Vec。Word2Vec 是用于文本处理的具有两层的神经网络。给定文本主体，网络将为文本中包含的单词提供特征向量。这些向量是单词特征的简单数学表示，并且可以在数字上与其他向量进行比较。这种比较通常被称为两个单词之间的距离。

Word2Vec 的工作原理是，通过确定两个单词同时出现的概率，可以对单词进行分类。由于这种方法，Word2Vec 不仅可以用于句子的分类。任何可以用文本标签表示的对象或数据都可以用这个网络进行分类。

Doc2Vec 是 Word2Vec 的扩展。这个网络不是像 Word2Vec 那样建立代表单个单词与其他单词相比的特征的向量，而是将单词与给定的标签进行比较。向量被设置来表示文档的主题或整体含义。我们的下一个例子展示了这些特征向量是如何与特定的文档相关联的。

## 通过标签对文本进行分类

在我们使用 Doc2Vec 的第一个例子中，我们将我们的文档与三个标签相关联:健康、金融和科学。但是在我们可以将数据与标签相关联之前，我们必须定义这些标签，并训练我们的模型来识别这些标签。每个标签代表一段特定文本的含义或分类。

在这个例子中，我们将使用样本文档，每个文档都预先标注了我们的类别:健康、金融或科学。我们将使用这些段落来训练我们的模型，然后像前面的例子一样，使用一组测试数据来测试我们的模型。我们将使用位于[https://github . com/deep learning 4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources/para vec](https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources/paravec)的文件。我们基于为 DL4J 编写的示例代码编写了这个示例，可以在[https://github . com/deep learning 4j/DL4J-examples/blob/master/DL4J-examples/src/main/Java/org/deep learning 4j/examples/NLP/paragraph vectors/paragraphvectors classifier example . Java](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/paragraphvectors/ParagraphVectorsClassifierExample.java)找到。

首先，我们需要设置一些实例变量，以便稍后在代码中使用。我们将使用一个`ParagraphVectors`对象来创建我们的向量，一个`LabelAwareIterator`对象来遍历我们的数据，一个`TokenizerFactory`对象来标记我们的数据:

```
ParagraphVectors pVect; 
LabelAwareIterator iter; 
TokenizerFactory tFact; 

```

然后我们将设置我们的`ClassPathResource`。这指定了我们的项目中包含要分类的数据文件的目录。第一个资源包含我们用于培训目的的标记数据。然后，我们指示迭代器和标记器使用指定为`ClassPathResource`的资源。我们还指定将使用`CommonPreprocessor`来预处理我们的数据:

```
ClassPathResource resource = new  
         ClassPathResource("paravec/labeled"); 

iter = new FileLabelAwareIterator.Builder() 
        .addSourceFolder(resource.getFile()) 
        .build(); 

tFact = new DefaultTokenizerFactory(); 
tFact.setTokenPreProcessor(new CommonPreprocessor()); 

```

接下来，我们构建我们的`ParagraphVectors`。这是我们指定学习率、批量大小和训练时期数量的地方。我们还在设置过程中包含了迭代器和标记器。一旦我们构建了我们的`ParagraphVectors`，我们调用`fit`方法来使用`paravec/labeled`目录中的训练数据训练我们的模型:

```
pVect = new ParagraphVectors.Builder() 
        .learningRate(0.025) 
        .minLearningRate(0.001) 
        .batchSize(1000) 
        .epochs(20) 
        .iterate(iter) 
        .trainWordVectors(true) 
        .tokenizerFactory(tFact) 
        .build(); 

pVect.fit(); 

```

现在我们已经训练了我们的模型，我们可以使用我们的未标记数据进行测试。我们为未标记的数据创建一个新的`ClassPathResource`,并创建一个新的`FileLabelAwareIterator`:

```
ClassPathResource unlabeledText =  
         new ClassPathResource("paravec/unlabeled"); 
FileLabelAwareIterator unlabeledIter =  
         new FileLabelAwareIterator.Builder() 
               .addSourceFolder(unlabeledText.getFile()) 
               .build(); 

```

下一步涉及遍历我们的未标记数据，并为每个文档识别正确的标签。一般来说，我们可以预期每个文档将属于多个标签，但是每个标签具有不同的权重或匹配百分比。因此，虽然一篇文章可能主要被归类为健康文章，但它可能有足够的信息也被归类为科学文章，只是程度较低。

接下来，我们设置一个`MeansBuilder`和`LabelSeeker`对象。这些类访问包含单词和标签之间关系的表，我们将在我们的`ParagraphVectors`中使用这些表。`InMemoryLookupTable`类提供对单词查找的默认表的访问:

```
MeansBuilder mBuilder =  
   new MeansBuilder((InMemoryLookupTable<VocabWord>)  
      pVect.getLookupTable(),tFact); 
LabelSeeker lSeeker =  
    new LabelSeeker(iter.getLabelsSource().getLabels(), 
               (InMemoryLookupTable<VocabWord>)
    pVect.getLookupTable()); 

```

最后，我们遍历未标记的文档。对于每个文档，我们将把文档转换成一个向量，并使用我们的`LabelSeeker`来获得每个文档的分数。我们记录每个文档的分数，并打印出带有相应标签的分数:

```
while (unlabeledIter.hasNextDocument()) { 
    LabelledDocument doc = unlabeledIter.nextDocument(); 
    INDArray docCentroid = mBuilder.documentAsVector(doc); 
    List<Pair<String, Double>> scores =  
              lSeeker.getScores(docCentroid); 
    out.println("Document '" + doc.getLabel() +  
       "' falls into the following categories: "); 
    for (Pair<String, Double> score : scores) { 
       out.println ("        " + score.getFirst() + ": " +  
             score.getSecond()); 
        } 

} 

```

我们前面的打印语句的输出如下:

```
Document 'finance' falls into the following categories: 
finance: 0.2889593541622162
health: 0.11753179132938385
science: 0.021202782168984413
Document 'health' falls into the following categories: 
finance: 0.059537000954151154
health: 0.27373185753822327
science: 0.07699354737997055

```

在每一个例子中，我们的文档都被正确分类，正如分配到正确标签类别的百分比较高所证明的那样。这种分类可以与其他数据分析技术结合使用，以得出有关文件中包含的数据的其他结论。通常，文本分类是数据分析过程中的初始或早期步骤，因为文档被分类成组以供进一步分析。

## 根据相似度对文本进行分类

在下一个例子中，我们将根据结构和相似性来匹配不同的文本样本。我们将仍然使用我们在前面的例子中使用的`ParagraphVectors`类。首先，从 GitHub([https://GitHub . com/deep learning 4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources](https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/resources))下载`raw_sentences.txt`文件，并将其添加到您的项目中。这个文件包含一个句子列表，我们将读入这些句子，标记它们，然后进行比较。

首先，我们设置我们的`ClassPathResource`并分配一个迭代器来处理我们的文件数据。我们在这个例子中使用了一个`SentenceIterator`:

```
ClassPathResource srcFile = new  
      ClassPathResource("/raw_sentences.txt"); 
File file = srcFile.getFile(); 
SentenceIterator iter = new BasicLineIterator(file); 

```

接下来，我们将再次使用`TokenizerFactory`来标记我们的数据。我们还想创建一个新的`LabelsSource`对象。这允许我们定义句子标签的格式。我们选择在每一行前面加上`LINE_`:

```
TokenizerFactory tFact = new DefaultTokenizerFactory(); 
tFact.setTokenPreProcessor(new CommonPreprocessor()); 
LabelsSource labelFormat = new LabelsSource("LINE_"); 

```

现在我们已经准备好构建我们的`ParagraphVectors`。我们的设置过程包括这些方法:`minWordFrequency`，它指定在训练语料库中使用的最小词频，以及`iterations`，它指定每个小批量的迭代次数。我们还设置了历元数、层大小和学习速率。此外，我们包括前面定义的`LabelsSource`，以及我们的迭代器和标记器。`trainWordVectors`方法指定了单词和文档表示是否应该一起构建。最后，`sampling`确定是否应该进行二次采样。然后我们调用我们的`build`和`fit`方法:

```
ParagraphVectors vec = new ParagraphVectors.Builder() 
        .minWordFrequency(1) 
        .iterations(5) 
        .epochs(1) 
        .layerSize(100) 
        .learningRate(0.025) 
        .labelsSource(labelFormat) 
        .windowSize(5) 
        .iterate(iter) 
        .trainWordVectors(false) 
        .tokenizerFactory(tFact) 
        .sampling(0) 
        .build(); 

vec.fit(); 

```

接下来，我们将包含一些语句来评估我们分类的准确性。值得注意的是，虽然文档本身从`1`开始，但是索引过程从`0`开始。例如，文档中的行`9836`将与标签`LINE_9835`相关联。我们将首先比较三个应该归类为有些相似的句子，然后比较两个不相似的句子。`similarity`方法获取两个标签，并以`double`的形式返回它们之间的相对距离:

```
double similar1 = vec.similarity("LINE_9835", "LINE_12492"); 
out.println("Comparing lines 9836 & 12493  
       ('This is my house .'/'This is my world .')  
       Similarity = " + similar1); 

double similar2 = vec.similarity("LINE_3720", "LINE_16392"); 
out.println("Comparing lines 3721 & 16393  
       ('This is my way .'/'This is my work .')  
       Similarity = " + similar2); 

double similar3 = vec.similarity("LINE_6347", "LINE_3720"); 
out.println("Comparing lines 6348 & 3721  
       ('This is my case .'/'This is my way .')  
       Similarity = " + similar3); 

double dissimilar1 = vec.similarity("LINE_3720", "LINE_9852"); 
out.println("Comparing lines 3721 & 9853  
       ('This is my way .'/'We now have one .')  
       Similarity = " + dissimilar1); 

double dissimilar2 = vec.similarity("LINE_3720", "LINE_3719"); 
out.println("Comparing lines 3721 & 3720  
       ('This is my way .'/'At first he says no .')  
       Similarity = " + dissimilar2); 

```

我们的打印语句的输出如下所示。比较`similarity`方法对三个相似句子和两个不相似句子的结果。特别要注意的是，最后一个例子的`similarity`方法结果，两个非常不同的句子，返回了一个负数。这意味着更大的差异:

```
16:56:15.423 [main] INFO o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3171540]; Lines vectorized so far: [485810]; learningRate: [1.0E-4]
Comparing lines 9836 & 12493 ('This is my house .'/'This is my world .') Similarity = 0.7641470432281494
Comparing lines 3721 & 16393 ('This is my way .'/'This is my work .') Similarity = 0.7246013879776001
Comparing lines 6348 & 3721 ('This is my case .'/'This is my way .') Similarity = 0.8988922834396362
Comparing lines 3721 & 9853 ('This is my way .'/'We now have one .') Similarity = 0.5840312242507935
Comparing lines 3721 & 3720 ('This is my way .'/'At first he says no .') Similarity = -0.6491150259971619

```

虽然这个例子像我们的第一个分类例子一样使用了`ParagraphVectors`,但是这展示了我们方法的灵活性。我们可以使用这些 DL4J 库以多种方式对数据进行分类。

# 了解标记和位置

词性涉及识别句子中的成分类型。比如这个句子有几个成分，包括动词“has”，几个名词如“example”、“elements”，形容词如“几个”。标记，或者更具体地说**词性标记**，是将元素类型与单词相关联的过程。

词性标注很有用，因为它增加了关于句子的更多信息。我们可以确定单词之间的关系以及它们的相对重要性。标记的结果通常用于后面的处理步骤。

这项任务可能很困难，因为我们不能依靠简单的词典来确定它们的类型。例如，单词`lead`既可以用作名词，也可以用作动词。我们可以在下面两个句子中使用它:

```
He took the lead in the play.
Lead the way!

```

词性标注会尝试将正确的标签与句子中的每个单词相关联。

## 使用 OpenNLP 识别 POS

为了说明这个过程，我们将使用 OpenNLP([https://opennlp.apache.org/](https://opennlp.apache.org/))。这是一个开源的 Apache 项目，支持许多其他的 NLP 处理任务。

我们将使用`POSModel`类，它可以被训练来识别 POS 元素。在这个例子中，我们将把它与先前基于**佩恩树库** **标签集**([http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html](http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html))训练的模型一起使用。在[http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/)可以找到各种经过预训练的模型。我们将使用`en-pos-maxent.bin`型号。这已经用所谓的最大熵在英语文本上进行了训练。

最大熵指的是模型中不确定性的数量最大化。对于一个给定的问题，有一组概率描述了数据集的已知情况。这些概率用于建立模型。例如，我们可能知道有 23%的可能性某个特定事件会遵循某个条件。我们不想对未知的概率做任何假设，所以我们避免添加不合理的信息。最大熵方法试图保持尽可能多的不确定性；因此它试图最大化熵。

我们还将使用`POSTaggerME`类，它是一个最大熵标记器。这是将进行标签预测的类。对于任何一个句子，都可能有不止一种方式来对其成分进行分类或标记。

我们从代码开始，以获取先前训练的英语标记器模型和要标记的简单句子:

```
try (InputStream input = new FileInputStream( 
        new File("en-pos-maxent.bin"));) { 
    String sentence = "Let's parse this sentence."; 
    ... 
} catch (IOException ex) { 
    // Handle exceptions 
} 

```

标记器使用字符串数组，其中每个字符串都是一个单词。下面的序列采用前面的句子并创建一个名为`words`的数组。第一部分使用`Scanner`类解析句子字符串。如果需要，我们可以使用其他代码从文件中读取数据。之后，`List`类的`toArray`方法用于创建字符串数组:

```
List<String> list = new ArrayList<>(); 
Scanner scanner = new Scanner(sentence); 
while(scanner.hasNext()) { 
    list.add(scanner.next()); 
} 
String[] words = new String[1]; 
words = list.toArray(words); 

```

然后，使用包含模型的文件构建模型:

```
POSModel posModel = new POSModel(input); 

```

然后根据模型创建标记:

```
POSTaggerME posTagger = new POSTaggerME(posModel); 

```

`tag`方法完成实际的工作。它被传递一个单词数组并返回一个标签数组。然后显示单词和标签:

```
String[] posTags = posTagger.tag(words); 
for(int i=0; i<posTags.length; i++) { 
    out.println(words[i] + " - " + posTags[i]); 
} 

```

此示例的输出如下:

```
Let's - NNP
parse - NN
this - DT
sentence. - NN

```

分析已经确定单词`let's`是单数专有名词，而单词`parse`和`sentence`是单数名词。`this`这个词是一个限定词，也就是说，它是一个修饰另一个词的词，有助于识别一个短语是一般的还是特定的。下一节提供了标签列表。

## 了解 POS 标签

POS 元素返回缩写。在[https://www . ling . upenn . edu/courses/Fall _ 2003/ling 001/Penn _ tree bank _ pos . html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)可以找到 **Penn TreeBankPOS** 标签列表。以下是该列表的简短版本:

| **标签** | **描述** | **标签** | **描述** |
| `DT` | 限定词 | `RB` | 副词 |
| `JJ` | 形容词 | `RBR` | 副词，比较 |
| `JJR` | 形容词，比较级 | `RBS` | 副词，最高级 |
| `JJS` | 形容词，最高级 | `RP` | 颗粒 |
| `NN` | 名词，单数或复数 | `SYM` | 标志 |
| `NNS` | Noun, plural | `TOP` | 解析树的顶部 |
| `NNP` | 专有名词，单数 | `VB` | 动词，基本形式 |
| `NNPS` | 专有名词，复数 | `VBD` | 动词，过去式 |
| `POS` | 所有格结尾 | `VBG` | 动词、动名词或现在分词 |
| `PRP` | 人称代词 | `VBN` | 动词，过去分词 |
| `PRP$` | 所有格代名词 | `VBP` | 动词，非第三人称单数现在时 |
| `S` | 简单陈述句 | `VBZ` | 动词，第三人称单数现在时 |

如前所述，一个句子可能有不止一组可能的词性赋值。如下所示的`topKSequences`方法将返回各种可能的赋值和分数。该方法返回一个由`Sequence`对象组成的数组，这些对象的`toString`方法返回分数和位置列表:

```
    Sequence sequences[] = posTagger.topKSequences(words); 
    for(Sequence sequence : sequences) { 
        out.println(sequence); 
    } 

```

前一个句子的输出如下，其中最后一个序列被认为是最可能的选择:

```
-2.3264880694837213 [NNP, NN, DT, NN]
-2.6610271245387853 [NNP, VBD, DT, NN]
-2.6630142638557217 [NNP, VB, DT, NN]

```

每行输出为句子中的每个单词分配可能的标签。我们可以看到，只有第二个单词`parse`被确定为具有其他可能的标签。

接下来，我们将演示如何从文本中提取关系。

# 从句子中提取关系

在许多分析任务中，了解句子元素之间的关系是很重要的。它有助于评估句子的重要内容并洞察句子的含义。这种类型的分析已经被用于从语法检查到语音识别到语言翻译的任务。

在上一节中，我们演示了一种用于提取词性的方法。使用这种技术，我们能够识别句子中存在的句子成分类型。然而，这些元素之间的关系是缺失的。我们需要解析句子来提取句子元素之间的关系。

## 使用 OpenNLP 提取关系

有几种技术和 API 可以用来提取这种类型的信息。在这一节中，我们将使用 OpenNLP 来演示一种提取句子结构的方法。演示围绕着`ParserTool`类展开，它使用了一个以前训练过的模型。解析过程将返回句子中提取的元素正确的概率。正如许多 NLP 任务一样，通常可能有多个答案。

我们从 try-with-resource 块开始，为模型打开一个输入流。`en-parser-chunking.bin`文件包含一个将文本解析成 POS 的模型。在这种情况下，它是为英语而训练的:

```
try (InputStream modelInputStream = new FileInputStream( 
            new File("en-parser-chunking.bin"));) { 
    ... 
} catch (Exception ex) { 
    // Handle exceptions 
}  

```

在 try 块中，使用输入流创建了一个`ParserModel`类的实例。接下来使用`ParserFactory`类的`create`方法创建实际的解析器:

```
ParserModel parserModel = new ParserModel(modelInputStream); 
Parser parser = ParserFactory.create(parserModel); 

```

我们将使用下面的句子来测试解析器。`ParserTool`类的`parseLine`方法执行实际的解析并返回一个`Parse`对象的数组。这些对象中的每一个都有一个解析选项。`parseLine`方法的最后一个参数指定返回多少个备选方案:

```
String sentence = "Let's parse this sentence."; 
Parse[] parseTrees = ParserTool.parseLine(sentence, parser, 3); 

```

下一个序列显示了每种可能性:

```
for(Parse tree : parseTrees) { 
    tree.show(); 
} 

```

本例中 show 方法的输出如下。标签先前已在*了解位置标签*一节中定义:

```
(TOP (NP (NP (NNP Let's) (NN parse)) (NP (DT this) (NN sentence.))))
(TOP (S (NP (NNP Let's)) (VP (VB parse) (NP (DT this) (NN sentence.)))))
(TOP (S (NP (NNP Let's)) (VP (VBD parse) (NP (DT this) (NN sentence.)))))

```

以下示例将最后两个输出重新格式化，以更好地显示关系。他们对动词 parse 的分类不同:

```
(TOP 
(S 
(NP (NNP Let's)) 
(VP (VB parse) 
(NP (DT this) (NN sentence.))
)
)
)
(TOP 
(S 
(NP (NNP Let's)) 
(VP (VBD parse) 
(NP (DT this) (NN sentence.)) 
)
)
)

```

当有多个解析选择时，`Parse`类的`getProb`返回一个概率，该概率反映了模型对选择的置信度。以下序列演示了此方法:

```
for(Parse tree : parseTrees) { 
    out.println("Probability: " + tree.getProb()); 
} 

```

输出如下:

```
Probability: -3.6810244423259078
Probability: -3.742475884515823
Probability: -4.16148634555491

```

另一个有趣的 NLP 任务是情感分析，我们将在下面演示。

# 情感分析

情感分析包括基于单词的上下文、含义和情感含义对单词进行评估和分类。通常，如果我们在字典中查找一个单词，我们会找到该单词的含义或定义，但是，脱离句子的上下文，我们可能无法详细准确地描述该单词的含义。

例如，单词 toast 可以被简单地定义为一片加热并变成褐色的面包。但是在句子*的上下文中，他是烤面包！*，意思完全变了。情感分析试图根据上下文和用法推导出单词的含义。

值得注意的是，高级情感分析将超越简单的积极或消极分类，并赋予单词详细的情感含义。将单词分为积极或消极要简单得多，但将它们分为快乐、愤怒、冷漠或焦虑要有用得多。

这种类型的分析属于有效计算的范畴，一种对技术工具的情感含义和使用感兴趣的计算。鉴于如今社交媒体网站上可用于分析的受情绪影响的数据越来越多，这种类型的计算尤为重要。

能够确定文本的情感内容使得能够做出更有针对性和适当的反应。例如，能够在客户和技术代表之间的聊天会话中判断情绪反应可以让代表做得更好。当他们之间存在文化或语言差异时，这一点尤为重要。

这种类型的分析也可以应用于视觉图像。它可以用来衡量一个人对新产品的反应，比如在进行品尝测试时，或者判断人们对电影或广告场景的反应。

作为示例的一部分，我们将使用单词袋模型。词袋模型通过包含一组被称为**袋**的词来简化自然语言处理的词表示，而不考虑语法或词序。单词具有用于分类的特征，最重要的是每个单词的频率。因为有些词，如 the、a 或 and，在任何文本中自然会有较高的出现频率，所以这些词也会被赋予一定的权重。上下文重要性较低的常用词将具有较小的权重，并且在文本分析中的影响较小。

## 下载并提取 Word2Vec 模型

为了演示情感分析，我们将使用 Google 的 Word2Vec 模型和 DL4J 来简单地根据评论中使用的词将电影评论分为正面或负面。本例改编自 Alex Black 所做的工作([https://github . com/deep learning 4j/dl4j-examples/blob/master/dl4j-examples/src/main/Java/org/deep learning 4j/examples/recurrent/word 2 vessentiment/word 2 vessentimentrnn . Java](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java))。正如本章前面所讨论的，Word2Vec 由两层神经网络组成，经过训练可以从单词的上下文中构建含义。我们还将使用来自 http://ai.stanford.edu/~amaas/data/sentiment/[的大量电影评论。](http://ai.stanford.edu/~amaas/data/sentiment/)

在我们开始之前，你需要从 https://code.google.com/p/word2vec/下载 Word2Vec 数据。基本流程包括:

*   下载并提取电影评论
*   加载 Word2Vec 谷歌新闻矢量
*   加载每个电影评论

评论中的单词被分解成向量，用于训练网络。我们将在五个时期内训练网络，并在每个时期后评估网络的性能。

首先，我们先声明三个最终变量。第一个是检索训练数据的 URL，第二个是存储我们提取的数据的位置，第三个是本地机器上 Google News vectors 的位置。修改第三个变量以反映本地计算机上的位置:

```
public static final String TRAINING_DATA_URL =  
    "http://ai.stanford.edu/~amaas/" +  
    "data/sentiment/aclImdb_v1.tar.gz"; 
public static final String EXTRACT_DATA_PATH =  
    FilenameUtils.concat(System.getProperty( 
    "java.io.tmpdir"), "dl4j_w2vSentiment/"); 
public static final String GNEWS_VECTORS_PATH =  
    "C:/YOUR_PATH/GoogleNews-vectors-negative300.bin" +  
    "/GoogleNews-vectors-negative300.bin"; 

```

接下来，我们下载并提取模型数据。接下来的两个方法模仿 DL4J 示例中的代码。我们首先创建一个新方法，`getModelData`。接下来完整地示出了该方法。

首先，我们使用之前定义的`EXTRACT_DATA_PATH`创建一个新的`File`。如果文件不存在，我们创建一个新的目录。接下来，我们再创建两个`File`对象，一个用于归档 TAR 文件的路径，一个用于提取数据的路径。在我们尝试提取数据之前，我们检查这两个文件是否存在。如果存档路径不存在，我们从`TRAINING_DATA_URL`下载数据，然后提取数据。如果提取的文件不存在，那么我们提取数据:

```

private static void getModelData() throws Exception { 
    File modelDir = new File(EXTRACT_DATA_PATH); 
    if (!modelDir.exists()) { 
        modelDir.mkdir(); 
    } 
    String archivePath = EXTRACT_DATA_PATH + "aclImdb_v1.tar.gz"; 
    File archiveName = new File(archivePath); 
    String extractPath = EXTRACT_DATA_PATH + "aclImdb"; 
    File extractName = new File(extractPath); 
    if (!archiveName.exists()) { 
        FileUtils.copyURLToFile(new URL(TRAINING_DATA_URL), 
                archiveName); 
        extractTar(archivePath, EXTRACT_DATA_PATH); 
    } else if (!extractName.exists()) { 
        extractTar(archivePath, EXTRACT_DATA_PATH); 
    } 
} 

```

为了提取数据，我们将创建另一个名为`extractTar`的方法。我们将为该方法提供两个输入，之前定义的`archivePath`和`EXTRACT_DATA_PATH`。我们还需要定义提取过程中使用的缓冲区大小:

```
private static final int BUFFER_SIZE = 4096; 

```

我们首先创建一个新的`TarArchiveInputStream`。我们使用`GzipCompressorInputStream`是因为它提供了对提取`.gz`文件的支持。我们还使用`BufferedInputStream`来提高提取过程的性能。压缩文件非常大，下载和解压缩可能需要一些时间。

接下来我们创建一个`TarArchiveEntry`，并开始使用`TarArchiveInputStream` `getNextEntry`方法读入数据。当我们处理压缩文件中的条目时，我们首先检查该条目是否是一个目录。如果是，我们将在提取位置创建一个新目录。最后，我们创建一个新的`FileOutputStream`和`BufferedOutputStream`，并使用`write`方法将数据写入提取的位置:

```
private static void extractTar(String dataIn, String dataOut)
    throws IOException {
        try (TarArchiveInputStream inStream =
            new TarArchiveInputStream(
                new GzipCompressorInputStream(
                    new BufferedInputStream(
                        new FileInputStream(dataIn))))) {
            TarArchiveEntry tarFile;
            while ((tarFile = (TarArchiveEntry) inStream.getNextEntry())
                != null) {
                if (tarFile.isDirectory()) {
                    new File(dataOut + tarFile.getName()).mkdirs();
                }else {
                    int count;
                    byte data[] = new byte[BUFFER_SIZE];
                    FileOutputStream fileInStream =
                      new FileOutputStream(dataOut + tarFile.getName());
                    BufferedOutputStream outStream = 
                      new BufferedOutputStream(fileInStream,
                        BUFFER_SIZE);
                    while ((count = inStream.read(data, 0, BUFFER_SIZE))
                        != -1) {
                            outStream.write(data, 0, count);
                    }
                }
            }
        }
    }
```

## 构建我们的模型并对文本进行分类

既然我们已经创建了下载和提取数据的方法，我们需要声明和初始化用于控制模型执行的变量。我们的`batchSize`指的是我们在每个例子中处理的单词量，在这个例子中是`50`。我们的`vectorSize`决定了向量的大小。谷歌新闻模型有大小为`300`的词向量。`nEpochs`指我们尝试运行训练数据的次数。最后，`truncateReviewsToLength`指定如果电影评论超过特定长度，出于内存利用的目的，我们是否应该截断电影评论。我们选择截断超过`300`个单词的评论:

```
int batchSize = 50;      
int vectorSize = 300; 
int nEpochs = 5;         
int truncateReviewsToLength = 300;   

```

现在我们可以建立我们的神经网络了。我们将使用一个`MultiLayerConfiguration`网络，如[第 8 章](part0061.xhtml#aid-1Q5IA2 "Chapter 8. Deep Learning")、*深度学习*中所讨论的。事实上，我们这里的例子与配置和构建模型中构建的模型非常相似，只是有一些不同。特别是，在这个模型中，我们将在第 0 层使用更快的学习速率和一个`GravesLSTM`递归网络。我们将拥有与向量中单词数量相同的输入神经元，在这种情况下，`300`。我们还使用`gradientNormalization`，这是一种用来帮助我们的算法找到最优解的技术。注意我们正在使用`softmax`激活函数，这在[第 8 章](part0061.xhtml#aid-1Q5IA2 "Chapter 8. Deep Learning")、*深度学习*中讨论过。该函数使用回归，特别适用于分类算法:

```
MultiLayerConfiguration sentimentNN =  
         new NeuralNetConfiguration.Builder() 
        .optimizationAlgo(OptimizationAlgorithm 
                 .STOCHASTIC_GRADIENT_DESCENT).iterations(1) 
        .updater(Updater.RMSPROP) 
        .regularization(true).l2(1e-5) 
        .weightInit(WeightInit.XAVIER) 
        .gradientNormalization(GradientNormalization 
                 .ClipElementWiseAbsoluteValue) 
                 .gradientNormalizationThreshold(1.0) 
        .learningRate(0.0018) 
        .list() 
        .layer(0, new GravesLSTM.Builder() 
                 .nIn(vectorSize).nOut(200) 
                .activation("softsign").build()) 
        .layer(1, new RnnOutputLayer.Builder() 
                .activation("softmax") 
                .lossFunction(LossFunctions.LossFunction.MCXENT) 
                .nIn(200).nOut(2).build()) 
        .pretrain(false).backprop(true).build(); 

```

然后我们可以创建我们的`MultiLayerNetwork`，初始化网络，并设置监听器。

```
MultiLayerNetwork net = new MultiLayerNetwork(sentimentNN); 
net.init(); 
net.setListeners(new ScoreIterationListener(1)); 

```

接下来，我们创建一个`WordVectors`对象来加载我们的 Google 数据。我们使用一个`DataSetIterator`来测试和训练我们的数据。`AsyncDataSetIterator`允许我们在一个单独的线程中加载数据，以提高性能。此过程需要大量内存，因此此类改进对于优化性能至关重要:

```
WordVectors wordVectors = WordVectorSerializer
DataSetIterator trainData = new AsyncDataSetIterator(
    new SentimentExampleIterator(EXTRACT_DATA_PATH, wordVectors,
        batchSize, truncateReviewsToLength, true), 1);
DataSetIterator testData = new AsyncDataSetIterator(
    new SentimentExampleIterator(EXTRACT_DATA_PATH, wordVectors,
        100, truncateReviewsToLength, false), 1);

```

最后，我们准备好训练和评估我们的数据。我们浏览数据`nEpochs`次；在这种情况下，我们有五次迭代。每次迭代针对我们的训练数据执行`fit`方法，然后使用`testData`创建一个新的`Evaluation`对象来评估我们的模型。该评估基于大约 25，000 条电影评论，可能需要很长时间来运行。当我们评估数据时，我们创建`INDArray`来存储信息，包括来自我们数据的特征矩阵和标签。该数据稍后用于`evalTimeSeries`方法的评估。最后，我们打印出我们的评估统计数据:

```
for (int i = 0; i < nEpochs; i++) { 
    net.fit(trainData); 
    trainData.reset(); 

    Evaluation evaluation = new Evaluation(); 
    while (testData.hasNext()) { 
        DataSet t = testData.next(); 
        INDArray dataFeatures = t.getFeatureMatrix(); 
        INDArray dataLabels = t.getLabels(); 
        INDArray inMask = t.getFeaturesMaskArray(); 
        INDArray outMask = t.getLabelsMaskArray(); 
        INDArray predicted = net.output(dataFeatures, false,  
            inMask, outMask); 

        evaluation.evalTimeSeries(dataLabels, predicted, outMask); 
    } 
    testData.reset(); 

    out.println(evaluation.stats()); 
} 

```

最终迭代的输出如下所示。我们归类为`0`的示例被视为负面评价，归类为`1`的示例被视为正面评价:

```
Epoch 4 complete. Starting evaluation:
Examples labeled as 0 classified by model as 0: 11122 times
Examples labeled as 0 classified by model as 1: 1378 times
Examples labeled as 1 classified by model as 0: 3193 times
Examples labeled as 1 classified by model as 1: 9307 times
==========================Scores===================================Accuracy: 0.8172
Precision: 0.824
Recall: 0.8172
F1 Score: 0.8206
===================================================================

```

如果与以前的迭代相比，您应该注意到分数和准确性随着每次评估而提高。随着每次迭代，我们的模型在将电影评论分类为负面或正面方面提高了其准确性。



# 总结

在本章中，我们介绍了一些 NLP 任务，并展示了它们是如何被支持的。特别是，我们使用了 OpenNLP 和 DL4J 来说明它们是如何执行的。虽然还有许多其他可用的库，但这些示例很好地介绍了这些技术。

我们首先介绍了基本的 NLP 术语和概念，比如命名实体识别、词性以及句子元素之间的关系。命名实体识别涉及查找和标记句子的各个部分，如人、地点和事物。词性将标签与句子的元素联系起来。例如，`NN`指名词，`VB`指动词。

然后我们讨论了 Word2Vec 和 Doc2Vec 神经网络。这些被用来分类文本，既有标签，也有与其他单词的相似性。我们演示了如何使用 DL4J 资源来创建与标签相关联的文档的特征向量。

虽然这些关联的识别是有趣的，但是当从句子中提取关系时，执行更有用的分析。我们演示了如何使用 OpenNLP 找到关系。POS 与每个单词相关联，单词之间的关系使用一组标签和括号来显示。这种类型的分析可用于更复杂的分析，如语言翻译和语法检查。

最后，我们讨论并展示了情感分析的例子。这个过程包括根据文本的语气或上下文含义对文本进行分类。我们研究了将电影评论分为正面或负面的过程。

在这一章中，我们展示了文本分析和分类的各种技术。在下一章中，我们将研究为视频和音频分析设计的技术。**