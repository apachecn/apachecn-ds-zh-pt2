# 第二章。数据采集

使用格式不正确的代码或使用的变量名不能表达其预期目的的代码从来都不是什么有趣的事情。数据也是如此，只是坏数据会导致不准确的结果。因此，数据采集是数据分析中的一个重要步骤。数据可以从许多来源获得，但是在它有用之前必须进行检索和最终处理。它可以从各种来源获得。我们可以在众多的公共数据源中找到简单的文件，也可以在互联网上找到更复杂的形式。在这一章中，我们将演示如何从这些网站中获取数据，包括各种互联网网站和一些社交媒体网站。

我们可以通过下载特定的文件或者通过一个叫做**网络抓取**的过程从互联网上获取数据，这个过程包括提取网页的内容。我们还探索了一个被称为**网络爬行**的相关主题，它涉及到应用程序检查一个网站以确定它是否是感兴趣的，然后跟随嵌入的链接以识别其他潜在的相关页面。

我们也可以从社交媒体网站提取数据。如果我们知道如何访问，这些类型的网站通常拥有现成的数据宝库。在本章中，我们将演示如何从几个站点提取数据，包括:

*   推特
*   维基百科(一个基于 wiki 技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书
*   闪烁（光）
*   油管（国外视频网站）

从站点提取数据时，可能会遇到许多不同的数据格式。我们将研究三种基本类型:文本、音频和视频。然而，即使在文本、音频和视频数据中，也存在许多格式。单就音频数据来说，在[https://en . Wikipedia . org/wiki/Comparison _ of _ audio _ coding _ formats](https://en.wikipedia.org/wiki/Comparison_of_audio_coding_formats)就比较了 45 种音频编码格式。对于文本数据，在 http://fileinfo.com/filetypes/text 的[列出了将近 300 种格式。在这一章中，我们将着重于如何下载和提取这些类型的文本作为纯文本，以供最终处理。](http://fileinfo.com/filetypes/text)

我们将简要分析不同的数据格式，然后分析可能的数据源。我们需要这些知识来演示如何使用不同的数据采集技术获取数据。

# 了解数据科学应用中使用的数据格式

当我们讨论数据格式时，我们指的是内容格式，而不是底层的文件格式，后者对大多数开发人员来说可能是不可见的。由于可用的格式数量巨大，我们无法检查所有可用的格式。相反，我们将处理几种更常见的格式，提供足够的例子来解决最常见的数据检索需求。具体来说，我们将演示如何检索以下列格式存储的数据:

*   超文本标记语言
*   便携文档格式
*   CSV/TSV
*   电子表格
*   数据库
*   JSON
*   可扩展标记语言

其中一些格式在其他地方得到了很好的支持和记录。例如，XML 已经使用了很多年，并且有几种成熟的技术可以在 Java 中访问 XML 数据。对于这些类型的数据，我们将概述可用的主要技术，并展示一些示例来说明它们是如何工作的。这将为那些不熟悉该技术的读者提供一些对其本质的洞察。

最常见的数据格式是二进制文件。例如，Word、Excel 和 PDF 文档都是以二进制存储的。这些需要特殊的软件来从中提取信息。文本数据也很常见。

## CSV 数据概述

**逗号分隔值** ( **CSV** )文件，包含以行列格式组织的表格数据。以明文形式存储的数据按行存储，也称为**记录**。每条记录都包含用逗号分隔的字段。这些文件也与其他分隔文件密切相关，最显著的是**制表符分隔值** ( **TSV** )文件。以下是一个简单 CSV 文件的一部分，这些数字并不代表任何特定类型的数据:

```
JURISDICTION NAME,COUNT PARTICIPANTS,COUNT FEMALE,PERCENT FEMALE 
10001,44,22,0.5 
10002,35,19,0.54 
10003,1,1,1 

```

请注意，第一行包含描述后续记录的标题数据。每个值由逗号分隔，并对应于相同位置的标题。在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中，我们将更深入地讨论 CSV 文件，并检查对不同类型分隔符的支持。

## 电子表格概述

电子表格是表格数据的一种形式，其中信息存储在行和列中，很像二维数组。它们通常包含数字和文本信息，并使用公式来总结和分析其内容。大多数人都熟悉 Excel 电子表格，但它们也是其他产品套件的一部分，如 OpenOffice。

电子表格是一种重要的数据源，因为在过去的几十年中，它们被用于在许多行业和应用中存储信息。它们的表格性质使它们易于处理和分析。了解如何从这个无处不在的数据源中提取数据非常重要，这样我们就可以利用存储在数据源中的大量信息。

对于我们的一些示例，我们将使用一个简单的 Excel 电子表格，它由一系列包含 ID 的行以及最小值、最大值和平均值组成。这些数字并不代表任何特定类型的数据。电子表格如下所示:

| **ID** | **最小值** | **最大值** | **平均值** |
| `12345` | `45` | `89` | `65.55` |
| `23456` | `78` | `96` | `86.75` |
| `34567` | `56` | `89` | `67.44` |
| `45678` | `86` | `99` | `95.67` |

在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中，我们将学习如何从电子表格中提取数据。

## 数据库概述

数据可以在**数据库管理系统** ( **DBMS** )中找到，这些系统和电子表格一样，无处不在。Java 为访问和处理 DBMS 中的数据提供了丰富的选项。本节的目的是提供使用 Java 访问数据库的基本介绍。

我们将演示使用 JDBC 连接到数据库、存储信息和检索信息的本质。对于这个例子，我们使用 MySQL 数据库管理系统。但是，它也适用于其他数据库管理系统，只需更改数据库驱动程序。我们在 **MySQL 工作台**中使用以下命令创建了一个名为`example`的数据库和一个名为`URLTABLE`的表。还有其他工具可以达到同样的效果:

```
CREATE TABLE IF NOT EXISTS `URLTABLE` ( 
  `RecordID` INT(11) NOT NULL AUTO_INCREMENT, 
  `URL` text NOT NULL, 
  PRIMARY KEY (`RecordID`) 
); 

```

我们从处理异常的`try`块开始。连接到 DBMS 需要一个驱动程序。在这个例子中，我们使用了`com.mysql.jdbc.Driver`。为了连接到数据库，使用了`getConnection`方法，传递数据库服务器位置、用户 ID 和密码。这些值取决于所使用的 DBMS:

```
    try { 
        Class.forName("com.mysql.jdbc.Driver"); 
        Stri­ng url = "jdbc:mysql://localhost:3306/example"; 
        connection = DriverManager.getConnection(url, "user ID", 
            "password"); 
            ... 
    } catch (SQLException | ClassNotFoundException ex) { 
        // Handle exceptions 
    } 

```

接下来，我们将说明如何向数据库添加信息，然后如何读取它。SQL `INSERT`命令是在字符串中构造的。MySQL 数据库的名字是`example`。该命令将在数据库的`URLTABLE`表中插入值，其中问号是要插入值的占位符:

```
    String insertSQL = "INSERT INTO  `example`.`URLTABLE` " 
        + "(`url`) VALUES " + "(?);"; 

```

`PreparedStatement`类表示要执行的 SQL 语句。`prepareStatement`方法使用`INSERT` SQL 语句创建该类的一个实例:

```
    PreparedStatement stmt = connection.prepareStatement(insertSQL); 

```

然后，我们使用`setString`方法和`execute`方法向表中添加 URL。`setString`方法有两个论点。第一个指定要插入数据的列索引，第二个是要插入的值。`execute`方法执行实际的插入。我们在下一个序列中添加两个 URL:

```
    stmt.setString(1, "https://en.wikipedia.org/wiki/Data_science"); 
    stmt.execute(); 
    stmt.setString(1,  
      "https://en.wikipedia.org/wiki/Bishop_Rock,_Isles_of_Scilly"); 
    stmt.execute(); 

```

为了读取数据，我们使用在`selectSQL`字符串中声明的 SQL `SELECT`语句。这将从`URLTABLE`表中返回所有的行和列。`createStatement`方法创建了一个`Statement`类的实例，用于`INSERT`类型的语句。`executeQuery`方法执行查询并返回保存表内容的`ResultSet`实例:

```
    String selectSQL = "select * from URLTABLE"; 
    Statement statement = connection.createStatement(); 
    ResultSet resultSet = statement.executeQuery(selectSQL); 

```

下面的序列遍历表，一次显示一行。`getString`方法的参数指定我们想要使用结果集的第二列，它对应于 URL 字段:

```
    out.println("List of URLs"); 
    while (resultSet.next()) { 
        out.println(resultSet.getString(2)); 
    }  

```

该示例在执行时的输出如下:

```
List of URLs
https://en.wikipedia.org/wiki/Data_science
https://en.wikipedia.org/wiki/Bishop_Rock,_Isles_of_Scilly

```

如果需要清空表中的内容，请使用以下顺序:

```
    Statement statement = connection.createStatement(); 
    statement.execute("TRUNCATE URLTABLE;"); 

```

这是对使用 Java 访问数据库的简单介绍。有许多资源可以提供关于这个主题的更深入的报道。例如，甲骨文在[https://docs.oracle.com/javase/tutorial/jdbc/](https://docs.oracle.com/javase/tutorial/jdbc/)提供了关于这个主题的更深入的介绍。

## PDF 文件概述

**可移植文档格式** ( **PDF** )是一种不依赖于特定平台或软件应用的格式。PDF 文档可以保存格式化的文本和图像。PDF 是一种开放标准，使它在许多地方都很有用。

有大量的文档存储为 PDF 格式，这使得它成为一个有价值的数据来源。有几个 Java APIs 允许访问 PDF 文档，包括 **Apache POI** 和 **PDFBox** 。从 PDF 文档中提取信息的技术在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中有说明。

## JSON 概述

**JavaScript 对象符号**(**JSON**)**([http://www.JSON.org/](http://www.JSON.org/))是一种用于交换数据的数据格式。对于人类或机器来说，读和写很容易。JSON 受到许多语言的支持，包括 Java，Java 有几个 JSON 库列在[http://www.JSON.org/](http://www.JSON.org/)上。**

**JSON 实体由一组用花括号括起来的名称-值对组成。我们将在几个例子中使用这种格式。在处理 YouTube 时，我们将使用一个 JSON 对象，下面显示了其中的一部分，它代表了一个 YouTube 视频请求的结果:**

```
{ 
  "kind": "youtube#searchResult", 
  "etag": "etag", 
  "id": { 
    "kind": string, 
    "videoId": string, 
    "channelId": string, 
    "playlistId": string 
  }, 
  ... 
} 
```

**访问这样一个文档的字段和值并不难，在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中有说明。**

## **XML 概述**

****可扩展标记语言** ( **XML** )是一种指定标准文档格式的标记语言。XML 广泛用于应用程序之间和互联网上的通信，由于它相对简单和灵活，所以很受欢迎。用 XML 编码的文档是基于字符的，很容易被机器和人阅读。**

**XML 文档包含标记和内容字符。这些字符允许解析器对文档中包含的信息进行分类。文档由标签组成，元素存储在标签中。元素还可以包含其他标记标签并形成子元素。此外，元素可能包含存储为名称-值对的属性或特定特征。**

**XML 文档必须是格式良好的。这意味着它必须遵循一定的规则，比如总是使用结束标签并且只使用一个根标签。其他规则在[https://en . Wikipedia . org/wiki/XML # Well-formed ness _ and _ error-handling](https://en.wikipedia.org/wiki/XML#Well-formedness_and_error-handling)讨论。**

**用于 XML 处理的 Java API 由三个用于解析 XML 数据的接口组成。**文档对象模型** ( **DOM** )接口解析 XML 文档并返回描述文档结构的树形结构。DOM 接口将整个文档作为一个整体来解析。或者，**XML 的简单 API**(**SAX**)一次解析一个文档元素。当内存使用是个问题时，SAX 是更好的选择，因为 DOM 需要更多的资源来构建树。然而，DOM 比 SAX 更灵活，因为任何元素都可以在任何时间以任何顺序访问。**

**第三个 Java API 被称为 XML 的**流 API**(**StAX**)。这种流模型旨在通过在不牺牲资源的情况下提供灵活性来容纳 DOM 和 SAX 模型的最佳部分。StAX 表现出更高的性能，代价是一次只能访问文档中的一个位置。如果您已经知道如何处理文档，StAX 是首选技术，但是对于可用内存有限的应用程序，它也很受欢迎。**

**下面是一个简单的 XML 文件。每个`<text>`代表一个标签，标记标签中包含的元素。在这种情况下，我们文件中最大的节点是`<music>`,其中包含歌曲数据集。一个`<song>`标签中的每个标签描述了另一个对应于那首歌的元素。每个标签最终都会有一个结束标签，比如`</song>`。请注意，第一个标记包含应该使用哪个 XML 版本来解析文件的信息:**

```
<?xml version="1.0"?> 
<music> 
   <song id="1234"> 
      <artist>Patton, Courtney</artist> 
      <name>So This Is Life</name> 
      <genre>Country</genre> 
      <price>2.99</price> 
   </song> 
   <song id="5678"> 
      <artist>Eady, Jason</artist> 
      <name>AM Country Heaven</name> 
      <genre>Country</genre> 
      <price>2.99</price> 
   </song> 
</music> 
```

**还有许多其他与 XML 相关的技术。例如，我们可以使用 DTD 文档或专门为 XML 文档编写的 XML 模式来验证特定的 XML 文档。使用 XLST 可以将 XML 文档转换成不同的格式。**

## **流数据概述**

**流数据指的是以连续流的形式生成并以顺序、逐段的方式访问的数据。普通互联网用户访问的大部分数据都是流媒体，包括视频和音频频道，或者社交媒体网站上的文本和图像数据。当数据是新的且变化很快时，或者当需要收集大量数据时，流式数据是首选方法。**

**流式数据通常是数据科学研究的理想选择，因为它通常以大量的原始格式存在。许多公共流数据都是免费的，并得到 Java APIs 的支持。在这一章中，我们将研究如何从流媒体来源获取数据，包括 Twitter、Flickr 和 YouTube。尽管使用了不同的技术和 API，但是您会注意到从这些站点获取数据的技术之间的相似之处。**

## **Java 中的音频/视频/图像概述**

**有大量的格式用于表示图像、视频和音频。这种类型的数据通常以二进制格式存储。模拟音频流被采样和数字化。图像通常只是代表像素颜色的位的集合。以下链接提供了对其中一些格式的更深入的讨论:**

*   **音频:[https://en.wikipedia.org/wiki/Audio_file_format](https://en.wikipedia.org/wiki/Audio_file_format)**
*   **图片:[https://en.wikipedia.org/wiki/Image_file_formats](https://en.wikipedia.org/wiki/Image_file_formats)**
*   **视频:[https://en.wikipedia.org/wiki/Video_file_format](https://en.wikipedia.org/wiki/Video_file_format)**

**通常，这种类型的数据可能非常大，必须进行压缩。当数据被压缩时，使用两种方法。第一种是无损压缩，使用更少的空间并且没有信息损失。第二种是有损，即信息丢失。丢失信息并不总是一件坏事，因为有时这种丢失对人类来说并不明显。**

**正如我们将在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中演示的那样，这种类型的数据通常会以一种不方便的方式遭到破坏，可能需要清理。例如，录音中可能有背景噪声，或者图像可能需要在处理之前进行平滑处理。图像平滑在[第 3 章](part0032.xhtml#aid-UGI02 "Chapter 3. Data Cleaning")、*数据清理*中演示，使用 OpenCV 库。**

 **# 数据采集技术

在这一节中，我们将说明如何从网页中获取数据。网页包含大量潜在的有用信息。我们将演示如何使用几种技术访问 web 页面，从由`HttpUrlConnection`类支持的低级方法开始。为了找到页面，经常使用网络爬虫应用程序。一旦确定了有用的页面，就需要从页面中提取信息。这通常使用 HTML 解析器来执行。提取这些信息非常重要，因为它们通常隐藏在杂乱的 HTML 标记和 JavaScript 代码中。

## 使用 HttpUrlConnection 类

可以使用`HttpUrlConnection`类访问网页的内容。这是一种低级的方法，需要开发人员做大量的工作来提取相关的内容。然而，他或她能够对如何处理内容进行更大的控制。在某些情况下，这种方法可能比使用其他 API 库更好。

我们将演示如何使用这个类下载维基百科数据科学页面的内容。我们从一个`try` / `catch`块开始处理异常。URL 对象是使用数据科学 URL 字符串创建的。`openConnection`方法将创建一个到维基百科服务器的连接，如下所示:

```
    try { 
        URL url = new URL( 
            "https://en.wikipedia.org/wiki/Data_science"); 
        HttpURLConnection connection = (HttpURLConnection)  
            url.openConnection(); 
       ... 
    } catch (MalformedURLException ex) { 
        // Handle exceptions 
    } catch (IOException ex) { 
        // Handle exceptions 
    } 

```

用 HTTP `GET`命令初始化`connection`对象。然后执行`connect`方法来连接服务器:

```
    connection.setRequestMethod("GET"); 
    connection.connect(); 

```

假设没有遇到错误，我们可以使用`getResponseCode`方法确定响应是否成功。一个正常返回值是`200`。网页的内容可能会有所不同。例如，`getContentType`方法返回一个描述页面内容的字符串。`getContentLength`方法返回它的长度:

```
    out.println("Response Code: " + connection.getResponseCode()); 
    out.println("Content Type: " + connection.getContentType()); 
    out.println("Content Length: " + connection.getContentLength()); 

```

假设我们得到了一个 HTML 格式的页面，下一个序列说明了如何得到这个内容。创建一个`BufferedReader`实例，从 web 站点一次读入一行并附加到一个`BufferedReader`实例。然后显示缓冲区:

```
    InputStreamReader isr = new InputStreamReader((InputStream)  
        connection.getContent()); 
    BufferedReader br = new BufferedReader(isr); 
    StringBuilder buffer = new StringBuilder(); 
    String line; 
    do { 
        line = br.readLine(); 
        buffer.append(line + "\n"); 
    } while (line != null); 
    out.println(buffer.toString()); 

```

这里显示了简短的输出:

```
Response Code: 200 
Content Type: text/html; charset=UTF-8 
Content Length: -1
<!DOCTYPE html> 
<html lang="en" dir="ltr" class="client-nojs"> 
<head> 
<meta charset="UTF-8"/>

<script>document.documentElement.className =
...
"wgHostname":"mw1251"});});</script> 
</body>
</html>

```

虽然这是可行的，但是有更简单的方法来获取网页的内容。下一节将讨论其中一种技术。

## Java 中的网络爬虫

Web 爬行是遍历一系列相互连接的网页并从这些网页中提取相关信息的过程。它通过隔离然后跟随页面上的链接来做到这一点。虽然有许多现成的预编译数据集，但仍有必要直接从互联网上收集数据。一些来源，如新闻网站，不断更新，需要不时地重新访问。

网络爬虫是访问各种站点并收集信息的应用程序。web 爬行过程由一系列步骤组成:

1.  选择要访问的 URL
2.  获取页面
3.  解析页面
4.  提取相关内容
5.  提取相关网址进行访问

对每个访问的 URL 重复这个过程。

在获取和解析页面时，需要考虑几个问题，例如:

*   页面重要性:我们不想处理不相关的页面。
*   例如，我们通常不会关注图片的链接。
*   蜘蛛陷阱(Spider traps):我们希望绕过那些可能导致无限请求的网站。在动态生成的页面中，一个请求导致另一个请求，就会发生这种情况。
*   **重复**:避免多次抓取同一页面很重要。
*   **礼貌**:不要向网站发出过多的请求。观察`robot.txt`文件；它们指定网站的哪些部分不应该被爬网。

创建一个网络爬虫的过程可能是令人生畏的。对于除了最简单的需求之外的所有需求，建议使用几种开源网络爬虫中的一种。部分列表如下:

*   **纳特**:【http://nutch.apache.org】T2
*   **爬虫军**:【https://github.com/yasserg/crawler4j】T2
*   **JSpider**:【http://j-spider.sourceforge.net/】T2
*   **WebSPHINX**:【http://www.cs.cmu.edu/~rcm/websphinx/ 
*   **赫莉特里克斯**:【https://webarchive.jira.com/wiki/display/Heritrix】T2

我们既可以创建自己的网络爬虫，也可以使用现有的爬虫。在本章中，我们将研究这两种方法。对于专门的处理，最好使用定制的爬行器。我们将演示如何用 Java 创建一个简单的网络爬虫，以便更深入地了解网络爬虫是如何工作的。接下来是对其他网络爬虫的简单讨论。

### 创建自己的网络爬虫

现在我们对网络爬虫有了基本的了解，我们准备创建自己的爬虫。在这个简单的网络爬虫中，我们将使用`ArrayList`实例跟踪被访问的页面。此外，jsoup 将用于解析网页，我们将限制我们访问的页面数量。jsoup([https://jsoup.org/](https://jsoup.org/))是一个开源的 HTML 解析器。这个例子演示了一个网络爬虫的基本结构，也强调了创建一个网络爬虫所涉及的一些问题。

我们将使用`SimpleWebCrawler`类，如下所示:

```
public class SimpleWebCrawler { 

    private String topic; 
    private String startingURL; 
    private String urlLimiter; 
    private final int pageLimit = 20; 
    private ArrayList<String> visitedList = new ArrayList<>(); 
    private ArrayList<String> pageList = new ArrayList<>(); 
    ... 
    public static void main(String[] args) { 
        new SimpleWebCrawler(); 
    } 

} 

```

实例变量的详细信息如下:

| **变量** | **使用** |
| `topic` | 要使页面被接受，页面中需要包含的关键字 |
| `startingURL` | 第一页的 URL |
| `urlLimiter` | 必须包含在链接中才能被跟踪的字符串 |
| `pageLimit` | 要检索的最大页数 |
| `visitedList` | 包含已经访问过的页面的`ArrayList` |
| `pageList` | 一个包含感兴趣页面的 URL 的`ArrayList` |

在`SimpleWebCrawler`构造函数中，我们初始化实例变量，从维基百科页面开始搜索意大利海岸附近的 Bishop Rock 岛。这是为了最大限度地减少可能检索到的页面数量。正如我们将会看到的，维基百科中关于主教洛克的页面比我们想象的要多得多。

`urlLimiter`变量被设置为`Bishop_Rock`，这将限制嵌入的链接只跟随那些包含该字符串的链接。每个感兴趣的页面必须包含存储在`topic`变量中的值。`visitPage`方法执行实际的爬行:

```
    public SimpleWebCrawler() { 
        startingURL = https://en.wikipedia.org/wiki/Bishop_Rock, " 
            + "Isles_of_Scilly"; 
        urlLimiter = "Bishop_Rock"; 
        topic = "shipping route"; 
        visitPage(startingURL); 
    } 

```

在`visitPage`方法中，检查`pageList` ArrayList 以查看是否超过了接受页面的最大数量。如果超出了限制，则搜索终止:

```
    public void visitPage(String url) { 
        if (pageList.size() >= pageLimit) { 
            return; 
        } 
       ... 
    } 

```

如果页面已经被访问过，那么我们忽略它。否则，它将被添加到已访问列表中:

```
    if (visitedList.contains(url)) { 
        // URL already visited 
    } else { 
        visitedList.add(url); 
            ... 
    } 

```

`Jsoup`用于解析页面并返回一个`Document`对象。可能会出现许多不同的异常和问题，例如格式错误的 URL、检索超时或简单的坏链接。`catch`区块需要处理这些类型的问题。我们将在 Java 的 web 抓取中对 jsoup 进行更深入的解释:

```
    try { 
        Document doc = Jsoup.connect(url).get(); 
            ... 
        } 
    } catch (Exception ex) { 
        // Handle exceptions 
    } 

```

如果文档包含主题文本，则显示链接并添加到`pageList`数组列表中。获取每个嵌入的链接，如果链接包含限制文本，那么递归调用`visitPage`方法:

```
    if (doc.text().contains(topic)) { 
        out.println((pageList.size() + 1) + ": [" + url + "]"); 
        pageList.add(url); 

        // Process page links 
        Elements questions = doc.select("a[href]"); 
        for (Element link : questions) { 
            if (link.attr("href").contains(urlLimiter)) { 
                visitPage(link.attr("abs:href")); 
            } 
        } 
    } 

```

这种方法只检查那些包含主题文本的页面中的链接。将`for`循环移出 if 语句将测试所有页面的链接。

输出如下:

```
1: [https://en.wikipedia.org/wiki/Bishop_Rock,_Isles_of_Scilly]
2: [https://en.wikipedia.org/wiki/Bishop_Rock_Lighthouse]
3: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&oldid=717634231#Lighthouse]
4: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&diff=prev&oldid=717634231]
5: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&oldid=716622943]
6: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&diff=prev&oldid=716622943]
7: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&oldid=716608512]
8: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&diff=prev&oldid=716608512]
...
20: [https://en.wikipedia.org/w/index.php?title=Bishop_Rock,_Isles_of_Scilly&diff=prev&oldid=716603919]

```

在本例中，我们没有将爬网结果保存在外部源中。通常这是必要的，可以存储在文件或数据库中。

### 使用 crawler4j 网络爬虫

这里我们将举例说明爬虫 4j(【https://github.com/yasserg/crawler4j】)网络爬虫的使用。我们将使用基本爬虫的改编版本，该版本位于[https://github . com/yasserg/crawler 4j/tree/master/src/test/Java/edu/UCI/ics/crawler 4j/examples/basi](https://github.com/yasserg/crawler4j/tree/master/src/test/java/edu/uci/ics/crawler4j/examples/basic)[c](https://github.com/yasserg/crawler4j/tree/master/src/test/java/edu/uci/ics/crawler4j/examples/basic)。我们将创建两个类:`CrawlerController`和`SampleCrawler`。前一个类设置爬虫，而后一个类包含控制将处理哪些页面的逻辑。

和我们之前的爬虫一样，我们将抓取维基百科中关于主教岩的文章。使用这个爬虫的结果会更小，因为许多无关的页面会被忽略。

我们先来看一下`CrawlerController`类。crawler 使用了几个参数，详情如下:

*   **抓取存储文件夹**:抓取数据存储的位置
*   **爬虫数量**:控制用于爬行的线程数量
*   **礼貌延迟**:请求之间暂停多少秒
*   **爬行深度**:爬行的深度
*   **要读取的最大页数**:要读取多少页
*   **二进制数据**:是否抓取 PDF 文件等二进制数据

这里显示了基本类:

```
public class CrawlerController { 

  public static void main(String[] args) throws Exception { 
    int numberOfCrawlers = 2; 
    CrawlConfig config = new CrawlConfig(); 
    String crawlStorageFolder = "data"; 

    config.setCrawlStorageFolder(crawlStorageFolder); 
    config.setPolitenessDelay(500); 
    config.setMaxDepthOfCrawling(2); 
    config.setMaxPagesToFetch(20); 
    config.setIncludeBinaryContentInCrawling(false); 
    ... 
  } 
}
```

接下来，创建并配置`CrawlController`类。注意用于处理`robot.txt`文件的`RobotstxtConfig`和`RobotstxtServer`类。这些文件包含供网络爬虫阅读的指令。它们提供了帮助爬虫做得更好的方向，例如指定站点的哪些部分不应该被爬行。这对自动生成的页面很有用:

```
    PageFetcher pageFetcher = new PageFetcher(config); 
    RobotstxtConfig robotstxtConfig = new RobotstxtConfig(); 
    RobotstxtServer robotstxtServer =  
        new RobotstxtServer(robotstxtConfig, pageFetcher); 
    CrawlController controller =  
        new CrawlController(config, pageFetcher, robotstxtServer); 

```

爬行器需要从一个或多个页面开始。`addSeed`方法添加开始页面。虽然我们在这里只使用了一次该方法，但它可以根据需要多次使用:

```
    controller.addSeed( 
      "https://en.wikipedia.org/wiki/Bishop_Rock,_Isles_of_Scilly"); 

```

`start`方法将开始爬行过程:

```
    controller.start(SampleCrawler.class, numberOfCrawlers); 

```

`SampleCrawler`类包含两个有趣的方法。第一个是确定页面是否被访问的`shouldVisit`方法和实际处理页面的`visit`方法。我们从类声明和 Java 正则表达式类`Pattern`对象的声明开始。这将是确定一个页面是否会被访问的一种方式。在此声明中，指定了标准图像，并将忽略这些图像:

```
    public class SampleCrawler extends WebCrawler { 
        private static final Pattern IMAGE_EXTENSIONS =  
            Pattern.compile(".*\\.(bmp|gif|jpg|png)$"); 

        ... 
    } 

```

向`shouldVisit`方法传递了一个对找到该 URL 的页面的引用。如果任何图像匹配，该方法返回`false`并且页面被忽略。另外，网址必须以 https://en.wikipedia.org/wiki/的[开头。我们添加此内容是为了将我们的搜索限制在维基百科网站:](https://en.wikipedia.org/wiki/)

```
    public boolean shouldVisit(Page referringPage, WebURL url) { 
        String href = url.getURL().toLowerCase(); 
        if (IMAGE_EXTENSIONS.matcher(href).matches()) { 
            return false; 
        } 
        return href.startsWith("https://en.wikipedia.org/wiki/"); 
    }
```

向`visit`方法传递一个表示被访问页面的`Page`对象。在这个实现中，只有那些包含字符串`shipping route`的页面才会被处理。这进一步限制了访问的页面。当我们找到这样一个页面时，它的`URL`、`Text`和`Text length`会显示出来:

```
    public void visit(Page page) { 
        String url = page.getWebURL().getURL(); 

        if (page.getParseData() instanceof HtmlParseData) { 
            HtmlParseData htmlParseData =  
                (HtmlParseData) page.getParseData(); 
            String text = htmlParseData.getText(); 
            if (text.contains("shipping route")) { 
                out.println("\nURL: " + url); 
                out.println("Text: " + text); 
                out.println("Text length: " + text.length()); 
            } 
        } 
    } 

```

以下是该程序执行时的截断输出:

```
URL: https://en.wikipedia.org/wiki/Bishop_Rock,_Isles_of_Scilly
Text: Bishop Rock, Isles of Scilly...From Wikipedia, the free encyclopedia ... Jump to: ... navigation, search For the Bishop Rock in the Pacific Ocean, see Cortes Bank. Bishop Rock Bishop Rock Lighthouse (2005)
...
Text length: 14677

```

请注意，只返回了一页。该网络爬虫能够识别并忽略主网页的先前版本。

我们可以执行进一步的处理，但是这个例子提供了一些关于 API 如何工作的见解。访问一个页面可以获得大量的信息。在这个例子中，我们只使用了 URL 和文本的长度。以下是您可能有兴趣获取的其他数据的示例:

*   path
*   父 URL
*   锚
*   HTML 文本
*   传出链接
*   文档 ID

## Java 中的网页抓取

网页抓取是从网页中提取信息的过程。该页面通常使用一系列 HTML 标记进行格式化。HTML 解析器用于浏览一个页面或一系列页面，并访问页面的数据或元数据。

jsoup([https://jsoup.org/](https://jsoup.org/))是一个开源的 Java 库，它使用 HTML 解析器来帮助提取和操作 HTML 文档。它有许多用途，包括 web 抓取、从 HTML 页面中提取特定元素以及清理 HTML 文档。

有几种方法可以获得有用的 HTML 文档。HTML 文档可以从以下位置提取:

*   统一资源定位器
*   线
*   文件

第一种方法在下面举例说明，其中数据科学的维基百科页面被加载到一个`Document`对象中。这个`Jsoup`对象代表 HTML 文档。`connect`方法连接到站点，`get`方法检索`document`:

```
    try { 
        Document document = Jsoup.connect( 
            "https://en.wikipedia.org/wiki/Data_science").get(); 
        ... 
     } catch (IOException ex) { 
        // Handle exception 
    } 

```

从文件加载使用如下所示的`File`类。重载的`parse`方法使用文件来创建`document`对象:

```
    try { 
        File file = new File("Example.html"); 
        Document document = Jsoup.parse(file, "UTF-8", ""); 
        ... 
    } catch (IOException ex) { 
        // Handle exception 
    } 

```

`Example.html`文件如下:

```
<html> 
<head><title>Example Document</title></head> 
<body> 
<p>The body of the document</p> 
Interesting Links: 
<br> 
<a href="https://en.wikipedia.org/wiki/Data_science">Data Science</a> 
<br> 
<a href="https://en.wikipedia.org/wiki/Jsoup">Jsoup</a> 
<br> 
Images: 
<br> 
 <img src="eyechart.jpg" alt="Eye Chart">  
</body> 
</html> 

```

为了从一个字符串创建一个`Document`对象，我们将使用下面的序列，其中`parse`方法处理复制前面 HTML 文件的字符串:

```
    String html = "<html>\n" 
        + "<head><title>Example Document</title></head>\n" 
        + "<body>\n" 
        + "<p>The body of the document</p>\n" 
        + "Interesting Links:\n" 
        + "<br>\n" 
        + "<a href="https://en.wikipedia.org/wiki/Data_science">" + 
          "DataScience</a>\n"
        + "<br>\n" 
        + "<a href="https://en.wikipedia.org/wiki/Jsoup">" + 
          "Jsoup</a>\n"
        + "<br>\n" 
        + "Images:\n" 
        + "<br>\n" 
        + " <img src="eyechart.jpg" alt="Eye Chart"> \n"
        + "</body>\n" 
        + "</html>"; 
    Document document = Jsoup.parse(html);
```

`Document`类拥有许多有用的方法。`title`方法返回标题。为了获取文档的文本内容，使用了`select`方法。此方法使用指定要检索的文档元素的字符串:

```
    String title = document.title(); 
    out.println("Title: " + title); 
    Elements element = document.select("body"); 
    out.println("  Text: " + element.text()); 

```

这里显示了 Wikipedia 数据科学页面的输出。为了节省空间，它被缩短了:

```
Title: Data science - Wikipedia, the free encyclopedia
Text: Data science From Wikipedia, the free encyclopedia Jump to: navigation, search Not to be confused with information science. Part of a 
...
policy About Wikipedia Disclaimers Contact Wikipedia Developers Cookie statement Mobile view

```

`select`方法的参数类型是字符串。通过使用字符串，选择的信息类型很容易改变。在 https://jsoup.org/apidocs/[的`Selector`类的 jsoup Javadocs 中可以找到关于如何构造这个字符串的细节:](https://jsoup.org/apidocs/)

我们可以使用`select`方法来检索文档中的图像，如下所示:

```
    Elements images = document.select("img[src$=.png]"); 
    for (Element image : images) { 
        out.println("\nImage: " + image); 
    } 

```

这里显示了 Wikipedia 数据科学页面的输出。为了节省空间，它被缩短了:

```
Image: <img alt="Data Visualization" src="//upload.wikimedia.org/...>
Image: <img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/...>

```

如下所示，可以轻松检索链接:

```
    Elements links = document.select("a[href]"); 
    for (Element link : links) { 
        out.println("Link: " + link.attr("href") 
            + " Text: " + link.text()); 
    } 

```

这里显示了`Example.html`页面的输出:

```
Link: https://en.wikipedia.org/wiki/Data_science Text: Data Science
Link: https://en.wikipedia.org/wiki/Jsoup Text: Jsoup

```

jsoup 拥有许多附加功能。然而，这个例子演示了网页抓取过程。还有其他可用的 Java HTML 解析器。在[https://en.wikipedia.org/wiki/Comparison_of_HTML_parsers](https://en.wikipedia.org/wiki/Comparison_of_HTML_parsers)可以找到 Java HTML 解析器的比较。

## 使用 API 调用访问常见的社交媒体网站

社交媒体包含大量可以处理的信息，并被许多数据分析应用程序使用。在这一节中，我们将说明如何使用 Java APIs 访问其中的一些资源。它们中的大多数需要某种访问密钥，这通常很容易获得。我们从讨论`OAuth`类开始，它提供了一种认证访问数据源的方法。

当使用这种类型的数据源时，请记住数据并不总是公开的，这一点很重要。虽然数据可能是可访问的，但数据的所有者可能是不一定希望信息被共享的个人。大多数 API 都提供了一种方法来决定如何分配数据，这些请求应该被接受。当使用私人信息时，必须获得作者的许可。

此外，这些网站对可以发出的请求数量有限制。从一个站点提取数据时，请记住这一点。如果需要超出这些限制，那么大多数网站都提供了一种方法。

### 使用 OAuth 认证用户

OAuth 是一种开放标准，用于对许多不同网站的用户进行身份验证。资源所有者有效地委托对服务器资源的访问，而不必共享他们的凭证。它适用于 HTTPS。OAuth 2.0 继承了 OAuth，并且不向后兼容。它为客户端开发人员提供了一种简单的身份验证方式。一些公司使用 OAuth 2.0，包括 PayPal、Comcast 和暴雪娱乐。

OAuth 2.0 提供商列表可在 https://en.wikipedia.org/wiki/List_of_OAuth_providers[找到。我们将在讨论中使用其中的几个。](https://en.wikipedia.org/wiki/List_of_OAuth_providers)

### 递推特

庞大的数据量和该网站在名人和公众中的受欢迎程度，使 Twitter 成为挖掘社交媒体数据的宝贵资源。Twitter 是一个流行的社交媒体平台，允许用户阅读和发布名为 **tweets** 的短信。Twitter 为发布和拉取推文提供 API 支持，包括来自所有公共用户的流数据。虽然有一些服务可用于提取整个公共 tweet 数据集，但我们将研究其他选项，这些选项虽然限制了一次检索的数据量，但却是免费的。

我们将关注用于检索流数据的 Twitter API。从特定用户那里检索推文以及向特定账户发布数据还有其他选择，但我们不会在本章中讨论这些。默认访问级别的公共流 API 允许用户提取当前在 Twitter 上流动的公共 tweets 的样本。通过指定参数来跟踪关键字、特定用户和位置，可以细化数据。

对于这个例子，我们将使用 Java HTTP 客户端 HBC。你可以在 https://github.com/twitter/hbc 下载一个示例 HBC 应用程序。如果您喜欢使用不同的 HTTP 客户端，请确保它将返回增量响应数据。Apache HTTP 客户机是一种选择。在创建 HTTP 连接之前，您必须首先创建一个 Twitter 帐户和该帐户中的一个应用程序。若要开始使用该应用程序，请访问 apps.twitter.com。创建应用程序后，将为您分配一个消费者密钥、消费者密码、访问令牌和访问密码令牌。我们也将使用 OAuth，正如本章前面所讨论的。

首先，我们将编写一个方法来执行身份验证并从 Twitter 请求数据。我们方法的参数是创建应用程序时 Twitter 给我们的认证信息。我们将创建一个`BlockingQueue`对象来保存我们的流数据。在本例中，我们将默认容量设置为 10，000。我们还将指定端点并关闭停止警告:

```
    public static void streamTwitter( 
        String consumerKey, String consumerSecret,  
        String accessToken, String accessSecret)  
            throws InterruptedException { 

        BlockingQueue<String> statusQueue =  
            new LinkedBlockingQueue<String>(10000); 
        StatusesSampleEndpoint ending =  
            new StatusesSampleEndpoint(); 
        ending.stallWarnings(false); 
        ... 
    } 

```

接下来，我们使用`OAuth1`创建一个`Authentication`对象，它是`OAuth`类的变体。然后，我们可以构建我们的连接客户端并完成 HTTP 连接:

```
    Authentication twitterAuth = new OAuth1(consumerKey,  
        consumerSecret, accessToken, accessSecret); 
    BasicClient twitterClient = new ClientBuilder() 
            .name("Twitter client") 
            .hosts(Constants.STREAM_HOST) 
            .endpoint(ending) 
            .authentication(twitterAuth) 
            .processor(new StringDelimitedProcessor(statusQueue)) 
            .build(); 
    twitterClient.connect(); 

```

出于这个例子的目的，我们将简单地读取从流中接收到的消息，并将它们打印到屏幕上。消息以 JSON 格式返回，如何在实际应用程序中处理它们将取决于该应用程序的目的和限制:

```
    for (int msgRead = 0; msgRead < 1000; msgRead++) { 
      if (twitterClient.isDone()) { 
        out.println(twitterClient.getExitEvent().getMessage()); 
        break; 
      } 

      String msg = statusQueue.poll(10, TimeUnit.SECONDS); 
      if (msg == null) { 
        out.println("Waited 10 seconds - no message received"); 
      } else { 
        out.println(msg); 
      } 
    } 
    twitterClient.stop(); 

```

为了执行我们的方法，我们只需将我们的认证信息传递给`streamTwitter`方法。为了安全起见，我们在这里更换了我们的私人钥匙。应该始终保护身份验证信息:

```
    public static void main(String[] args) { 

      try { 
        SampleStreamExample.streamTwitter( 
            myKey, mySecret, myToken, myAccess);  
      } catch (InterruptedException e) { 
        out.println(e); 
      } 
    } 

```

下面是使用上面列出的方法检索的截断样本数据。您的数据会因 Twitter 的实时流而异，但应该类似于以下示例:

```
{"created_at":"Fri May 20 15:47:21 +0000 2016","id":733685552789098496,"id_str":"733685552789098496","text":"bwisit si em bahala sya","source":"\u003ca href="http:\/\/twitter.com" rel="nofollow"\u003eTwitter Web 
...
ntions":[],"symbols":[]},"favorited":false,"retweeted":false,"filter_level":"low","lang":"tl","timestamp_ms":"1463759241660"}

```

Twitter 还支持为一个特定的用户帐户提取所有数据，以及将数据直接发布到一个帐户。REST API 也是可用的，它通过 search API 为特定的查询提供支持。这些也使用 OAuth 标准，并在 JSON 文件中返回数据。

### 处理维基百科

维基百科([https://www.wikipedia.org/](https://www.wikipedia.org/))是文本和图像类型信息的有用来源。这是一个互联网百科全书，拥有超过 250 种语言的 3800 万篇文章(【https://en.wikipedia.org/wiki/Wikipedia】T2)。因此，了解如何以编程方式访问其内容是很有用的。

MediaWiki 是一个开源的 Wiki 应用程序，支持 wiki 类型的站点。它用于支持维基百科和许多其他网站。MediaWiki API([http://www.mediaWiki.org/wiki/API](http://www.mediawiki.org/wiki/API))通过 HTTP 提供对 wiki 数据和元数据的访问。使用该 API 的应用程序可以登录、读取数据以及向站点发布更改。

有几个 Java APIs 支持对维基站点的编程访问，如在[https://www.mediawiki.org/wiki/API:Client_code#Java](https://www.mediawiki.org/wiki/API:Client_code#Java)所列。为了演示 Java 对 wiki 的访问，我们将使用 https://bitbucket.org/axelclk/info.bliki.wiki/wiki/Home[的 Bliki。它提供了良好的访问，并且易于用于大多数基本操作。](https://bitbucket.org/axelclk/info.bliki.wiki/wiki/Home)

MediaWiki API 很复杂，有许多特性。本节的目的是说明使用这个 API 从维基百科文章中获取文本的基本过程。这里不可能完全涵盖 API。

我们将使用`info.bliki.api`和`info.bliki.wiki.model`包中的以下类:

*   `Page`:表示检索到的页面
*   `User`:代表一个用户
*   `WikiModel`:代表维基

Bliki 的 Javadocs 可以在 http://www.javadoc.io/doc/info.bliki.wiki/bliki-core/3.1.0 找到。

以下例子改编自[http://www . integrating stuff . com/2012/04/06/hook-into-Wikipedia-using-Java-and-the-mediawiki-API/](http://www.integratingstuff.com/2012/04/06/hook-into-wikipedia-using-java-and-the-mediawiki-api/)。这个例子将访问主题为数据科学的英文维基百科页面。我们首先创建一个`User`类的实例。三参数构造函数的前两个参数分别是`user ID`和`password`。在这种情况下，它们是空字符串。这种组合允许我们在不建立账户的情况下阅读页面。第三个参数是 MediaWiki API 页面的 URL:

```
    User user = new User("", "",  
        "http://en.wikipedia.org/w/api.php"); 
    user.login(); 

```

帐户将使我们能够修改文件。`queryContent`方法返回在字符串数组中找到的主题的`Page`对象列表。每个字符串应该是一个页面的标题。在本例中，我们访问一个页面:

```
    String[] titles = {"Data science"}; 
    List<Page> pageList = user.queryContent(titles); 

```

每个`Page`对象包含一个页面的内容。有几种方法可以返回页面的内容。对于每个页面，使用双参数构造函数创建一个`WikiModel`实例。第一个参数是图像基本 URL，第二个参数是链接基本 URL。这些 URL 使用名为`image`和`title`的维基变量，这些变量将在创建链接时被替换:

```
    for (Page page : pageList) { 
        WikiModel wikiModel = new WikiModel("${image}",  
            "${title}"); 
        ... 
    } 

```

`render`方法将获取 wiki 页面并将其呈现为 HTML。还有一种将页面呈现为 PDF 文档的方法:

```
    String htmlText = wikiModel.render(page.toString()); 

```

然后显示 HTML 文本:

```
    out.println(htmlText); 

```

输出的部分列表如下:

```
<p>PageID: 35458904; NS: 0; Title: Data science; 
Image url: 
Content:
{{distinguish}}
{{Use dmy dates}}
{{Data Visualization}}</p>
<p><b>Data science</b> is an interdisciplinary field about processes and systems to extract <a href="Knowledge" >knowledge</a> 
...

```

我们还可以使用如下所示的几种方法之一来获取文章的基本信息:

```
    out.println("Title: " + page.getTitle() + "\n" + 
        "Page ID: " + page.getPageid() + "\n" + 
        "Timestamp: " + page.getCurrentRevision().getTimestamp()); 

```

还可以获得文章中的参考文献列表和标题列表。这里显示了一个参考列表:

```
    List <Reference> referenceList = wikiModel.getReferences(); 
    out.println(referenceList.size()); 
    for(Reference reference : referenceList) { 
        out.println(reference.getRefString()); 
    } 

```

下面说明了获取节标题的过程:

```
    ITableOfContent toc = wikiModel.getTableOfContent(); 
    List<SectionHeader> sections = toc.getSectionHeaders(); 
    for(SectionHeader sh : sections) { 
        out.println(sh.getFirst()); 
    } 

```

维基百科的全部内容都可以下载。这个过程将在[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)进行讨论。

建立自己的维基百科服务器来处理你的请求可能是可取的。

### 处理 Flickr

Flickr([https://www.flickr.com/](https://www.flickr.com/))是一款在线照片管理和分享应用。它可能是图像和视频的来源。Flickr 开发者指南(【https://www.flickr.com/services/developer/】T2)是了解 Flickr API 的一个很好的起点。

使用 Flickr API 的第一步是请求一个 API 密钥。这个密钥用于签署您的 API 请求。获取密钥的过程从[https://www.flickr.com/services/apps/create/](https://www.flickr.com/services/apps/create/)开始。商业密钥和非商业密钥都可用。当你获得一个密钥时，你也将获得一个“秘密”这两者都是使用 API 所必需的。

我们将举例说明从 Flickr 定位和下载图像的过程。该流程包括:

*   创建 Flickr 类实例
*   指定查询的搜索参数
*   执行搜索
*   下载图像

在此过程中可能会抛出`FlickrException`或`IOException`。有几个 API 支持 Flickr 访问。我们将使用位于 https://github.com/callmeal/Flickr4Java 的 Flickr4Java。Flickr4Java Javadocs 可以在 http://flickrj.sourceforge.net/api/[找到。我们将从一个`try`块以及`apikey`和`secret`声明开始:](http://flickrj.sourceforge.net/api/)

```
    try { 
        String apikey = "Your API key"; 
        String secret = "Your secret"; 

    } catch (FlickrException | IOException ex) { 
        // Handle exceptions 
    } 

```

接下来创建`Flickr`实例，其中`apikey`和`secret`作为前两个参数被提供。最后一个参数指定了用于访问 Flickr 服务器的传输技术。目前，使用`REST`类支持 REST 传输:

```
    Flickr flickr = new Flickr(apikey, secret, new REST()); 

```

为了搜索图像，我们将使用`SearchParameters`类。这个类支持许多标准，这些标准可以缩小查询返回的图像数量，包括纬度、经度、媒体类型和用户 ID 等标准。在下面的序列中，`setBBox`方法指定了搜索的经度和纬度。这些参数是(按顺序):最小经度、最小纬度、最大经度和最大纬度。`setMedia`方法指定了媒体的类型。有三个可能的参数— `"all"`、`"photos"`和`"videos"`:

```
    SearchParameters searchParameters = new SearchParameters(); 
    searchParameters.setBBox("-180", "-90", "180", "90"); 
    searchParameters.setMedia("photos"); 

```

`PhotosInterface`类拥有一个`search`方法，该方法使用`SearchParameters`实例来检索照片列表。`getPhotosInterface`方法返回`PhotosInterface`类的一个实例，如下所示。`SearchParameters`实例是第一个参数。第二个参数决定每页检索多少张照片，第三个参数是偏移量。返回一个`PhotoList`类实例:

```
    PhotosInterface pi = new PhotosInterface(apikey, secret,  
        new REST()); 
    PhotoList<Photo> list = pi.search(searchParameters, 10, 0); 

```

下一个序列说明了几种方法的使用，以获取有关图像检索的信息。使用`get`方法访问每个`Photo`实例。将显示标题、图像格式、公共标志和照片 URL:

```
    out.println("Image List"); 
    for (int i = 0; i < list.size(); i++) { 
        Photo photo = list.get(i); 
        out.println("Image: " + i + 
            `"\nTitle: " + photo.getTitle() +  
            "\nMedia: " + photo.getOriginalFormat() + 
            "\nPublic: " + photo.isPublicFlag() + 
            "\nUrl: " + photo.getUrl() + 
            "\n"); 
    } 
    out.println(); 

```

此处显示了部分列表，其中许多特定值已被修改以保护原始数据:

```
Image List
Image: 0
Title: XYZ Image
Media: jpg
Public: true
Url: https://flickr.com/photos/7723...@N02/269...
Image: 1
Title: IMG_5555.jpg
Media: jpg
Public: true
Url: https://flickr.com/photos/2665...@N07/264...
Image: 2
Title: DSC05555
Media: jpg
Public: true
Url: https://flickr.com/photos/1179...@N04/264...

```

这个例子返回的图片列表会有所不同，因为我们使用了一个相当大的搜索范围，而且图片一直在增加。

有两种方法可以用来下载图像。第一个使用图像的 URL，第二个使用一个`Photo`对象。图像的 URL 可以从许多来源获得。我们在这个例子中使用了`Photo`类`getUrl`方法。

在下面的序列中，我们使用其构造函数获得了一个`PhotosInterface`的实例，以说明另一种方法:

```
    PhotosInterface pi = new PhotosInterface(apikey, secret,  
        new REST()); 

```

我们从前面的列表中获取第一个`Photo`实例，然后用它的`getUrl`获取图像的 URL。`PhotosInterface`类的`getImage`方法返回一个代表图像的`BufferedImage`对象，如下所示:

```
    Photo currentPhoto = list.get(0);  
    BufferedImage bufferedImage =  
        pi.getImage(currentPhoto.getUrl()); 

```

然后使用`ImageIO`类将图像保存到一个文件中:

```
    File outputfile = new File("image.jpg"); 
    ImageIO.write(bufferedImage, "jpg", outputfile); 

```

`getImage`方法被重载。这里，`Photo`实例和所需图像的大小被用作参数来获得`BufferedImage`实例:

```
    bufferedImage = pi.getImage(currentPhoto, Size.SMALL); 

```

可以使用前面的技术将图像保存到文件中。

Flickr4Java API 支持许多其他处理 Flickr 图像的技术。

### 处理 YouTube

YouTube 是一个受欢迎的视频网站，用户可以上传和分享视频([https://www.youtube.com/](https://www.youtube.com/))。它已经被用来分享幽默视频，提供如何做任何事情的指令，并在其观众之间分享信息。这是一个有用的信息来源，因为它捕捉了不同人群的思想和观点。这为分析和洞察人类行为提供了一个有趣的机会。

YouTube 可以作为视频和视频元数据的有用来源。Java API 可用于访问其内容([https://developers.google.com/youtube/v3/](https://developers.google.com/youtube/v3/))。API 的详细文档可以在 https://developers.google.com/youtube/v3/docs/的[找到。](https://developers.google.com/youtube/v3/docs/)

在本节中，我们将演示如何通过关键字搜索视频并检索感兴趣的信息。我们还将展示如何下载视频。要使用 YouTube API，你需要一个谷歌账户，可以在[https://www.google.com/accounts/NewAccount](https://www.google.com/accounts/NewAccount)获得。接下来，在谷歌开发者控制台中创建一个账户(【https://console.developers.google.com/】T2)。使用 API 密钥或 OAuth 2.0 凭证支持 API 访问。在[https://developers . Google . com/YouTube/registrating _ an _ application # create _ project](https://developers.google.com/youtube/registering_an_application#create_project)讨论项目创建过程和关键点。

#### 按关键字搜索

按关键字搜索视频的过程改编自[https://developers . Google . com/YouTube/v3/code _ samples/Java # search _ by _ keyword](https://developers.google.com/youtube/v3/code_samples/java#search_by_keyword)。其他可能有用的代码示例可以在[https://developers.google.com/youtube/v3/code_samples/java](https://developers.google.com/youtube/v3/code_samples/java)找到。该过程已经过简化，因此我们可以专注于搜索过程。我们从 try 块和一个`YouTube`实例的创建开始。这个类提供了对 API 的基本访问。这个 API 的 Javadocs 可以在[https://developers . Google . com/resources/API-libraries/documentation/YouTube/v3/Java/latest/](https://developers.google.com/resources/api-libraries/documentation/youtube/v3/java/latest/)找到。

`YouTube.Builder`类用于构造一个`YouTube`实例。它的构造函数有三个参数:

*   `Transport`:用于 HTTP 的对象
*   `JSONFactory`:用于处理 JSON 对象
*   这个例子不需要任何东西

许多 API 响应将以 JSON 对象的形式出现。YouTube 类的'`setApplicationName`方法给它一个名字，`build`方法创建一个新的 YouTube 实例:

```
    try { 
        YouTube youtube = new YouTube.Builder( 
            Auth.HTTP_TRANSPORT, 
            Auth.JSON_FACTORY, 
            new HttpRequestInitializer() { 
                public void initialize(HttpRequest request)  
                        throws IOException { 
                } 
            }) 
                .setApplicationName("application_name") 
        ... 
    } catch (GoogleJSONResponseException ex) { 
        // Handle exceptions 
    } catch (IOException ex) { 
        // Handle exceptions 
    } 

```

接下来，我们初始化一个字符串来保存感兴趣的搜索词。在这种情况下，我们将查找包含单词`cats`的视频:

```
    String queryTerm = "cats"; 

```

类`YouTube.Search.List`维护一个搜索结果的集合。`YouTube`类的`search`方法指定了要返回的资源类型。在这种情况下，字符串指定了要返回的搜索结果的`id`和`snippet`部分:

```
    YouTube.Search.List search = youtube 
        .search() 
        .list("id,snippet"); 

```

搜索结果是一个 JSON 对象，其结构如下。在[https://developers . Google . com/YouTube/v3/docs/playlist items # methods](https://developers.google.com/youtube/v3/docs/playlistItems#methods)中有更详细的描述。在前面的序列中，只返回搜索的`id`和`snippet`部分，从而提高了操作效率:

```
{ 
  "kind": "youtube#searchResult", 
  "etag": etag, 
  "id": { 
    "kind": string, 
    "videoId": string, 
    "channelId": string, 
    "playlistId": string 
  }, 
  "snippet": { 
    "publishedAt": datetime, 
    "channelId": string, 
    "title": string, 
    "description": string, 
    "thumbnails": { 
      (key): { 
        "url": string, 
        "width": unsigned integer, 
        "height": unsigned integer 
      } 
    }, 
    "channelTitle": string, 
    "liveBroadcastContent": string 
  } 
} 

```

接下来，我们需要指定 API 键和各种搜索参数。指定查询术语以及要返回的媒体类型。在这种情况下，只会返回视频。另外两个选项包括`channel`和`playlist`:

```
    String apiKey = "Your API key"; 
    search.setKey(apiKey); 
    search.setQ(queryTerm); 
    search.setType("video"); 

```

此外，我们进一步指定要返回的字段，如下所示。这些对应于 JSON 对象的字段:

```
    search.setFields("items(id/kind,id/videoId,snippet/title," +  
        "snippet/description,snippet/thumbnails/default/url)"); 

```

我们还指定了使用`setMaxResults`方法检索的结果的最大数量:

```
    search.setMaxResults(10L); 

```

`execute`方法将执行实际的查询，返回一个`SearchListResponse`对象。它的`getItems`方法返回一个`SearchResult`对象列表，每个对象对应一个检索到的视频:

```
    SearchListResponse searchResponse = search.execute(); 
    List<SearchResult> searchResultList =  
        searchResponse.getItems(); 

```

在这个例子中，我们没有遍历每个返回的视频。相反，我们检索第一个视频并显示关于该视频的信息。`SearchResult` video 变量允许我们访问 JSON 对象的不同部分，如下所示:

```
    SearchResult video = searchResultList.iterator().next(); 
    Thumbnail thumbnail = video 
        .getSnippet().getThumbnails().getDefault(); 

    out.println("Kind: " + video.getKind()); 
    out.println("Video Id: " + video.getId().getVideoId()); 
    out.println("Title: " + video.getSnippet().getTitle()); 
    out.println("Description: " +  
        video.getSnippet().getDescription()); 
    out.println("Thumbnail: " + thumbnail.getUrl()); 

```

一种可能的输出如下，其中部分输出已被修改:

```
Kind: null
Video Id: tntO...
Title: Funny Cats ...
Description: Check out the ...
Thumbnail: https://i.ytimg.com/vi/tntO.../default.jpg

```

为了简化示例，我们已经跳过了许多错误检查步骤，但是在业务应用程序中实现时，应该考虑这些步骤。

如果我们需要下载视频，最简单的方法之一就是使用 axet/wget 在[https://github.com/axet/wget](https://github.com/axet/wget)找到。它提供了一种使用视频 ID 下载视频的简单易用的技术。

在下面的示例中，使用视频 ID 创建了一个 URL。您需要提供视频 ID 才能正常工作。文件以视频标题作为文件名保存到当前目录:

```
    String url = "http://www.youtube.com/watch?v=videoID"; 
    String path = "."; 
    VGet vget = new VGet(new URL(url), new File(path)); 
    vget.download(); 

```

GitHub 网站上还有其他更复杂的下载技术。



# 总结

在这一章中，我们讨论了对数据科学有用的数据类型，这些数据在互联网上很容易获得。这一讨论包括关于最常见类型数据源的文件规范和格式的细节。

我们还研究了 Java APIs 和其他检索数据的技术，并用多个数据源说明了这个过程。我们特别关注基于文本的文档格式和多媒体文件的类型。我们使用网络爬虫来访问网站，然后执行网络抓取来从我们遇到的网站中检索数据。

最后，我们从社交媒体网站提取数据，并检查可用的 Java 支持。我们从 Twitter、Wikipedia、Flickr 和 YouTube 检索数据，并检查可用的 API 支持。**