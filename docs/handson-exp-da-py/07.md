# 相互关系

在本章中，我们将探讨不同因素之间的相关性，并估计这些不同因素的可靠程度。此外，我们将了解我们可以进行的不同类型的检查，以发现数据之间的关系，包括单变量分析、双变量分析和多变量分析。我们将使用泰坦尼克号数据集进行这些分析。我们还将介绍辛普森悖论。同样，我们将深入了解一个众所周知的事实:相关性并不意味着因果关系。

在本章中，我们将涵盖以下主题:

*   引入相关性
*   理解单变量分析
*   理解二元分析
*   理解多元分析
*   使用泰坦尼克号数据集讨论多元分析
*   概述辛普森悖论
*   相关性并不意味着因果关系

# 技术要求

本章的代码可以在`chapter 7`文件夹内的 GitHub 资源库([https://GitHub . com/PacktPublishing/用 python 动手探索数据分析](https://github.com/PacktPublishing/hands-on-exploratory-data-analysis-with-python))中找到:

*   **数据集 A** :本章使用的汽车数据集在`chapter 7/automobile.csv`文件夹里面。
*   **数据集 B** :本章使用的泰坦尼克号数据集有 Open ML 提供。你可以在[https://www.openml.org/d/40945](https://www.openml.org/d/40945)下载。数据集已在文件夹中为您下载。
*   **GitHub**:[https://GitHub . com/PacktPublishing/动手-探索-数据-分析-用 python/tree/master/Chapter % 207](https://github.com/PacktPublishing/hands-on-exploratory-data-analysis-with-python/tree/master/Chapter%207)。

# 引入相关性

我们要分析的任何数据集都将有代表不同事实的多个观察值(即变量)的不同字段(即列)。数据集的列很可能是相互关联的，因为它们是从同一事件中收集的。记录的一个字段可能会也可能不会影响另一个字段的值。为了检查这些列之间的关系类型并分析它们之间的因果关系，我们必须努力找出变量之间存在的依赖关系。数据集的两个字段之间的这种关系的强度称为**相关性**，由-1 到 1 之间的数值表示。

换句话说，检验关系并解释变量对是否相互关联以及关联程度的统计技术被称为相关性。相关性回答了诸如一个变量相对于另一个变量如何变化的问题。如果它确实改变了，那么改变到什么程度或强度？此外，如果这些变量之间的关系足够强，那么我们可以对未来的行为做出预测。

比如身高和体重都有关系；也就是说，个子高的人往往比个子矮的人更重。如果我们有一个比我们之前观察到的平均身高更高的新人，那么他们的体重更有可能超过我们观察到的平均体重。

相关性告诉我们变量是如何一起变化的，既有相同的方向，也有相反的方向，还有关系的大小(即强度)。为了找到相关性，我们计算皮尔逊相关系数，用ρ(希腊字母*ρ*)表示。这是通过将协方差除以变量标准差的乘积得到的:

![](assets/55897448-66a7-452e-ba1b-5b40528eee44.png)

就关系的强度而言，两个变量 *A* 和 *B* 之间的相关值在+1 和-1 之间变化。如果相关是+1，那么就说是完美的正/线性相关(即变量 A 与变量 B 成正比)，而-1 的相关是完美的负相关(即变量 A 与变量 B 成反比)。请注意，接近 0 的值根本不应该相关。如果相关系数的绝对值接近 1，那么这些变量就被称为强相关；相比之下，那些接近 0.5 的被称为弱相关。

让我们看一些使用散点图的例子。散点图显示一个变量受另一个变量影响的程度:

![](assets/82df74df-2369-4942-8a66-fcf5ed00725d.png)

如第一个和最后一个图表所示，当绘制成直线时，数据点之间的距离越近，相关变量之间的相关性越高。它们之间的相关性越高，变量之间的关系就越强。绘制时数据点越分散(因此没有模式)，两个变量之间的相关性越低。在这里，您应该注意以下四个要点:

*   当数据点图有一条直线穿过原点到达 *x* 和 *y* 值时，则称变量间有**正相关**。
*   当数据点绘制成从 *y* 轴上的高值到 *x* 轴上的高值的线时，变量被称为具有**负相关**。
*   完美相关性的值为 1。
*   完美负相关的值为-1。

高度正相关的值接近 1。高度负相关的值接近-1。在上图中，+0.8 表示高度正相关，-0.8 表示高度负相关。数字越接近 0(在图中，这是+0.3 和-0.3)，相关性越弱。

在分析数据集中的相关性之前，让我们了解各种类型的分析。

# 分析类型

在本节中，我们将探讨不同类型的分析。我们将从单变量分析开始，然后进行双变量分析，最后，我们将讨论多变量分析。

# 理解单变量分析

还记得我们在[第五章](05.html)、*描述性统计*中使用的描述性统计的测量变量吗？我们有一组从 2 到 12 的整数。我们计算了该集合的平均值、中值和模式，并分析了整数的分布模式。然后，我们计算了每种类型汽车数据集的高度列中可用值的平均值、模式、中值和标准偏差。对单一类型数据集的这种分析称为**单变量分析**。

单变量分析是分析数据的最简单形式。这意味着我们的数据只有一种类型的变量，我们对它进行分析。单变量分析的主要目的是获取数据，总结数据，并在值之间找到模式。它不涉及原因或价值观之间的关系。描述单变量数据中发现的模式的几种技术包括中心趋势(即平均值、模式和中值)和离差(即范围、方差、最大和最小四分位数(包括四分位数之间的范围)和标准偏差)。

你为什么不试着对同一组数据再做一次分析呢？这一次，记住这是单变量分析:

1.  首先导入所需的库并加载数据集:

```py
#import libraries
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
```

2.  现在，加载数据:

```py
# loading dataset as Pandas dataframe
df = pd.read_csv("data.csv")
df.head()
```

该代码的输出如下所示:

![](assets/f963e29e-d9d0-4b3a-8e95-6ed08b9ca94f.png)

3.  首先，检查每列的数据类型。现在，您必须熟悉以下内容:

```py
df.dtypes
```

输出如下:

```py
symboling int64
normalized-losses int64
make object
aspiration object
num-of-doors object
body-style object
drive-wheels object
engine-location object
wheel-base float64
length float64
width float64
height float64
curb-weight int64
engine-type object
num-of-cylinders object
engine-size int64
fuel-system object
bore float64
stroke float64
compression-ratio float64
horsepower float64
peak-rpm float64
city-mpg int64
highway-mpg int64
price float64
city-L/100km float64
horsepower-binned object
diesel int64
gas int64
dtype: object
```

4.  现在计算高度柱中心趋势的度量。回想一下，我们在[第 5 章](05.html)*描述性统计:*中讨论了几个描述性统计

```py
#calculate mean, median and mode of dat set height
mean = df["height"].mean()
median =df["height"].median()
mode = df["height"].mode()
print(mean , median, mode)
```

这些描述性函数的输出如下:

```py
53.766666666666715 54.1 0 50.8
dtype: float64
```

5.  现在，让我们在图表中可视化这个分析:

```py
#distribution plot
sns.FacetGrid(df,size=5).map(sns.distplot,"height").add_legend()
```

该代码将在高度列中生成值的分布图:

![](assets/0623f861-cddd-41db-a8ec-c9b361045f19.png)

从图表中，我们可以观察到最大汽车的最大高度在 53 到 57 之间。现在，让我们对价格列进行同样的操作:

```py
#distribution plot
sns.FacetGrid(df,size=5).map(sns.distplot,"price").add_legend()
```

该代码的输出如下所示:

![](assets/ded241b3-340a-46b8-8169-0d777880e302.png)

看这张图，可以说价格在 5000 到 45000 之间，但是最高车价在 5000 到 10000 之间。

箱线图也是直观表示单变量分析中的中位数和四分位数等统计指标的有效方法:

```py
#boxplot for price of cars
sns.boxplot(x="price",data=df)
plt.show()
```

根据前面的代码生成的方框图如下所示:

![](assets/4bf62578-7ecf-46c2-bf30-c62d8506d2fd.png)

方框的右边框是 Q3，也就是第三个四分位数，方框的左边框是 Q1，也就是第一个四分位数。线条从方框边界的两侧向最小值和最大值延伸。基于我们的绘图工具使用的惯例，虽然，他们可能只扩展到某个统计数据；超出这些统计数据的任何值都被标记为异常值(使用点)。

该分析针对的是仅包含单一类型变量的数据集。现在，让我们看一下下一种对具有两种类型变量的数据集的分析形式，称为二元分析。

# 理解二元分析

顾名思义，这是对一个以上(也就是正好两个)类型变量的分析。双变量分析用于找出两个不同变量之间是否存在关系。当我们在笛卡尔平面上绘制一个变量与另一个变量的散点图时(想想 *x* 和 *y* 轴)，它给了我们一幅数据试图告诉我们的画面。如果数据点似乎符合直线或曲线，则两个变量之间存在关系或相关性。一般来说，如果我们知道自变量的值，二元分析有助于我们预测一个变量(即因变量)的值。

这是一张图表，显示了一段时间内广告费用和销售费率的散点图:

![](assets/a2d74b41-e562-4586-90e6-7f003317b726.png)

这个图是二元分析的散点图，其中**销售额**和**广告费**是两个变量。绘制散点图时，我们可以看到销售价值取决于广告费用；也就是说，随着广告费的增加，销售价值也随之增加。对两个变量之间关系的这种理解将指导我们未来的预测:

1.  现在是时候对我们的汽车数据集进行二元分析了。让我们看看马力是否是汽车定价的一个依赖因素:

```py
# plot the relationship between “horsepower” and ”price”
plt.scatter(df["price"], df["horsepower"])
plt.title("Scatter Plot for horsepower vs price")
plt.xlabel("horsepower")
plt.ylabel("price")
```

该代码将生成散点图，其 *y* 轴上的`price`范围和 *x* 轴上的`horsepower`值如下:

![](assets/eeea6b73-78bf-4c0a-9832-5a54ad176a0a.png)

正如你在上图中看到的，汽车的马力是价格的一个依赖因素。随着汽车马力的增加，汽车的价格也随之增加。

方框图也是查看一些统计度量以及两个值之间关系的好方法。

2.  现在，让我们在汽车的发动机位置和价格之间画一个方框图:

```py
#boxplot
sns.boxplot(x="engine-location",y="price",data=df)
plt.show()
```

该代码将生成一个方框图，价格范围在 *y* 轴，发动机位置类型在 *x* 轴:

![](assets/90122062-7be5-4a8c-b906-671c8c98eb93.png)

该图显示，前置发动机位置为**的汽车价格比后置发动机位置为**的汽车价格低得多。此外，还有一些异常值具有前置引擎位置，但价格要高得多。****

3.  接下来，用价格范围和驱动轮类型绘制另一个方框图:

```py
#boxplot to visualize the distribution of "price" with types of "drive-wheels"
sns.boxplot(x="drive-wheels", y="price",data=df)
```

该代码的输出如下所示:

![](assets/70c54191-95cb-4cd5-9dc4-173e1dd1269c.png)

这张图表显示了不同车轮类型汽车的价格范围。这里，方框图显示了各个车轮类型的平均和中间价格以及一些异常值。

这是一个简短的介绍，以及一些二元分析的实践例子。现在，让我们了解一种更有效的数据分析实践，多元分析。

# 理解多元分析

多元分析是对三个或更多变量的分析。这使我们能够观察相关性(即一个变量相对于另一个变量如何变化)，并试图比二元分析更准确地预测未来行为。

最初，我们探索了单变量分析和双变量分析的可视化；同样，让我们把多元分析的概念形象化。

绘制多变量数据的一种常见方法是绘制矩阵散点图，称为对图。矩阵图或成对图显示了每对变量之间的相互关系。配对图让我们可以看到单个变量的分布和两个变量之间的关系:

1.  我们可以使用`pandas.tools.plotting`包中的`scatter_matrix()`功能或`seaborn`包中的`seaborn.pairplot()`功能来完成此操作:

```py
# pair plot with plot type regression
sns.pairplot(df,vars = ['normalized-losses', 'price','horsepower'], kind="reg")
plt.show()
```

该代码将绘制一个 3×3 矩阵，其中包含标准化损失、价格和马力列中的不同数据:

![](assets/b0393685-17bf-4c49-b49c-8dfe1a09b1e9.png)

如前图所示，对角线上的直方图允许我们说明单个变量的分布。上三角形和下三角形上的回归图展示了两个变量之间的关系。第一行中间的图为回归图；这表明正常损失和汽车价格之间没有相关性。相比之下，最下面一排中间的回归图说明了价格和马力之间有着巨大的相关性。

2.  同样，我们可以通过指定颜色、标签、绘图类型、对角线绘图类型和变量，使用成对绘图进行多元分析。那么，让我们制作另一个配对图:

```py
#pair plot (matrix scatterplot) of few columns 
sns.set(style="ticks", color_codes=True)
sns.pairplot(df,vars = ['symboling', 'normalized-losses','wheel-base'], hue="drive-wheels")
plt.show()
```

该代码的输出如下所示:

![](assets/e25e0f57-8431-42eb-9639-48b059f2029f.png)

这是一对符号化、标准化损失、轮基和驱动轮柱的记录图。

对角线上的密度图允许我们看到单个变量的分布，而上下三角形上的散点图显示了两个变量之间的关系(或相关性)。色调参数是用于数据点标签的列名；在此图中，驱动轮类型用颜色标注。第二行最左边的图显示了归一化损失与轴距的散点图。

如前所述，相关性分析是发现多元数据集中的任何变量是否相关的有效技术。要计算一对变量的线性(皮尔逊)相关系数，我们可以使用`pandas`包中的`dataframe.corr(method ='pearson')`函数和`scipy.stats`包中的`pearsonr()`函数:

3.  例如，要计算价格和马力的相关系数，请使用以下公式:

```py
from scipy import stats
corr = stats.pearsonr(df["price"], df["horsepower"])
print("p-value:\t", corr[1])
print("cor:\t\t", corr[0])
```

输出如下:

```py
p-value: 6.369057428260101e-48 
cor: 0.8095745670036559
```

这里这两个变量的相关性为`0.80957`，接近+1。因此，我们可以确保价格和马力高度正相关。

4.  使用 pandas `corr(`函数，整个数值记录之间的相关性可以计算如下:

```py
correlation = df.corr(method='pearson')
correlation
```

该代码的输出如下所示:

![](assets/d0400bbc-2a93-4106-a7b5-18d53679b160.png)

5.  现在，让我们使用热图来可视化这种相关性分析。热图是让它看起来更漂亮、更容易理解的最佳技术:

```py
sns.heatmap(correlation,xticklabels=correlation.columns,
            yticklabels=correlation.columns)
```

该代码的输出如下所示:

![](assets/07d9c1aa-d03f-4066-afde-7fdf94a4d8e0.png)

系数接近 1 意味着这两个变量之间有很强的正相关。对角线是变量与其自身的相关性，所以它们当然是 1。

这是一个简单的介绍，以及一些多元分析的实践例子。现在，让我们用流行的数据集泰坦尼克号来练习它们，泰坦尼克号在世界各地经常被用来练习数据分析和机器学习算法。数据来源在本章*技术要求*部分有提及。

# 使用泰坦尼克号数据集讨论多元分析

1912 年 4 月 15 日，当时制造的最大客轮在处女航时与冰山相撞。泰坦尼克号沉没时，2224 名乘客和船员中有 1502 人遇难。`titanic.csv`([https://web . Stanford . edu/class/archive/cs/cs109/cs109.1166/stuff/Titanic . CSV](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv))文件中包含了 887 名真实泰坦尼克号乘客的数据。每行代表一个人。这些列描述了船上人员的不同属性，其中`PassengerId`列是乘客的唯一 ID，`Survived`是幸存(1)或死亡(0)的数字，`Pclass`是乘客的级别(即第一、第二或第三)，`Name`是乘客的姓名，`Sex`是乘客的性别，`Age`是乘客的年龄，`Siblings/Spouses Aboard`是泰坦尼克号上的兄弟姐妹/配偶的数量，`Parents/Children Aboard`是泰坦尼克号上的父母/子女的数量，`Ticket`是 `Fare`是每张票的票价，`Cabin`是舱号，`Embarked`是乘客上船的地点(例如:C 指瑟堡，S 指南安普顿，Q 指皇后镇)。

让我们分析一下泰坦尼克号的数据集，找出那些对乘客生存有最大依赖性的属性:

1.  首先加载数据集和所需的库:

```py
# load python libraries
import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
#load dataset
titanic=pd.read_csv("/content/titanic.csv")
titanic.head()
```

该代码的输出如下所示:

![](assets/7f893113-cba6-46f9-b79a-c849b43da274.png)

让我们看看代码中数据框的形状:

```py
titanic.shape
```

输出如下:

```py
(891, 12)
```

3.  让我们看一下数据集中丢失的记录数:

```py
total = titanic.isnull().sum().sort_values(ascending=False)
total
```

输出如下:

```py
Cabin 687
Age 177
Embarked 2
Fare 0
Ticket 0
Parch 0
SibSp 0
Sex 0
Name 0
Pclass 0
Survived 0
PassengerId 0
dtype: int64
```

除了`Embarked`、`Age`和`Cabin`之外，所有记录似乎都很好。`Cabin`功能需要进一步研究才能填充这么多，但我们不要在分析中使用它，因为它缺少 77%。此外，处理`Age`功能会相当棘手，它有 177 个缺失值。我们不能忽视年龄因素，因为它可能与存活率相关。`Embarked`功能只有两个缺失值，可以轻松填充。

由于`PassengerId`、`Ticket`和`Name`列具有唯一的值，因此它们与高存活率不相关。

4.  首先，让我们找出从灾难中幸存下来的女性和男性的百分比:

```py
#percentage of women survived
women = titanic.loc[titanic.Sex == 'female']["Survived"]
rate_women = sum(women)/len(women)

#percentage of men survived
men = titanic.loc[titanic.Sex == 'male']["Survived"]
rate_men = sum(men)/len(men)

print(str(rate_women) +" % of women who survived." )
print(str(rate_men) + " % of men who survived." )
```

输出如下:

```py
0.7420382165605095 % of women who survived.
0.18890814558058924 % of men who survived.
```

5.  在这里，你可以看到幸存的女性数量很高，所以性别可能是一个有助于分析任何变量(人)存活率的属性。让我们用男性和女性的存活数来形象化这个信息:

```py
titanic['Survived'] = titanic['Survived'].map({0:"not_survived", 1:"survived"})

fig, ax = plt.subplots(1, 2, figsize = (10, 8))
titanic["Sex"].value_counts().plot.bar(color = "skyblue", ax = ax[0])
ax[0].set_title("Number Of Passengers By Sex")
ax[0].set_ylabel("Population")
sns.countplot("Sex", hue = "Survived", data = titanic, ax = ax[1])
ax[1].set_title("Sex: Survived vs Dead")
plt.show()
```

该代码的输出如下所示:

![](assets/346b35c0-6242-49d4-9a1e-8aa1db404dda.png)

6.  让我们想象一下不同类别的幸存者和死亡人数:

```py
fig, ax = plt.subplots(1, 2, figsize = (10, 8))
titanic["Pclass"].value_counts().plot.bar(color = "skyblue", ax = ax[0])
ax[0].set_title("Number Of Passengers By Pclass")
ax[0].set_ylabel("Population")
sns.countplot("Pclass", hue = "Survived", data = titanic, ax = ax[1])
ax[1].set_title("Pclass: Survived vs Dead")
plt.show()
```

该代码的输出如下所示:

![](assets/7588f3a5-8de7-4498-b03d-9934ae254af3.png)

7.  嗯，看起来`Pclass`**的乘客数量很高，大部分都活不下去。在`Pclass` **2** 中，死亡人数高。并且，`Pclass`T7】1 显示了幸存乘客的最大数量:**

```py
fig, ax = plt.subplots(1, 2, figsize = (10, 8))
titanic["Embarked"].value_counts().plot.bar(color = "skyblue", ax = ax[0])
ax[0].set_title("Number Of Passengers By Embarked")
ax[0].set_ylabel("Number")
sns.countplot("Embarked", hue = "Survived", data = titanic, ax = ax[1])
ax[1].set_title("Embarked: Survived vs Unsurvived")
plt.show()
```

代码的输出如下所示:

![](assets/f652ca84-df90-49da-96ed-fb1650e7efd0.png)

大多数乘客似乎是从南安普敦上船的，其中近 450 人没有生还。

8.  为了可视化`Age`记录，我们将使用`distplot()`方法绘制数据分布。正如我们之前分析的那样，`Age`记录中有 177 个空值，因此我们将在绘制数据之前删除它们:

```py
sns.distplot(titanic['Age'].dropna())
```

该代码的输出如下所示:

![](assets/32d760c7-5a58-4136-bd4a-c1e299ee7835.png)

9.  现在，让我们首先使用`Survived`、`Pclass`、`Fear`和`Age`变量对泰坦尼克号数据集进行多元分析:

```py
sns.set(style="ticks", color_codes=True)
sns.pairplot(titanic,vars = [ 'Fare','Age','Pclass'], hue="Survived")
plt.show()
```

该代码的输出如下所示:

![](assets/ad4d6dd5-6203-40db-b936-49aace782bcf.png)

10.  现在，让我们用热图查看相关表。请注意，第一个`Embarked`映射记录带有整数值，因此我们也可以将`Embarked`包括在我们的相关性分析中:

```py
titanic['Embarked'] = titanic['Embarked'].map({"S":1, "C":2,"Q":2,"NaN":0})
Tcorrelation = titanic.corr(method='pearson')
Tcorrelation
```

该代码的输出如下所示:

![](assets/e314e17a-525d-404e-af73-10113e03a0d5.png)

11.  结果很简单。它显示了各个列之间的相关性。如您所见，在该表中，`PassengerId`与`Fare`和`Age`列呈弱正相关关系:

```py
sns.heatmap(Tcorrelation,xticklabels=Tcorrelation.columns,
            yticklabels=Tcorrelation.columns)
```

该代码的输出如下所示:

![](assets/4e47df39-3c12-4a92-9576-2284c2b82550.png)

如果你想练习更多的分析和预测算法，你可以在 Kaggle 中获得相同的数据集。

到目前为止，您已经了解了数据分析的相关性和类型。您也对数据集进行了不同的分析。现在，我们需要仔细考虑事实，然后根据我们获得的输出对分析做出任何结论。

# 概述辛普森悖论

通常，我们根据数据集做出的决策会受到我们应用于它们的统计度量的输出的影响。这些输出告诉我们相关的类型和数据集的基本可视化。然而，有时，当我们将数据分组并应用统计度量时，或者当我们将数据聚合在一起并应用统计度量时，决策会有所不同。同一数据集结果中的这种异常行为一般称为**辛普森悖论**。简而言之，辛普森悖论是在两种不同情况下分析数据集时出现在分析趋势中的差异:第一，当数据被分成组时，第二，当数据被聚合时。

这里有一个表格，分别代表了男性和女性对两种不同游戏主机的推荐率，也包括:

|  | **推荐****PS4** | **推荐****Xbox One** |
| 男性的 | 50/150=30% | 180/360=50% |
| 女性的 | 200/250=80% | 36/40=90% |
| 联合的；共同的 | 250/400=62.5% | 216/400=54% |

上表按男性和女性分别列出了 PS4 和 Xbox One 两款不同游戏机的推荐率，包括单独推荐和组合推荐。

假设你打算买一个有最大推荐的游戏机。如上表所示，Xbox One 的男女推荐比例均高于 PS4。但是，使用相同的数据组合时，根据所有用户，PS4 具有更高的推荐百分比(62.5%)。那么，你怎么决定选哪一个呢？这些计算看起来不错，但从逻辑上讲，决策似乎并不顺利。这就是辛普森悖论。这里的同一个数据集证明了两个相反的论点。

在这种情况下，主要问题是，当我们只看到单独数据的百分比时，它没有考虑样本量。由于每个分数都显示了通过询问的数量推荐游戏控制台的用户数量，因此考虑整个样本大小是有意义的。男性和女性单独数据中的样本量相差很大。比如男性对 PS4 的推荐是 50，而对 Xbox One 的推荐是 180。这些数字有着巨大的差异。Xbox One 的男性响应远远多于女性，而 PS4 的情况正好相反。因为推荐 PlayStation 的男性更少，这导致数据合并后 PS4 的平均评分更低，这导致了矛盾。

为了就我们应该使用哪个控制台做出单一决定，我们需要决定数据是可以合并还是应该单独查看。在这种情况下，我们必须找出哪个控制台最有可能同时满足男性和女性。可能还有其他因素影响这些评论，但我们没有这些数据，所以我们寻找最大数量的好评论，而不考虑性别偏见。在这里，聚合数据最有意义。我们将结合评审结果，与总体平均水平进行比较。因为我们的目标是合并评论并查看总平均值，所以数据的汇总更有意义。

看起来辛普森悖论是一个牵强的问题，理论上是可能的，但实际上从未发生过，因为我们对整体可用数据的统计分析是准确的。然而，现实世界中有许多关于辛普森悖论的著名研究。

现实世界的一个例子是精神健康治疗，如抑郁症。下表列出了两种治疗方法对患者的疗效:

|  | **疗法 A** | **疗法 B** |
| 轻度萧条 | 81/87=93% | 234/270=87% |
| 严重抑郁 | 192/263=73% | 55/80=69% |
| 两者 | 273/350=78% | 289/350=83% |

如你所见，在上表中，有两种疗法: **T** **疗法 A** 和**疗法 B** 。治疗 A 似乎对轻度和重度抑郁症都更有效，但汇总数据显示，治疗 B 对两种情况都更有效。这怎么可能？嗯，我们不能断定数据汇总后的结果是正确的，因为每种治疗方法的样本量是不同的。为了就我们应该采用哪种疗法做出一个单一的决定，我们需要实事求是地思考:数据是如何产生的？还有哪些因素影响了根本看不到的结果？

现实中，轻度抑郁被医生认为是不太严重的情况，A 疗法比 b 疗法更便宜，因此，医生建议轻度抑郁采用更简单的 A 疗法。

我们的数据集中没有提到这两种治疗类型的细节和事实。抑郁症的种类和病情的严重程度会导致混杂变量(混杂变量是我们在数据表中看不到的，但它们可以通过对数据的背景分析来确定)，因为它会影响选择何种治疗和康复方法的决定。因此，决定哪种治疗对患者更有效的因素取决于混杂变量，这就是这里情况的严重性。为了确定哪种疗法效果更好，我们需要病例严重性的报告，然后需要比较两种疗法的恢复率，而不是跨组的汇总数据。

从一组数据中回答我们想要的问题有时需要更多的分析，而不仅仅是查看可用的数据。从辛普森悖论中得到的教训是，光有数据是不够的。数据从来都不是纯粹客观的，最终的剧情也不是。因此，在处理一组数据时，我们必须考虑我们是否得到了整个故事。

在根据我们得到的输出结论分析之前，必须考虑的另一个事实是**相关性并不意味着因果关系**。这在统计领域非常重要，所以维基百科有一篇单独的文章介绍了这一说法。

# 相关性并不意味着因果关系

*C* *或关联* *并不意味着因果关系*是一个有趣的短语，你会在统计学和详细学习数据科学时听到它。但这意味着什么呢？嗯，这只是表明，仅仅因为两件事相关，并不总是意味着一个导致另一个。例如，挪威的冬天很冷，人们往往比夏天花更多的钱购买汤等热食。然而，这并不意味着寒冷的天气会导致人们在汤上花更多的钱。因此，虽然挪威人的支出与天气寒冷有关，但支出并不是天气寒冷的原因。因此，相关性不是因果关系。

请注意，在这个短语中有两个基本术语:相关性和因果关系。**相关性**揭示了一对变量之间的相互关联和共同变化的程度。**因果关系**解释说，一个变量的值的任何变化都会导致另一个变量的量的差异。在这种情况下，一个变量使另一个变量发生。这种现象被称为**因果**。比如你运动的时候( *X* )，你燃烧的热量( *Y* )每分钟都比较高。因此， *X* 导致 *Y* 。根据逻辑理论，我们可以这样说:

![](assets/8e73f608-c147-44cc-8ce9-7f3fcd36b7a6.png)

任何数据分析书中最常见的例子都是关于冰淇淋的销售和凶杀案的兴衰。根据这个例子，随着冰淇淋销量的增加，凶杀案的数量也在增加。基于相关性，这两个事件是高度相关的。然而，冰淇淋的消费并没有导致人的死亡。这两件事不是基于因果理论。因此，相关性并不意味着因果关系。

那么，这个关键短语的要点是什么？嗯，首先，我们不应该根据相关性太快地形成我们的结论。为了理解任何关键的、隐藏的因素，有必要投入一些时间来寻找数据的潜在因素。

# 摘要

在本章中，我们讨论了相关性。相关性是一种统计度量，可用于检查一对变量之间的关系。理解这些关系可以帮助你从一组变量中决定最重要的特征。一旦我们理解了相关性，我们就可以用它来做出更好的预测。变量之间的关系越高，预测的准确性越高。由于相关性更重要，在本章中，我们已经介绍了几种相关方法和不同类型的分析，包括单变量分析、双变量分析和多变量分析。

在下一章中，我们将仔细研究时间序列分析。我们将使用几个真实的数据库，包括时间序列分析，以便进行探索性的数据分析。

# 进一步阅读

*   *关联和相关性*，作者:*李·贝克*，*帕克特出版*，2019 年 6 月 28 日
*   *Python 数据科学*，作者:*罗汉·乔普拉*、*艾伦·英伦*和*穆罕默德·努尔登·阿劳登*、*帕克特出版*，2019 年 7 月
*   *与 NumPy 和 Pandas 的实践数据分析*，作者:*柯蒂斯·米勒***帕克特出版*，2018 年 6 月***